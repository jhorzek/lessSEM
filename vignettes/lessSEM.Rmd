---
title: "lessSEM"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{lessSEM}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Regularized Structural Equation Modeling

Regularized structural equation modeling (REGSEM) has been proposed by Jacobucci et al. (2016)
and Huang et al. (2017). The objective is to reduce overfitting in small samples 
and to allow for more flexibility. The general idea is to regularize some parameters
towards zero. To this end, a penalty function $p(\pmb\theta)$ is added to the 
vanilla objective function. In lessSEM, this objective function is given by the
full information maximum likelihood function $F_{\text{ML}}(\pmb\theta)$. The new
objective function is defined as:

$$F_{\text{REGSEM},\lambda}(\pmb\theta) = F_{\text{ML}}(\pmb\theta)+ \lambda N p(\pmb\theta)$$

* $F_{\text{ML}}(\pmb\theta)$ wants all parameters to be close to the ordinary 
maximum likelihood estimates
* $p(\pmb\theta)$ wants regularized parameters to be close to zero
* $\lambda$ allows us to fine tune which of the two forces mentioned above gets
more influence on the final parameter estimates
* $N$ is the sample size. Scaling with $N$ is done to stay consistent with results
returned by **regsem** and **lslx**.

There are many different penalty functions which could be used. In **lessSEM**,
we have implemented the following functions:

$$
\begin{array}{l|ll}
\text{penalty} & \text{function} & \text{reference}\\
\hline
\text{ridge} & p( x_j) = \lambda x_j^2 & \text{(Hoerl & Kennard, 1970)}\\
\text{lasso} & p( x_j) = \lambda| x_j| & \text{(Tibshirani, 1996)}\\
\text{adaptiveLasso} & p( x_j) = \frac{1}{w_j}\lambda| x_j| & \text{(Zou, 2006)}\\
\text{elasticNet} & p( x_j) = \alpha\lambda|x_j| + (1-\alpha)\lambda x_j^2 & \text{(Zou & Hastie, 2005)}\\
\text{cappedL1} & p( x_j) = \lambda \min(| x_j|, \theta); \theta > 0 & \text{(Zhang, 2010)}\\
\text{lsp} & p( x_j) = \lambda \log(1 + |x_j|\theta); \theta > 0 & \text{(Candès et al., 2008)} \\
\text{scad} & p( x_j) = \begin{cases}
\lambda |x_j| & \text{if } |x_j| \leq \lambda\\
\frac{-x_j^2 + 2\theta\lambda |x_j| - \lambda^2}{2(\theta -1)} & \text{if } \lambda < |x_j| \leq \lambda\theta \\
(\theta + 1) \lambda^2/2 & \text{if } |x_j| \geq \theta\lambda\\
\end{cases}; \theta > 2 & \text{(Fan & Li, 2001)} \\
\text{mcp} & p( x_j) = 
\begin{cases}
\lambda |x_j| - x_j^2/(2\theta) & \text{if } |x_j| \leq \theta\lambda\\
\theta\lambda^2/2 & \text{if } |x_j| > \lambda\theta
\end{cases}; \theta > 0 & \text{(Zhang, 2010)}
\end{array}
$$

## Objectives

The objectives of **lessSEM** are two-fold

1. We wanted to test general purpose optimizers (the default in regsem 1.8.0 is solnp) 
against specialized optimizers (the default in lslx is a variant of GLMNET)
2. To make the optimizers used within **lessSEM** available for other researchers
in order to simplify the implementation of new regularized SEM

## Regularizing SEM

**lessSEM** is heavily inspired by the **regsem** package. It also builds on **lavaan**
to set up the model. 

### Setting up a model

First, start with lavaan:

```{r}
library(lavaan)
library(lessSEM)
set.seed(4321)
# let's simulate data for a simple 
# cfa with 7 observed variables
data <- lessSEM::simulateExampleData(N = 50, 
                                     loadings = c(rep(1,4),
                                                  rep(0,3))
)
head(data)

# we assume a single factor structure
lavaanSyntax <- "
      f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 + l6*y6 + l7*y7 
      f ~~ 1*f
      "
# estimate the model with lavaan
lavaanModel <- cfa(lavaanSyntax, 
                   data = data)
```

Next, decide which parameters should be regularized. Let's
go with l5-l7. In **lessSEM**, we always use the parameter
labels to specify which parameters should be regularized!
```{r}
regularized <- c("l5", "l6", "l7")
# tip: we can use paste to make this easier:
regularized <- paste0("l", 5:7)
```

Finally, we set up the regularized model. To this end, we must
first decide which penalty function we want to use. If we want
to shrink parameters without setting them to zero, we can use 
ridge regularization. Otherwise, we must use any of the other
penalty functions mentioned above. In **lessSEM**, there is 
a dedicated function for each of these penalties. The names
of these functions are identical to the "penalty" column in the
table above. For instance, let's have a look at the lasso penalty:

```{r, eval = FALSE}
fitLasso <- lasso(lavaanModel = lavaanModel, 
                  regularized = regularized,
                  nLambdas = 50)
```

```{r, include = FALSE}
fitLasso <- lasso(lavaanModel = lavaanModel, 
                  regularized = regularized,
                  nLambdas = 50)
```

Plot the paths to see what is going on:

```{r}
plot(fitLasso)
```

Note that the parameters are pulled towards zero as $\lambda$ increases. Note also
that we did not specify specific values for $\lambda$ in the lasso function above.
Instead, we only specified how many $\lambda$s we want to have (`nLambdas=50`).
If we use the lasso or adaptive lasso, **lessSEM** can automatically compute 
which $\lambda$ is necessary to set all parameters to zero. This is currently
not supported for any of the other penalties. 

You can access all parameters with the `coef()` function:

```{r}
head(coef(fitLasso))
```



Now, let's assume you also want 
to try out the scad penalty. In this case, all you have to do is to replace
the `lasso()` function with the `scad()` function:

```{r, eval = FALSE}
fitScad <- scad(lavaanModel = lavaanModel, 
                regularized = regularized,
                lambdas = seq(0,1,.1),
                thetas = seq(2.1, 5,length.out = 4))
```

```{r, include = FALSE}
fitScad <- scad(lavaanModel = lavaanModel, 
                regularized = regularized,
                lambdas = seq(0,1,.1),
                thetas = seq(2.1, 5,length.out = 4))
```

The scad penalty has two tuning parmeters $\lambda$ and $\theta$. The naming follows
that used by Gong et al. (2013). We can plot the results again, however this requires
the **plotly** package and is currently not supported in Rmarkdown.

```{r, eval = FALSE}
plot(fitScad)
```

The parameter estimates can again be accessed with the `coef()` function:

```{r}
head(coef(fitScad))
```

### Selecting a model

To select a model and report the final parameter estimates, you can use the 
AIC or BIC (other information criteria are also possible, but currently not 
implemented). There are two ways to use these information criteria.

First, you can compute them and select the model yourself:

```{r}
AICs <- AIC(fitLasso)
head(AICs)

fitLasso@parameters[which.min(AICs$AIC),]
```

An easier way is to use the `coef()` function again:

```{r}
coef(fitLasso, criterion = "AIC")
```

#### Cross-Validation

A very good alternative to information criteria is the use of cross-validation.
In **lessSEM**, there is a dedicated cross-validation function for each of the 
penalties discussed above. Let's look at the `lsp()` penalty this time. Now,
for your non-cross-validated lsp, you would use

```{r, eval = FALSE}
fitLsp <- lsp(lavaanModel = lavaanModel, 
              regularized = regularized,
              lambdas = seq(0,1,.1),
              thetas = seq(.1,2,length.out = 4))
```

To use a cross-validated version of the lsp, simply use the `cv` prefix. The function
is called `cvLsp()`:

```{r, include= FALSE}
fitCvLsp <- cvLsp(lavaanModel = lavaanModel, 
                  regularized = regularized,
                  lambdas = seq(0,1,.1),
                  thetas = seq(.1,2,length.out = 4))
```

```{r, eval = FALSE}
fitCvLsp <- cvLsp(lavaanModel = lavaanModel, 
                  regularized = regularized,
                  lambdas = seq(0,1,.1),
                  thetas = seq(.1,2,length.out = 4))
```

The best model can now be accessed with 
```{r}
coef(fitCvLsp)
```



### Missing Data

Most psychological data sets will have missing data. In **lessSEM**, we follow the
example of **regsem** and use the full information maximum likelihood function
to account for this missingness. Identical to **regsem**, **lessSEM** expects
that you already use the full information maximum likelihood method in **lavaan**.

```{r}
# let's simulate data for a simple 
# cfa with 7 observed variables
# and 10 % missing data
data <- lessSEM::simulateExampleData(N = 100, 
                                     loadings = c(rep(1,4),
                                                  rep(0,3)),
                                     percentMissing = 10
)
head(data)

# we assume a single factor structure
lavaanSyntax <- "
      f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 + l6*y6 + l7*y7 
      f ~~ 1*f
      "
# estimate the model with lavaan
lavaanModel <- cfa(lavaanSyntax, 
                   data = data,
                   missing = "ml") # important: use fiml for missing data
```

Note that we added the argument `missing = 'ml'` to the **lavaan** model.
This tells **lavaan** to use the full information maximum likelihood function.

Next, pass this model to any of the penalty functions in **lessSEM**. **lessSEM**
will automatically switch to the full information maximum likelihood function 
as well:

```{r, include = FALSE}
fitLasso <- lasso(lavaanModel = lavaanModel, 
                  regularized = regularized,
                  nLambdas = 10)
```

```{r, eval = FALSE}
fitLasso <- lasso(lavaanModel = lavaanModel, 
                  regularized = regularized,
                  nLambdas = 10)
```

To check if **lessSEM** did actually use the full information maximum
likelihood, we can compare the 2log-likelihood of **lavaan** and **lessSEM**
when no penalty is used ($\lambda = 0$):

```{r, include = FALSE}
fitLasso <- lasso(lavaanModel = lavaanModel, 
                  regularized = regularized,
                  lambdas = 0)
```
```{r, eval = FALSE}
fitLasso <- lasso(lavaanModel = lavaanModel, 
                  regularized = regularized,
                  lambdas = 0)
```
```{r, eval = TRUE}
fitLasso@fits$m2LL
```


Compare this to:
```{r}
-2*logLik(lavaanModel)
```


## More information

We provide more information in the documentation of the individual functions.
For instance, see `?lessSEM::lasso` for more details on the lasso penalty. If you
are interested in the general purpose interface, have a look at `?lessEM::gpLasso`,
`?lesssEM::gpMcp`, etc. To get more details on implementing the **lessSEM** optimizers in
your own package, have a look at the vignettes `vignette('General-Purpose-Optimization')` and 
`vignette('The-optimizer-interface')` and at the [lessLM](https://github.com/jhorzek/lessLM) package.

## Table of the most relevant functions

Fitting _regularized SEM_:

| function name | what it does |
| --- | --- |
| ridge | ridge regularization of SEM |
| cvRidge | cross-validated ridge regularization of SEM |
| lasso | lasso regularization of SEM |
| cvLasso | cross-validated lasso regularization of SEM |
| adaptiveLasso | adaptive lasso regularization of SEM |
| cvAdaptiveLasso | cross-validated adaptive lasso regularization of SEM |
| elasticNet | elastic net regularization of SEM |
| cvElasticNet | cross-validated elastic net regularization of SEM |
| cappedL1 | cappedL1 regularization of SEM |
| cvCappedL1 | cross-validated cappedL1 regularization of SEM |
| lsp | lsp regularization of SEM |
| cvLsp | cross-validated lsp regularization of SEM |
| mcp | mcp regularization of SEM |
| cvMcp | cross-validated mcp regularization of SEM |
| scad | scad regularization of SEM |
| cvScad | cross-validated scad regularization of SEM |

Using the optimizers in **lessSEM** for _general purpose optimization_:

| function name | what it does |
| --- | --- |
| gpRidge | ridge regularization for general purpose optimization |
| gpLasso | lasso regularization for general purpose optimization |
| gpAdaptiveLasso | adaptive lasso regularization for general purpose optimization |
| gpElasticNet | elastic net regularization for general purpose optimization |
| gpCappedL1 | cappedL1 regularization for general purpose optimization |
| gpLsp | lsp regularization for general purpose optimization |
| gpMcp | mcp regularization for general purpose optimization |
| gpScad | scad regularization for general purpose optimization |

Using the optimizers in **lessSEM** for _general purpose optimization with C++ functions_:

| function name | what it does |
| --- | --- |
| gpRidgeCpp | ridge regularization for general purpose optimization |
| gpLassoCpp | lasso regularization for general purpose optimization |
| gpAdaptiveLassoCpp | adaptive lasso regularization for general purpose optimization |
| gpElasticNetCpp | elastic net regularization for general purpose optimization |
| gpCappedL1Cpp | cappedL1 regularization for general purpose optimization |
| gpLspCpp | lsp regularization for general purpose optimization |
| gpMcpCpp | mcp regularization for general purpose optimization |
| gpScadCpp | scad regularization for general purpose optimization |


# References

## R - Packages / Software

* [lavaan](https://github.com/yrosseel/lavaan) Rosseel, Y. (2012). lavaan: An R Package for Structural Equation Modeling. Journal of Statistical Software, 48(2), 1–36. https://doi.org/10.18637/jss.v048.i02
* [regsem](https://github.com/Rjacobucci/regsem): Jacobucci, R. (2017). regsem: 
Regularized Structural Equation Modeling. ArXiv:1703.08489 [Stat]. http://arxiv.org/abs/1703.08489
* [lslx](https://github.com/psyphh/lslx): Huang, P.-H. (2020). lslx: 
Semi-confirmatory structural equation modeling via penalized likelihood. Journal 
of Statistical Software, 93(7). https://doi.org/10.18637/jss.v093.i07
* [fasta](https://cran.r-project.org/web/packages/fasta/index.html): 
Another implementation of the fista algorithm (Beck & Teboulle, 2009)
* [ensmallen](https://ensmallen.org/): Curtin, R. R., Edel, M., Prabhu, R. G., 
Basak, S., Lou, Z., & Sanderson, C. (2021). The ensmallen library for ﬂexible 
numerical optimization. Journal of Machine Learning Research, 22, 1–6.

## Regularized Structural Equation Modeling

* Huang, P.-H., Chen, H., & Weng, L.-J. (2017). A Penalized Likelihood Method 
for Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9
* Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural 
Equation Modeling. Structural Equation Modeling: A Multidisciplinary Journal, 23(4), 
555–566. https://doi.org/10.1080/10705511.2016.1154793

## Penalty Functions

* Candès, E. J., Wakin, M. B., & Boyd, S. P. (2008). Enhancing Sparsity by 
Reweighted l1 Minimization. Journal of Fourier Analysis and Applications, 14(5–6), 
877–905. https://doi.org/10.1007/s00041-008-9045-x
* Fan, J., & Li, R. (2001). Variable selection via nonconcave penalized 
likelihood and its oracle properties. Journal of the American Statistical 
Association, 96(456), 1348–1360. https://doi.org/10.1198/016214501753382273
* Hoerl, A. E., & Kennard, R. W. (1970). Ridge Regression: Biased Estimation 
for Nonorthogonal Problems. Technometrics, 12(1), 55–67. https://doi.org/10.1080/00401706.1970.10488634
* Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. 
Journal of the Royal Statistical Society. Series B (Methodological), 58(1), 267–288.
* Zhang, C.-H. (2010). Nearly unbiased variable selection under minimax concave penalty. 
The Annals of Statistics, 38(2), 894–942. https://doi.org/10.1214/09-AOS729
* Zhang, T. (2010). Analysis of Multi-stage Convex Relaxation for Sparse Regularization. 
Journal of Machine Learning Research, 11, 1081–1107.
* Zou, H. (2006). The adaptive lasso and its oracle properties. Journal of the 
American Statistical Association, 101(476), 1418–1429. https://doi.org/10.1198/016214506000000735
* Zou, H., & Hastie, T. (2005). Regularization and variable selection via the 
elastic net. Journal of the Royal Statistical Society: Series B, 67(2), 301–320. 
https://doi.org/10.1111/j.1467-9868.2005.00503.x

## Optimizer

### GLMNET 

* Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization paths for 
generalized linear models via coordinate descent. Journal of Statistical 
Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01
* Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). An improved GLMNET for 
l1-regularized logistic regression. The Journal of Machine Learning Research, 
13, 1999–2030. https://doi.org/10.1145/2020408.2020421

### Variants of ISTA

* Beck, A., & Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding 
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1), 
183–202. https://doi.org/10.1137/080716542
* Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). A general iterative 
shrinkage and thresholding algorithm for non-convex regularized optimization problems. 
Proceedings of the 30th International Conference on Machine Learning, 28(2)(2), 37–45.
* Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations and 
Trends in Optimization, 1(3), 123–231.


# Important Notes

THE SOFTWARE IS PROVIDED 'AS IS', WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, 
INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, 
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE 
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, 
WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN 
CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. 



