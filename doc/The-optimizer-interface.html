<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />



<title>The-optimizer-interface</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>



<style type="text/css">
code {
white-space: pre;
}
.sourceCode {
overflow: visible;
}
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
{ counter-reset: source-line 0; }
pre.numberSource code > span
{ position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
{ content: counter(source-line);
position: relative; left: -1em; text-align: right; vertical-align: baseline;
border: none; display: inline-block;
-webkit-touch-callout: none; -webkit-user-select: none;
-khtml-user-select: none; -moz-user-select: none;
-ms-user-select: none; user-select: none;
padding: 0 4px; width: 4em;
color: #aaaaaa;
}
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }
div.sourceCode
{ }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } 
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.at { color: #7d9029; } 
code span.bn { color: #40a070; } 
code span.bu { color: #008000; } 
code span.cf { color: #007020; font-weight: bold; } 
code span.ch { color: #4070a0; } 
code span.cn { color: #880000; } 
code span.co { color: #60a0b0; font-style: italic; } 
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.do { color: #ba2121; font-style: italic; } 
code span.dt { color: #902000; } 
code span.dv { color: #40a070; } 
code span.er { color: #ff0000; font-weight: bold; } 
code span.ex { } 
code span.fl { color: #40a070; } 
code span.fu { color: #06287e; } 
code span.im { color: #008000; font-weight: bold; } 
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.kw { color: #007020; font-weight: bold; } 
code span.op { color: #666666; } 
code span.ot { color: #007020; } 
code span.pp { color: #bc7a00; } 
code span.sc { color: #4070a0; } 
code span.ss { color: #bb6688; } 
code span.st { color: #4070a0; } 
code span.va { color: #19177c; } 
code span.vs { color: #4070a0; } 
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } 
</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>




<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">The-optimizer-interface</h1>



<p>So, you’ve decided to take a closer look at the optimization routines
implemented in <strong>lessSEM</strong>. That’s awesome! The following
document will give you a short introduction to the general idea of how
the optimizers were designed. It will not cover the optimizers
themselves; here, we recommend reading the following articles:</p>
<p><strong>GLMNET</strong>:</p>
<ul>
<li>Friedman, J., Hastie, T., &amp; Tibshirani, R. (2010).
Regularization paths for generalized linear models via coordinate
descent. Journal of Statistical Software, 33(1), 1–20. <a href="https://doi.org/10.18637/jss.v033.i01" class="uri">https://doi.org/10.18637/jss.v033.i01</a></li>
<li>Yuan, G.-X., Ho, C.-H., &amp; Lin, C.-J. (2012). An improved GLMNET
for l1-regularized logistic regression. The Journal of Machine Learning
Research, 13, 1999–2030. <a href="https://doi.org/10.1145/2020408.2020421" class="uri">https://doi.org/10.1145/2020408.2020421</a></li>
</ul>
<p><strong>Variants of ISTA</strong>:</p>
<ul>
<li>Beck, A., &amp; Teboulle, M. (2009). A Fast Iterative
Shrinkage-Thresholding Algorithm for Linear Inverse Problems. SIAM
Journal on Imaging Sciences, 2(1), 183–202. <a href="https://doi.org/10.1137/080716542" class="uri">https://doi.org/10.1137/080716542</a></li>
<li>Gong, P., Zhang, C., Lu, Z., Huang, J., &amp; Ye, J. (2013). A
general iterative shrinkage and thresholding algorithm for non-convex
regularized optimization problems. Proceedings of the 30th International
Conference on Machine Learning, 28(2)(2), 37–45.</li>
<li>Parikh, N., &amp; Boyd, S. (2013). Proximal Algorithms. Foundations
and Trends in Optimization, 1(3), 123–231.</li>
</ul>
<p>All optimizers implemented in <strong>lessSEM</strong> are located in
the inst/include directory. They are all written in C++ to improve the
performance and implemented as header-only files so that they can be
used by other packages. The glmnet optimizer is located in
inst/include/glmnet_class.hpp and the ista variants in
inst/include/ista_class.hpp.</p>
<div id="the-fitting-function" class="section level2">
<h2>The Fitting Function</h2>
<p>The objective of the optimizers is to minimize the fitting function.
In both, glmnet and ista we assume that this fitting function is given
by a differentiable part and a non-differentiable part. To be more
specific, the fitting function is given by:</p>
<p><span class="math display">\[f(\pmb{\theta}) =
\underbrace{l(\pmb\theta) +
s(\pmb\theta,\pmb{t}_s)}_{\text{differentiable}} +
\underbrace{p(\pmb\theta,\pmb{t}_p)}_{\text{non-differentiable}}\]</span>
where <span class="math inline">\(l(\pmb\theta)\)</span> is the
unregularized fitting function (e.g., the -2log-likelihood) in the SEMs
implemented in <strong>lessSEM</strong>. <span class="math inline">\(s(\pmb\theta,\pmb{t}_s)\)</span> is a
differentiable (smooth) penalty function (e.g., a ridge penalty) and
<span class="math inline">\(p(\pmb\theta,\pmb{t}_p)\)</span> is a
non-differentiable penalty function (e.g., a lasso penalty). All three
functions take the parameter estimates <span class="math inline">\(\pmb\theta\)</span> as input and return a single
value as output. The penalty functions <span class="math inline">\(s(\pmb\theta,\pmb{t}_s)\)</span> and <span class="math inline">\(p(\pmb\theta,\pmb{t}_p)\)</span> additionally
expect vectors with tuning parameters–<span class="math inline">\(\pmb{t}_s\)</span> in case of the smooth penalty
and <span class="math inline">\(\pmb{t}_p\)</span> in case of the
non-differentiable penalty. Thus, in theory both penalty functions can
use different tuning parameters.</p>
<p>A prototypical example for fitting functions of the form defined
above is the elastic net. Here, <span class="math display">\[f(\pmb{\theta}) = \underbrace{l(\pmb\theta) +
(1-\alpha)\lambda \sum_j\theta_j^2}_{\text{differentiable}} +
\underbrace{\alpha\lambda\sum_j|
\theta_j|}_{\text{non-differentiable}}\]</span></p>
<p>The elastic net is a combination of a ridge penalty and a lasso
penalty. Note that in this case, both penalties take in the same tuning
parameters (<span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\alpha\)</span>).</p>
<p>With this in mind, we can now have a closer look at the optimization
functions. We will start with glmnet (see function glmnet in the file
inst/include/glmnet_class.hpp). This function is called as follows:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>inline lessSEM<span class="sc">::</span>fitResults <span class="fu">glmnet</span>(model<span class="sc">&amp;</span> model_, </span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>                                  Rcpp<span class="sc">::</span>NumericVector startingValuesRcpp,</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>                                  penaltyLASSOGlmnet<span class="sc">&amp;</span> penalty_,</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>                                  penaltyRidgeGlmnet<span class="sc">&amp;</span> smoothPenalty_, </span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>                                  const tuningParametersEnetGlmnet tuningParameters,</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>                                  const controlGLMNET<span class="sc">&amp;</span> <span class="at">control_ =</span> <span class="fu">controlGlmnetDefault</span>())</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>{...}</span></code></pre></div>
<p>The first argument is a model. This model has to be created by you
and must inherit from the lessSEM::model class (see lessLM for an
example). Most importantly, this model must provide means to get the
value of the first part of the fitting function: <span class="math inline">\(l(\pmb\theta)\)</span>. It must also provide means
to compute the gradients of your fitting function.</p>
<p>The next argument are the starting values which are given as an
Rcpp::NumericVector. This type is chosen because it can have labels and
in its current implementation <strong>lessSEM</strong> expects that you
give all your startingValues names.</p>
<p>Now come the actual penalty functions. The first one is the
non-differentiable penalty: the lasso <span class="math inline">\(p(\pmb\theta)\)</span>. This must be an object of
class penaltyLASSOGlmnet which can be created with
lessSEM::penaltyLASSOGlmnet(). Next comes the differentiable ridge
penalty which must be of class penaltyRidgeGlmnet and can be created
with lessSEM::penaltyRidgeGlmnet.</p>
<p>Now, the tuning parameters deviate a bit from what we discussed
before. We said that the differentiable and the non-differentiable
penalty functions will each take their own vector of tuning parameters
(<span class="math inline">\(\pmb t_s\)</span> and <span class="math inline">\(\pmb t_p\)</span> respectively). Note, however,
that the elastic net uses the same tuning parameters for both, ridge and
lasso penalty. In the glmnet optimizer we therefore decided to combine
both of these into one: the tuningParametersEnetGlmnet struct.
tuningParametersEnetGlmnet has three slots: alpha (to set <span class="math inline">\(\alpha\)</span>), lambda (to set <span class="math inline">\(\lambda\)</span>) and a slot called weights. Now,
the weights allow us to switch off the penalty for selected parameters.
For instance, in a linear regression we would not want to penalize the
intercept. To this end, the fitting function that is actually
implemented internally is given by</p>
<p><span class="math display">\[f(\pmb{\theta}) =
\underbrace{l(\pmb\theta) + (1-\alpha)\lambda \sum_j\omega_j
\theta_j^2}_{\text{differentiable}} +
\underbrace{\alpha\lambda\sum_j\omega_j|
\theta_j|}_{\text{non-differentiable}}\]</span></p>
<p>If we set <span class="math inline">\(\omega_j = 0\)</span> for a
specific parameter, this parameter is unregularized. Setting <span class="math inline">\(\omega_j = 1\)</span> means that parameter <span class="math inline">\(j\)</span> is penalized. <span class="math inline">\(\omega_j\)</span> can also take any other value
(e.g., <span class="math inline">\(\omega_j = .4123\)</span>) which
allows for penalties such as the adaptive lasso. Importantly, the
weights vector must be of the same length as startingValuesRcpp. That
is, each parameter must have a weight associated with it in the weights
vector.</p>
<p>Finally, there is the controlGLMNET argument. This argument lets us
fine tune the optimizer. To use the control argument, create a new
control object in C++ as follows:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>lessSEM<span class="sc">::</span>controlGLMNET myControlObject <span class="ot">=</span> lessSEM<span class="sc">::</span><span class="fu">controlGlmnetDefault</span>();</span></code></pre></div>
<p>Now, you can tweak each element of myControlObject; e.g.,</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>myControlObject.maxIterOut <span class="ot">=</span> <span class="dv">100</span> <span class="sc">/</span><span class="er">/</span> only <span class="dv">100</span> outer iterations</span></code></pre></div>
<p>Currently, the following control elements can be changed:</p>
<ul>
<li>initialHessian: a matrix with the initial Hessian approximation</li>
<li>stepSize: the initial step size to be used in the line search</li>
<li>sigma: Controls the sigma parameter in Yuan, G.-X., Ho, C.-H., &amp;
Lin, C.-J. (2012). An improved GLMNET for l1-regularized logistic
regression. The Journal of Machine Learning Research, 13, 1999–2030. <a href="https://doi.org/10.1145/2020408.2020421" class="uri">https://doi.org/10.1145/2020408.2020421</a>.</li>
<li>gamma: Controls the gamma parameter in Yuan, G.-X., Ho, C.-H., &amp;
Lin, C.-J. (2012). An improved GLMNET for l1-regularized logistic
regression. The Journal of Machine Learning Research, 13, 1999–2030. <a href="https://doi.org/10.1145/2020408.2020421" class="uri">https://doi.org/10.1145/2020408.2020421</a>.</li>
<li>maxIterOut: maximal number of outer iterations</li>
<li>maxIterIn: maximal number of inner iterations</li>
<li>maxIterLine: maximal number of iterations given to the line search
procedure</li>
<li>breakOuter: small value; if the outer convergence criterion falls
below this threshold, it is assumed that the model converged</li>
<li>breakInner: small value; if the inner convergence criterion falls
below this threshold, it is assumed that the inner optimization</li>
<li>verbose: if set to a value &gt; 0, the fit every verbose iterations
is printed.</li>
</ul>
<p>If you take a closer look at how the two penalty functions are
handled within glmnet, you will realize that we basically absorb the
differentiable penalty in the normal fitting function. That is, only the
non-differentiable part gets a special treatment, while the
differentiable part is simply added to the differntiable <span class="math inline">\(l(\pmb\theta)\)</span>. To give an example:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>gradients_kMinus1 <span class="ot">=</span> <span class="fu">model_.gradients</span>(parameters_kMinus1, parameterLabels) <span class="sc">+</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>      <span class="fu">smoothPenalty_.getGradients</span>(parameters_kMinus1, parameterLabels, tuningParameters); <span class="sc">/</span><span class="er">/</span> ridge part</span></code></pre></div>
<p>Note how the gradients of <span class="math inline">\(l(\pmb\theta)\)</span> and <span class="math inline">\(s(\pmb\theta,\pmb{t}_s)\)</span> are combined into
one.</p>
</div>
<div id="the-ista-variants" class="section level2">
<h2>The ista variants</h2>
<p>Besides the glmnet optimizer, we also implemented variants of ista.
These are based on the publications mentioned above. The fitting
function is again given by <span class="math display">\[f(\pmb{\theta})
= \underbrace{l(\pmb\theta) +
s(\pmb\theta,\pmb{t}_s)}_{\text{differentiable}} +
\underbrace{p(\pmb\theta,\pmb{t}_p)}_{\text{non-differentiable}}\]</span>
However, ista is a lot more flexible than glmnet at the moment because
it allows for penalties that are not subsumed under the elastic net
umbrella. In the following, we will build a lot on what we’ve already
discussed regarding the glmnet optimizer above.</p>
<p>First, let’s have a look at the ista function located in
inst/include/ista_class.hpp. This function is called with:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>template<span class="sc">&lt;</span>typename T, typename U<span class="sc">&gt;</span> <span class="er">//</span> T is the type of the tuning parameters</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>inline lessSEM<span class="sc">::</span>fitResults <span class="fu">ista</span>(</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    model<span class="sc">&amp;</span> model_, </span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    Rcpp<span class="sc">::</span>NumericVector startingValuesRcpp,</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    proximalOperator<span class="sc">&lt;</span>T<span class="sc">&gt;</span><span class="er">&amp;</span> proximalOperator_, <span class="sc">/</span><span class="er">/</span> proximalOperator takes the tuning parameters</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    <span class="sc">/</span><span class="er">/</span> as input <span class="ot">-&gt;</span> <span class="er">&lt;</span>T<span class="sc">&gt;</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    penalty<span class="sc">&lt;</span>T<span class="sc">&gt;</span><span class="er">&amp;</span> penalty_, <span class="sc">/</span><span class="er">/</span> penalty takes the tuning parameters</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    smoothPenalty<span class="sc">&lt;</span>U<span class="sc">&gt;</span><span class="er">&amp;</span> smoothPenalty_, <span class="sc">/</span><span class="er">/</span> smoothPenalty takes the smooth tuning parameters</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    <span class="sc">/</span><span class="er">/</span> as input <span class="ot">-&gt;</span> <span class="er">&lt;</span>U<span class="sc">&gt;</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    const T<span class="sc">&amp;</span> tuningParameters, <span class="sc">/</span><span class="er">/</span> tuning parameters are of type T</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    const U<span class="sc">&amp;</span> smoothTuningParameters, <span class="sc">/</span><span class="er">/</span> tuning parameters are of type U</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    const control<span class="sc">&amp;</span> <span class="at">control_ =</span> <span class="fu">controlDefault</span>()</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>{...}</span></code></pre></div>
<p>This function is more complicated that the glmnet function discussed
above. But, let’s start with the part that stays the same: First, we
still have to pass our model to the function. This model must have a fit
and a gradients function which return the fit and the gradient
respectively. Next, the function again expects us to provide starting
values as an Rcpp::NumericVector. We will skip the proximalOperator and
the penalty for the moment (these relate to <span class="math inline">\(p(\pmb\theta,\pmb t_p)\)</span>) and concentrate
on the smoothPenalty first. This is the function <span class="math inline">\(s(\pmb\theta,\pmb t_s)\)</span>. In our previous
example, we looked at the elastic net penalty, where the smooth penalty
is a ridge penalty function. Now, in the ista optimizer, we can also
pass in the ridge penalty as a smooth penalty. In fact, this is exactly
what we do when we use ista to fit the elastic net (see class istaEnet
in src/elasticNet.cpp). This smooth penalty has the tuning parameters
<span class="math inline">\(\pmb t_s\)</span> which are called
smoothTuningParameters in the function call. In case of the elastic net,
these would again be <span class="math inline">\(\alpha\)</span> and
<span class="math inline">\(\lambda\)</span> (and the weights vector).
Similar to the glmnet procedure outlined above, the differentiable
penalty <span class="math inline">\(s(\pmb\theta,\pmb t_s)\)</span> is
simply absorbed in the unregularized fitting function <span class="math inline">\(l(\pmb\theta)\)</span>.</p>
<p>Now, for the non-differentiable part <span class="math inline">\(p(\pmb\theta,\pmb p_s)\)</span>, the ista
optimizer uses so-called proximal operators. The details are beyond the
scope here, but Parikh et al. (2013) provide a very good overview of
these algorithms. To make things work with ista, we must pass such a
proximal operator to the optimizer. Within <strong>lessSEM</strong>, we
have prepared a few of these proximal operators for well-known penalty
functions. For instance, you will find the proximal operator for the mcp
penalty in inst/include/mcp.hpp. Additionally, we need a function which
returns the acutal penalty value. This is the penalty object in the
function call. Finally, the penalty <span class="math inline">\(p(\pmb\theta,\pmb t_p)\)</span> gets its tuning
parameters <span class="math inline">\(\pmb t_p\)</span>. This is the
tuningParameters object above. To make things more concrete, let’s look
at the elastic net again. In this case, penalty would be of class
lessSEM::penaltyLASSO and the proximal operator of type
lessSEM::proximalOperatorLasso. The tuning parameters would again be
<span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\lambda\)</span> (and the weights vector). An
example is provided in the class istaEnet in src/elasticNet.cpp.</p>
<p>Note that many of the penalty function implemented in
<strong>lessSEM</strong> are typically not combined with a smooth
penalty (e.g., scad, mcp, …). In this case, you must still pass a
smoothPenalty object to ista. To this end, we created the
lessSEM::noSmoothPenalty class which can be used instead of a smooth
penalty like ridge. See class istaMcp in src/mcp.cpp for an example.</p>
<p>Finally, there is the control argument. This argument lets us fine
tune the optimizer. To use the control argument, create a new control
object in C++ as follows:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>lessSEM<span class="sc">::</span>control myControlObject <span class="ot">=</span> lessSEM<span class="sc">::</span><span class="fu">controlDefault</span>();</span></code></pre></div>
<p>Now, you can tweak each element of myControlObject; e.g.,</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>myControlObject.L0 <span class="ot">=</span> .<span class="dv">9</span></span></code></pre></div>
<p>Currently, the following control elements can be changed:</p>
<ul>
<li>L0: controls the step size used in the first iteration</li>
<li>eta: controls by how much the step size changes in the inner
iterations with <span class="math inline">\((eta^i)*L\)</span>, where
<span class="math inline">\(i\)</span> is the inner iteration</li>
<li>accelerate: if true, the extrapolation parameter is used to
accelerate ista (see, e.g., Parikh, N., &amp; Boyd, S. (2013). Proximal
Algorithms. Foundations and Trends in Optimization, 1(3), 123–231.,
p. 152)</li>
<li>maxIterOut: maximal number of outer iterations</li>
<li>maxIterIn: maximal number of inner iterations</li>
<li>breakOuter: small value; if the outer convergence criterion falls
below this threshold, it is assumed that the model converged</li>
<li>convCritInner: this is related to the inner breaking condition. If
set to ista, the one presented by Beck &amp; Teboulle (2009) is used;
see Remark 3.1 on p. 191 (ISTA with backtracking). If set to gist, the
one presented by Gong et al. (2013) (Equation 3) is used.<br />
</li>
<li>sigma: in (0,1); is used by the gist convergence criterion. larger
sigma# enforce larger improvement in fit</li>
<li>stepSizeIn: how should step sizes be carried forward from iteration
to iteration? If set to initial, the step size is reset to L0 in each
iteration, If set to istaStepInheritance, the previous step size is used
as an initial value for the next iteration. If set to barzilaiBorwein,
we use the Barzilai-Borwein procedure. Finally, if set to
stochasticBarzilaiBorwein, we also use the Barzilai-Borwein procedure,
but sometimes resets the step size; this can – in our experience – help
when the optimizer is caught in a bad spot.</li>
<li>sampleSize: can be used to scale the fitting function down</li>
<li>verbose: if set to a value &gt; 0, the fit every verbose iterations
is printed.</li>
</ul>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
