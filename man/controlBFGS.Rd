% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/controlOptimizer.R
\name{controlBFGS}
\alias{controlBFGS}
\title{controlBFGS}
\usage{
controlBFGS(
  startingValues = "est",
  initialHessian = ifelse(all(startingValues == "est"), "compute", 1),
  saveHessian = FALSE,
  stepSize = 0.9,
  sigma = 1e-05,
  gamma = 0,
  maxIterOut = 1000,
  maxIterIn = 1000,
  maxIterLine = 500,
  breakOuter = 1e-08,
  breakInner = 1e-10,
  convergenceCriterion = 0,
  verbose = 0
)
}
\arguments{
\item{startingValues}{option to provide initial starting values. Only used for the first lambda. Three options are supported. Setting to "est" will use the estimates
from the lavaan model object. Setting to "start" will use the starting values of the lavaan model. Finally, a labeled vector with parameter
values can be passed to the function which will then be used as starting values.}

\item{initialHessian}{option to provide an initial Hessian to the optimizer. Must have row and column names corresponding to the parameter labels. use getLavaanParameters(lavaanModel) to 
see those labels. If set to "scoreBased", the outer product of the scores will be used as an approximation (see https://en.wikipedia.org/wiki/Berndt%E2%80%93Hall%E2%80%93Hall%E2%80%93Hausman_algorithm).
If set to "compute", the initial hessian will be computed. If set to a single value, a diagonal matrix with the single value along the diagonal will be used.}

\item{saveHessian}{should the Hessian be saved for later use? Note: This may take a lot of memory!}

\item{stepSize}{Initial stepSize of the outer iteration (theta_{k+1} = theta_k + stepSize * Stepdirection)}

\item{sigma}{only relevant when lineSearch = 'GLMNET'. Controls the sigma parameter in Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). An improved GLMNET for l1-regularized logistic regression. The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421.}

\item{gamma}{Controls the gamma parameter in Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). An improved GLMNET for l1-regularized logistic regression. The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421. Defaults to 0.}

\item{maxIterOut}{Maximal number of outer iterations}

\item{maxIterIn}{Maximal number of inner iterations}

\item{maxIterLine}{Maximal number of iterations for the line search procedure}

\item{breakOuter}{Stopping criterion for outer iterations}

\item{breakInner}{Stopping criterion for inner iterations}

\item{convergenceCriterion}{which convergence criterion should be used for the outer iterations? possible are 0 = GLMNET, 1 = gradients, 2 = fitChange
Note that in case of gradients and GLMNET, we divide the gradients (and the Hessian) of the log-Likelihood by N as it would otherwise be
considerably more difficult for larger sample sizes to reach the convergence criteria.}

\item{verbose}{0 prints no additional information, > 0 prints GLMNET iterations}
}
\value{
object of class controlBFGS
}
\description{
Control the BFGS optimizer.
}
