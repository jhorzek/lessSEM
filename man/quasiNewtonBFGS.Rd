% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/quasiNewtonBFGS.R
\name{quasiNewtonBFGS}
\alias{quasiNewtonBFGS}
\title{quasiNewtonBFGS}
\usage{
quasiNewtonBFGS(
  SEM,
  individualPenaltyFunction,
  individualPenaltyFunctionGradient,
  individualPenaltyFunctionHessian,
  currentTuningParameters,
  penaltyFunctionArguments,
  initialHessian,
  stepSize,
  sig,
  gam,
  maxIterOut,
  maxIterIn,
  maxIterLine,
  epsOut,
  epsIn,
  convergenceCriterion,
  verbose = 0
)
}
\arguments{
\item{SEM}{model of class Rcpp_SEMCpp.}

\item{individualPenaltyFunction}{penalty function which takes the current parameter values as first argument, the tuning parameters as second, and the penaltyFunctionArguments as third argument and 
returns a single value - the value of the penalty function for a single person. If the true penalty function is non-differentiable (e.g., lasso) a smooth
approximation of this function should be provided.}

\item{individualPenaltyFunctionGradient}{gradients of the penalty function. Function should take the current parameter values as first argument, the tuning parameters as second, and the penaltyFunctionArguments as third argument and 
return a vector of the same length as parameters. If the true penalty function is non-differentiable (e.g., lasso) a smooth
approximation of this function should be provided.}

\item{individualPenaltyFunctionHessian}{Hessian of the penalty function. Function should take the current parameter values as first argument, the tuning parameters as second, and the penaltyFunctionArguments as third argument and 
return a matrix with (length as parameters)^2 number of elements. If the true penalty function is non-differentiable (e.g., lasso) a smooth
approximation of this function should be provided.}

\item{currentTuningParameters}{data.frame with tuning parameter values.}

\item{penaltyFunctionArguments}{arguments passed to individualPenaltyFunction, individualPenaltyFunctionGradient, and individualPenaltyFunctionHessian}

\item{initialHessian}{option to provide an initial Hessian to the optimizer}

\item{stepSize}{Initial stepSize of the outer iteration (theta_{k+1} = theta_k + stepSize * Stepdirection)}

\item{sig}{only relevant when lineSearch = 'GLMNET'. Controls the sigma parameter in Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). An improved GLMNET for l1-regularized logistic regression. The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421.}

\item{gam}{Controls the gamma parameter in Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). An improved GLMNET for l1-regularized logistic regression. The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421. Defaults to 0.}

\item{maxIterOut}{Maximal number of outer iterations}

\item{maxIterIn}{Maximal number of inner iterations}

\item{maxIterLine}{Maximal number of iterations for the line search procedure}

\item{epsOut}{Stopping criterion for outer iterations}

\item{epsIn}{Stopping criterion for inner iterations}

\item{convergenceCriterion}{which convergence criterion should be used for the outer iterations? possible are "GLMNET", "gradients", "fitChange"}

\item{verbose}{0 prints no additional information, > 0 prints GLMNET iterations}
}
\description{
Performs quasi-Newton optimization with aCV4SEM:::BFGS approximated Hessian
}
