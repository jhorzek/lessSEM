% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/customRegularization.R
\name{addCappedL1}
\alias{addCappedL1}
\title{addCappedL1}
\usage{
addCappedL1(mixedPenalty, regularized, lambdas, thetas)
}
\arguments{
\item{mixedPenalty}{model of class mixedPenalty created with the mixedPenalty function (see ?mixedPenalty)}

\item{regularized}{vector with names of parameters which are to be regularized.
If you are unsure what these parameters are called, use
getLavaanParameters(model) with your lavaan model object}

\item{lambdas}{numeric vector: values for the tuning parameter lambda}

\item{thetas}{parameters whose absolute value is above this threshold will be penalized with
a constant (theta)}
}
\value{
Model of class mixedPenalty. Use the fit() - function to fit the model
}
\description{
Implements cappedL1 regularization for structural equation models.
The penalty function is given by:
\deqn{p( x_j) = \lambda \min(| x_j|, \theta)}
where \eqn{\theta > 0}. The cappedL1 penalty is identical to the lasso for
parameters which are below \eqn{\theta} and identical to a constant for parameters
above \eqn{\theta}. As adding a constant to the fitting function will not change its
minimum, larger parameters can stay unregularized while smaller ones are set to zero.
}
\details{
Identical to \pkg{regsem}, models are specified using \pkg{lavaan}. Currently,
most standard SEM are supported. \pkg{lessSEM} also provides full information
maximum likelihood for missing data. To use this functionality,
fit your \pkg{lavaan} model with the argument \code{sem(..., missing = 'ml')}.
\pkg{lessSEM} will then automatically switch to full information maximum likelihood
as well.

CappedL1 regularization:
\itemize{
\item Zhang, T. (2010). Analysis of Multi-stage Convex Relaxation for Sparse Regularization.
Journal of Machine Learning Research, 11, 1081–1107.
}

Regularized SEM
\itemize{
\item Huang, P.-H., Chen, H., & Weng, L.-J. (2017). A Penalized Likelihood Method for Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9
\item Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural
Equation Modeling: A Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793
}

For more details on ISTA, see:
\itemize{
\item Beck, A., & Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1),
183–202. https://doi.org/10.1137/080716542
\item Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013).
A General Iterative Shrinkage and Thresholding Algorithm for Non-convex
Regularized Optimization Problems. Proceedings of the 30th International
Conference on Machine Learning, 28(2)(2), 37–45.
\item Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations and
Trends in Optimization, 1(3), 123–231.
}
}
