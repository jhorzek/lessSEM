% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/gpOptimizationInterface.R
\name{gpMcp}
\alias{gpMcp}
\title{gpMcp}
\usage{
gpMcp(
  par,
  fn,
  gr = NULL,
  additionalArguments = list(),
  regularized,
  lambdas,
  thetas,
  control = controlIsta()
)
}
\description{
Implements mcp regularization for general purpose optimization problems.
The penalty function is given by:
\deqn{p( x_j) = \begin{cases}
\lambda |x_j| - x_j^2/(2\theta) & \text{if } |x_j| \leq \theta\lambda\\
\theta\lambda^2/2 & \text{if } |x_j| > \lambda\theta
\end{cases}}
where \eqn{\theta > 0}.
}
\details{
The interface is similar to that of optim. Users have to supply a vector 
with starting values (important: This vector _must_ have labels) and a fitting
function. This fitting functions _must_ take exactly 3 arguments:

1. A vector with current parameter estimates
2. A vector with labels of parameters
3. Any additional arguments used by the function contained in a single list
(called additionalArguments below)

The gradient function gr is optional. If set to NULL, the \pkg{numDeriv} package
will be used to approximate the gradients. Supplying a gradient function
can result in considerable speed improvements.  

mcp regularization:

* Zhang, C.-H. (2010). Nearly unbiased variable selection under minimax concave penalty. 
The Annals of Statistics, 38(2), 894–942. https://doi.org/10.1214/09-AOS729
 
For more details on GLMNET, see:

* Friedman, J., Hastie, T., & Tibshirani, R. (2010). 
Regularization Paths for Generalized Linear Models via Coordinate Descent. 
Journal of Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01
* Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010).
A Comparison of Optimization Methods and Software for Large-scale 
L1-regularized Linear Classiﬁcation. Journal of Machine Learning Research, 11, 3183–3234.
* Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). 
An improved GLMNET for l1-regularized logistic regression. 
The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421

For more details on ISTA, see:

* Beck, A., & Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding 
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1),
183–202. https://doi.org/10.1137/080716542
* Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). 
A General Iterative Shrinkage and Thresholding Algorithm for Non-convex 
Regularized Optimization Problems. Proceedings of the 30th International 
Conference on Machine Learning, 28(2)(2), 37–45.
* Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations and 
Trends in Optimization, 1(3), 123–231.
}
