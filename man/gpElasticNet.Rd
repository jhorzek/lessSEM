% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/gpOptimizationInterface.R
\name{gpElasticNet}
\alias{gpElasticNet}
\title{gpElasticNet}
\usage{
gpElasticNet(
  par,
  weights,
  fn,
  gr = NULL,
  lambdas = NULL,
  nLambdas = NULL,
  alphas,
  additionalArguments,
  method = "ista",
  control = controlIsta()
)
}
\arguments{
\item{par}{labeled vector with starting values}

\item{weights}{labeled vector with weights for each of the parameters in the
model.}

\item{fn}{R function which takes the parameters AND their labels
as input and returns the fit value (a single value)}

\item{gr}{R function which takes the parameters AND their labels
as input and returns the gradients of the objective function.
If set to NULL, numDeriv will be used to approximate the gradients}

\item{lambdas}{numeric vector: values for the tuning parameter lambda}

\item{nLambdas}{alternative to lambda: If alpha = 1, lessSEM can automatically
compute the first lambda value which sets all regularized parameters to zero.
It will then generate nLambda values between 0 and the computed lambda.}

\item{alphas}{numeric vector with values of the tuning parameter alpha. Must be
in \link{0,1}. 0 = ridge, 1 = lasso.}

\item{additionalArguments}{additional argument passed to fn and gr}

\item{method}{which optimizer should be used? Currently implemented are ista
and glmnet. With ista, the control argument can be used to switch to related procedures
(currently gist).}

\item{control}{used to control the optimizer. This element is generated with
the controlIsta() and controlGlmnet() functions.}
}
\description{
Implements elastic net regularization for general purpose optimization problems.
The penalty function is given by:
\deqn{p( x_j) = p( x_j) = \frac{1}{w_j}\lambda| x_j|}
Note that the elastic net combines ridge and lasso regularization. If \eqn{\alpha = 0},
the elastic net reduces to ridge regularization. If \eqn{\alpha = 1} it reduces
to lasso regularization. In between, elastic net is a compromise between the shrinkage of
the lasso and the ridge penalty.
}
\details{
The interface is similar to that of optim. Users have to supply a vector
with starting values (important: This vector \emph{must} have labels) and a fitting
function. This fitting functions \emph{must} take exactly 3 arguments:
\enumerate{
\item A vector with current parameter estimates
\item A vector with labels of parameters
\item Any additional arguments used by the function contained in a single list
(called additionalArguments below)
}

The gradient function gr is optional. If set to NULL, the \pkg{numDeriv} package
will be used to approximate the gradients. Supplying a gradient function
can result in considerable speed improvements.

Elastic net regularization:
\itemize{
\item Zou, H., & Hastie, T. (2005). Regularization and variable selection via the elastic net.
Journal of the Royal Statistical Society: Series B, 67(2), 301–320. https://doi.org/10.1111/j.1467-9868.2005.00503.x
}

For more details on GLMNET, see:
\itemize{
\item Friedman, J., Hastie, T., & Tibshirani, R. (2010).
Regularization Paths for Generalized Linear Models via Coordinate Descent.
Journal of Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01
\item Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010).
A Comparison of Optimization Methods and Software for Large-scale
L1-regularized Linear Classiﬁcation. Journal of Machine Learning Research, 11, 3183–3234.
\item Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012).
An improved GLMNET for l1-regularized logistic regression.
The Journal of Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421
}

For more details on ISTA, see:
\itemize{
\item Beck, A., & Teboulle, M. (2009). A Fast Iterative Shrinkage-Thresholding
Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1),
183–202. https://doi.org/10.1137/080716542
\item Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013).
A General Iterative Shrinkage and Thresholding Algorithm for Non-convex
Regularized Optimization Problems. Proceedings of the 30th International
Conference on Machine Learning, 28(2)(2), 37–45.
\item Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations and
Trends in Optimization, 1(3), 123–231.
}
}
\examples{
# This example shows how to use the optimizers
# for other objective functions. We will use
# a linear regression as an example. Note that
# this is not a useful application of the optimizers
# as there are specialized packages for linear regression
# (e.g., glmnet)

library(lessSEM)
set.seed(123)

# first, we simulate data for our
# linear regression. The model is given by
# y = b1*x1 + b2*x2 + b3*x3 + b4*x4 + b5*x5 
n <- 100
df <- data.frame(
  x1 = rnorm(n),
  x2 = rnorm(n),
  x3 = rnorm(n),
  x4 = rnorm(n),
  x5 = rnorm(n)
)
df$y <- 0*df$x1 + .2*df$x2 + .3*df$x3 + .4*df$x4 + .5*df$x5 + rnorm(n,0,.3) 

# First, we must construct a fiting function
# which returns a single value. We will use
# the residual sum squared as fitting function.
# The optimizer expects that this function takes
# EXACTLY three arguments:
# 1) a vector with parameter values (par)
# 2) a vector with labels for these parameters (parameterLabels)
# 3) an additional argument which allows for passing anything
# else your function needs to run to the function.

# Let's start setting up the fitting function:
fittingFunction <- function(par, parameterLabels, additionalArguments){
  # Our function needs the observed data 
  # y and X. These are not part of the first two arguments
  # and must therefore be passed in the function with the 
  # third argument (additionalArguments). Here, we will use a list
  # and pass the arguments in through that list:
  pred <- additionalArguments$X \%*\% matrix(par, ncol = 1)
  sse <- sum((additionalArguments$y - pred)^2)
  return(sse)
}
# Now we need to construct the list additionalArguments used
# by fittingFunction
additionalArguments <- list(X = as.matrix(df[,grepl("x", colnames(df))]), 
                            y = df$y)


# let's define the starting values:
b <- rep(0,5)
names(b) <- paste0("b",1:5)
# and the weights used by the elastic net
weights <- b
weights[] <- 1

# optimize
enet <- gpElasticNet(
  par = b, 
  weights = weights, 
  fn = fittingFunction, 
  lambdas = seq(0,100,1), 
  alphas = 1, 
  additionalArguments = additionalArguments
)
plot(enet)
AIC(enet)
}
