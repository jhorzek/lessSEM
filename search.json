[{"path":"/articles/Definition-Variables-and-Multi-Group-SEM.html","id":"first-step-setting-up-a-multi-group-model","dir":"Articles","previous_headings":"","what":"First Step: Setting up a Multi-Group Model","title":"Definition-Variables-and-Multi-Group-SEM","text":"set multi-group model lessSEM, first fit separate models groups lavaan:","code":"library(lavaan)  # For simplicity, we will use a subset of the Holzinger Swineford data set  # which is also used at https://lavaan.ugent.be/tutorial/groups.html # to demonstrate multi-group SEM  # To use mutli-group SEM in lessSEM, we have to set up a separate model # for each of the groups:  # - Pasteur: Children attending the Pasteur school # - Grant_White: Children attending the Grant-White school data(HolzingerSwineford1939)  ## Pasteur ## Pasteur <- subset(HolzingerSwineford1939, school == \"Pasteur\")  model_Pasteur <- paste0('      visual  =~ l1_Pasteur*x1 + l2_Pasteur*x2 + l3_Pasteur*x3     x1 ~~ v1*x1     x2 ~~ v2*x2     x3 ~~ v3*x3          visual ~~ lv1*visual     x1 ~ m1*1     x2 ~ m2*1     x3 ~ m3*1') fit_Pasteur <- sem(model = model_Pasteur,                     data = Pasteur,                     std.lv = TRUE)  ## Grant-White Grant_White <- subset(HolzingerSwineford1939, school == \"Grant-White\")  model_Grant_White <- paste0('      visual  =~ l1_Grant_White*x1 + l2_Grant_White*x2 + l3_Grant_White*x3     x1 ~~ v1*x1     x2 ~~ v2*x2     x3 ~~ v3*x3          visual ~~ lv1*visual     x1 ~ m1*1     x2 ~ m2*1     x3 ~ m3*1') fit_Grant_White <- sem(model = model_Grant_White,                         data = Grant_White,                         std.lv = TRUE)"},{"path":"/articles/Definition-Variables-and-Multi-Group-SEM.html","id":"second-step-pass-the-model-to-lesssem","dir":"Articles","previous_headings":"","what":"Second Step: Pass the Model to lessSEM","title":"Definition-Variables-and-Multi-Group-SEM","text":"Now group-specific models, can pass lessSEM: Let’s look parameters: ’s curious! group-specific parameters, parameters provided group-specific names! important: Important: set multi-group model lessSEM, lessSEM assume parameters names also values. includes parameters may estimated, names provided lavaan (e.g., variances).","code":"library(lessSEM)  # We will just estimate the parameters using the BFGS optimizer without any  # regularization. # Note that we pass the two models as a vector. This is very important: lessSEM # will then set up the multi-group model fit <- bfgs(lavaanModel = c(fit_Pasteur, fit_Grant_White)) coef(fit) #>                                                                                #>   Tuning         ||--||  Estimates                                             #>  ------- ------- ||--|| ---------- ---------- ---------- ---------- ---------- #>   lambda   alpha ||--|| l1_Pasteur l2_Pasteur l3_Pasteur         v1         v2 #>  ======= ======= ||--|| ========== ========== ========== ========== ========== #>   0.0000  0.0000 ||--||     0.7240     0.5610     0.8824     0.8449     1.0711 #>                                                                            #>                                                                            #>  ---------- ---------- ---------- ---------- -------------- -------------- #>          v3         m1         m2         m3 l1_Grant_White l2_Grant_White #>  ========== ========== ========== ========== ============== ============== #>      0.6108     4.9212     6.0770     2.2281         0.7088         0.5536 #>                 #>                 #>  -------------- #>  l3_Grant_White #>  ============== #>          0.7360"},{"path":"/articles/Definition-Variables-and-Multi-Group-SEM.html","id":"different-models-with-shared-parameter-labels","dir":"Articles","previous_headings":"Second Step: Pass the Model to lessSEM","what":"Different Models with Shared Parameter Labels","title":"Definition-Variables-and-Multi-Group-SEM","text":"parameters labels multiple models constrained equality across models! careful, can result annoying mistakes. demonstrate , use two different models may share parameter names. Let’s first compare fit separate models multi-group model. lessSEM estimate models truly separately expect fit : Obviously, two fits . may happened? Looking parameter estimates multi-group model shows two models, fitPolDem fitHS share parameter labels! instance, intercepts x1 called x1~1 models. Therefore, lessSEM assumed wanted parameters exactly value models.","code":"# Model from ?lavaan::sem model <- '    # latent variable definitions      ind60 =~ x1 + x2 + x3      dem60 =~ y1 + a*y2 + b*y3 + c*y4      dem65 =~ y5 + a*y6 + b*y7 + c*y8    # regressions     dem60 ~ ind60     dem65 ~ ind60 + dem60    # residual correlations     y1 ~~ y5     y2 ~~ y4 + y6     y3 ~~ y7     y4 ~~ y8     y6 ~~ y8 '  fitPolDem <- sem(model,                   data = PoliticalDemocracy,                  meanstructure = TRUE)  # Model from ?lavaan::cfa HS.model <- ' visual  =~ x1 + x2 + x3               textual =~ x4 + x5 + x6               speed   =~ x7 + x8 + x9 '  fitHS <- cfa(HS.model,              data = HolzingerSwineford1939,              meanstructure = TRUE)  ## lessSEM does not care if the models passed to the function are similar  # or even use different data sets. Of course, it probably does not make much sense # to estimate two different models at the same time, but lessSEM won't stop you # from trying... fit <- bfgs(lavaanModel = c(fitPolDem, fitHS)) # fit for separate models -2*logLik(fitPolDem) + (-2)*logLik(fitHS) #> 'log Lik.' 10573.13 (df=39)  # fit for multi-group model: fit@fits$m2LL #> [1] 10813.96 coef(fit) #>                                                                                #>   Tuning         ||--||  Estimates                                             #>  ------- ------- ||--|| ---------- ---------- ---------- ---------- ---------- #>   lambda   alpha ||--||  ind60=~x2  ind60=~x3          a          b          c #>  ======= ======= ||--|| ========== ========== ========== ========== ========== #>   0.0000  0.0000 ||--||     1.8164     1.5582     1.1879     1.1706     1.2511 #>                                                                       #>                                                                       #>  ----------- ----------- ----------- ---------- ---------- ---------- #>  dem60~ind60 dem65~ind60 dem65~dem60     y1~~y5     y2~~y4     y2~~y6 #>  =========== =========== =========== ========== ========== ========== #>       1.4062      0.4696      0.8755     0.5389     1.4270     2.2120 #>                                                                               #>                                                                               #>  ---------- ---------- ---------- ---------- ---------- ---------- ---------- #>      y3~~y7     y4~~y8     y6~~y8     x1~~x1     x2~~x2     x3~~x3     y1~~y1 #>  ========== ========== ========== ========== ========== ========== ========== #>      0.7425     0.3718     1.3734     0.0000     1.3895     1.2086     1.8544 #>                                                                               #>                                                                               #>  ---------- ---------- ---------- ---------- ---------- ---------- ---------- #>      y2~~y2     y3~~y3     y4~~y4     y5~~y5     y6~~y6     y7~~y7     y8~~y8 #>  ========== ========== ========== ========== ========== ========== ========== #>      7.5981     4.9592     3.2006     2.2664     4.9911     3.6046     3.3135 #>                                                                          #>                                                                          #>  ------------ ------------ ------------ ---------- ---------- ---------- #>  ind60~~ind60 dem60~~dem60 dem65~~dem65       x1~1       x2~1       x3~1 #>  ============ ============ ============ ========== ========== ========== #>        0.5305       3.8021       0.2003     5.0409     5.8482     2.5445 #>                                                                               #>                                                                               #>  ---------- ---------- ---------- ---------- ---------- ---------- ---------- #>        y1~1       y2~1       y3~1       y4~1       y5~1       y6~1       y7~1 #>  ========== ========== ========== ========== ========== ========== ========== #>      5.4463     4.2330     6.5420     4.4295     5.1139     2.9502     6.1703 #>                                                                                 #>                                                                                 #>  ---------- ---------- ---------- ----------- ----------- ---------- ---------- #>        y8~1 visual=~x2 visual=~x3 textual=~x5 textual=~x6  speed=~x8  speed=~x9 #>  ========== ========== ========== =========== =========== ========== ========== #>      4.0150     0.2791     0.4461      1.1126      0.9221     1.1746     1.0056 #>                                                                    #>                                                                    #>  ---------- ---------- ---------- ---------- ---------- ---------- #>      x4~~x4     x5~~x5     x6~~x6     x7~~x7     x8~~x8     x9~~x9 #>  ========== ========== ========== ========== ========== ========== #>      0.3681     0.4434     0.3610     0.7750     0.4590     0.6025 #>                                                                             #>                                                                             #>  -------------- ---------------- ------------ --------------- ------------- #>  visual~~visual textual~~textual speed~~speed visual~~textual visual~~speed #>  ============== ================ ============ =============== ============= #>          1.3695           0.9839       0.4084          0.4666        0.2615 #>                                                                        #>                                                                        #>  -------------- ---------- ---------- ---------- ---------- ---------- #>  textual~~speed       x4~1       x5~1       x6~1       x7~1       x8~1 #>  ============== ========== ========== ========== ========== ========== #>          0.1746     3.0967     4.3804     2.2186     4.2060     5.5507 #>             #>             #>  ---------- #>        x9~1 #>  ========== #>      5.3943"},{"path":"/articles/Definition-Variables-and-Multi-Group-SEM.html","id":"regularized-multi-group-models","dir":"Articles","previous_headings":"","what":"Regularized Multi-Group Models","title":"Definition-Variables-and-Multi-Group-SEM","text":"multi-group models can regularized similar standard SEM: Instead using bfgs-function, use (instance), lasso-function: coefficients can extracted usual:","code":"fit <- lasso(lavaanModel = c(fit_Pasteur, fit_Grant_White),              regularized = c(\"l1_Pasteur\"),               nLambdas = 20) coef(fit, criterion = \"AIC\") #>                                                                                #>   Tuning         ||--||  Estimates                                             #>  ------- ------- ||--|| ---------- ---------- ---------- ---------- ---------- #>   lambda   alpha ||--|| l1_Pasteur l2_Pasteur l3_Pasteur         v1         v2 #>  ======= ======= ||--|| ========== ========== ========== ========== ========== #>   0.0000  1.0000 ||--||     0.7241     0.5611     0.8822     0.8447     1.0710 #>                                                                            #>                                                                            #>  ---------- ---------- ---------- ---------- -------------- -------------- #>          v3         m1         m2         m3 l1_Grant_White l2_Grant_White #>  ========== ========== ========== ========== ============== ============== #>      0.6110     4.9211     6.0768     2.2279         0.7089         0.5537 #>                 #>                 #>  -------------- #>  l3_Grant_White #>  ============== #>          0.7358"},{"path":"/articles/Definition-Variables-and-Multi-Group-SEM.html","id":"regularizing-differences-between-parameters-using-lesssem","dir":"Articles","previous_headings":"Regularized Multi-Group Models","what":"Regularizing Differences Between Parameters using lessSEM","title":"Definition-Variables-and-Multi-Group-SEM","text":"regularized multi-group models shine automatically testing group-differences. proposed Huang (2018) provides convenient way decide parameters group-specific. end, differences parameters must regularized. Say, interested loading l1 wonder indeed need separate loadings students attending Pasteur school (l1_Pasteur) Grant-White school (l1_Grant_White). Using Pasteur school baseline group (see Huang, 2018, details), can define l1_Grant_White = l1_Pasteur + l1_delta, l1_delta difference two schools. l1_delta zero, schools loading (.e., measurement invariance). Within lessSEM, can regularize differences using transformations (see vignette(topic = \"Parameter-transformations\", package = \"lessSEM\") details). Therefore, first step define transformation: Next, pass transformation model: Now, let’s look parameter estimates: l1_delta parameter set zero, can assume measurement invariance. Note won’t find l1_Grant_White parameters model. l1_Grant_White deterministic function actual parameters l1_Pasteur l1_delta. want find value l1_Grant_White, look : Note lslx (Huang, 2020) supports different penalties delta parameter (l1_delta) baseline parameter (l1_Pasteur). currently supported lessSEM.","code":"transformation <- \" parameters: l1_Pasteur, l1_Grant_White, l1_delta l1_Grant_White = l1_Pasteur + l1_delta; \" fit <- lasso(lavaanModel = c(fit_Pasteur, fit_Grant_White),              regularized = c(\"l1_delta\"), # we want to regularize the difference!              nLambdas = 20,              modifyModel = modifyModel(transformations = transformation)) coef(fit, criterion = \"AIC\")@estimates[,c(\"l1_Pasteur\", \"l1_delta\")] #> l1_Pasteur   l1_delta  #>   0.716718   0.000000 fit@transformations #>          lambda alpha l1_Grant_White #> 1  0.0056517504     1      0.7167576 #> 2  0.0053542898     1      0.7167180 #> 3  0.0050568293     1      0.7166009 #> 4  0.0047593687     1      0.7161368 #> 5  0.0044619082     1      0.7155761 #> 6  0.0041644476     1      0.7151688 #> 7  0.0038669871     1      0.7147807 #> 8  0.0035695265     1      0.7141878 #> 9  0.0032720660     1      0.7137695 #> 10 0.0029746054     1      0.7132645 #> 11 0.0026771449     1      0.7128818 #> 12 0.0023796844     1      0.7125411 #> 13 0.0020822238     1      0.7120493 #> 14 0.0017847633     1      0.7115889 #> 15 0.0014873027     1      0.7110626 #> 16 0.0011898422     1      0.7107554 #> 17 0.0008923816     1      0.7102373 #> 18 0.0005949211     1      0.7097089 #> 19 0.0002974605     1      0.7094042 #> 20 0.0000000000     1      0.7088852"},{"path":"/articles/Definition-Variables-and-Multi-Group-SEM.html","id":"cross-validation","dir":"Articles","previous_headings":"","what":"Cross-Validation","title":"Definition-Variables-and-Multi-Group-SEM","text":"Automatic cross-validation multi-group models , instance, cvLasso yet implemented. can difficult decide split data set submodel. want use cross-validation, unfortunately set procedure manually.","code":""},{"path":"/articles/Definition-Variables-and-Multi-Group-SEM.html","id":"definition-variables","dir":"Articles","previous_headings":"","what":"Definition Variables","title":"Definition-Variables-and-Multi-Group-SEM","text":"Models definition variables basically multi-group models, sole exception group-specific parameters estimated fixed specific values. main interest setting multi-group SEM lessSEM don’t care details, lessTemplates package (https://github.com/jhorzek/lessTemplates) provides means easily set models (see SEMWithDefinitionVariables function lessTemplates). following, look detail definition variables can used lessSEM","code":""},{"path":"/articles/Definition-Variables-and-Multi-Group-SEM.html","id":"the-details","dir":"Articles","previous_headings":"Definition Variables","what":"The details …","title":"Definition-Variables-and-Multi-Group-SEM","text":"Unfortunately, lavaan allow us set models \\(N=1\\), however. required many definition variable applications, latent growth curve models subject-specific measurement occasions. following, use workaround. Let’s first simulate data: Note times random subject-specific. need separate model subject. lavaan won’t let us set models, instead set models using entire data set replace data post-hoc. Exemplarily, makes sense look one models: Note loadings slope fixed time points person provided data. Now can pass models lessSEM: parameters given :","code":"#### Population parameters #### intercept_mu <- 0 intercept_sigma <- 1 slope_mu <- .3 slope_sigma <- 1  #### data set #### N <- 50 intercepts <- rnorm(n = N,                      mean = intercept_mu,                      sd = intercept_sigma) slopes <- rnorm(n = N,                  mean = slope_mu,                  sd = slope_sigma) times <- matrix(seq(0,5,1),                 nrow = N,                 ncol = 6,                 byrow = TRUE) +   cbind(0,matrix(round(runif(n = N*5, min = -.2,max = .2),2),                  nrow = N,                  ncol = 5,                  byrow = TRUE)) # we add some jitter to make the times person-specific  lgcData <- matrix(NA, nrow = N, ncol = ncol(times), dimnames = list(NULL, paste0(\"x\", 0:5)))  for(i in 1:N){   lgcData[i,] <- intercepts[i] + times[i,]* slopes[i] + rnorm(ncol(lgcData),0,.3) } lgcData <- as.data.frame(lgcData)  head(lgcData) #>            x0         x1         x2        x3        x4        x5 #> 1  0.26801876 -0.5353028 -1.0262295 -2.533564 -2.890246 -4.655061 #> 2  0.26260181 -0.4802635 -2.0790754 -2.686983 -3.677022 -4.540501 #> 3 -0.71054447 -0.5477048 -0.5747618 -1.231821 -1.229388 -1.418401 #> 4  0.14564354  1.0690979  1.8919916  2.842025  2.953471  4.654879 #> 5  0.09501816  1.6350899  2.6331440  4.729328  5.423381  6.799296 #> 6  2.51013193  3.0917904  4.1193617  5.749542  6.222599  7.716919  head(times) #>      [,1] [,2] [,3] [,4] [,5] [,6] #> [1,]    0 0.94 2.13 3.01 3.91 5.12 #> [2,]    0 0.84 2.13 2.91 4.10 5.19 #> [3,]    0 0.83 2.14 3.12 3.95 4.93 #> [4,]    0 0.88 2.03 3.16 4.01 5.03 #> [5,]    0 1.07 2.01 3.00 3.81 4.82 #> [6,]    0 1.17 2.11 2.88 4.06 5.06 models <- c() for(i in 1:N){ model_i <- paste0(   \" int =~ 1*x0 + 1*x1 + 1*x2 + 1*x3 + 1*x4 + 1*x5 slope =~ \",times[i,1],\"*x0 +           \",times[i,2],\"*x1 +           \",times[i,3],\"*x2 +            \",times[i,4],\"*x3 +           \",times[i,5],\"*x4 +           \",times[i,6],\"*x5                    int ~ intMean*1          slope ~ slopeMean*1                    int ~~ intVar*int + 0*slope          slope ~~ slopeVar*slope  x0 ~~ v*x0 x1 ~~ v*x1 x2 ~~ v*x2 x3 ~~ v*x3 x4 ~~ v*x4 x5 ~~ v*x5  x0 ~ 0*1 x1 ~ 0*1 x2 ~ 0*1 x3 ~ 0*1 x4 ~ 0*1 x5 ~ 0*1 \"   )  fit_i <- sem(model = model_i,               data = lgcData,               do.fit = FALSE) internalData <- lavInspect(fit_i, \"data\") # replace the data set fit_i@Data@X[[1]] <- as.matrix(lgcData[i,colnames(internalData),drop = FALSE])  models <- c(models,              fit_i) } cat(model_i) #>  #> int =~ 1*x0 + 1*x1 + 1*x2 + 1*x3 + 1*x4 + 1*x5 #> slope =~ 0*x0 +  #>          1.13*x1 +  #>          1.81*x2 +   #>          3.06*x3 +  #>          4.12*x4 +  #>          4.98*x5 #>           #>          int ~ intMean*1 #>          slope ~ slopeMean*1 #>           #>          int ~~ intVar*int + 0*slope #>          slope ~~ slopeVar*slope #>  #> x0 ~~ v*x0 #> x1 ~~ v*x1 #> x2 ~~ v*x2 #> x3 ~~ v*x3 #> x4 ~~ v*x4 #> x5 ~~ v*x5 #>  #> x0 ~ 0*1 #> x1 ~ 0*1 #> x2 ~ 0*1 #> x3 ~ 0*1 #> x4 ~ 0*1 #> x5 ~ 0*1 fit <- bfgs(lavaanModel = models) coef(fit) #>                                                                                #>   Tuning         ||--||  Estimates                                             #>  ------- ------- ||--|| ---------- ---------- ---------- ---------- ---------- #>   lambda   alpha ||--||    intMean  slopeMean     intVar   slopeVar          v #>  ======= ======= ||--|| ========== ========== ========== ========== ========== #>   0.0000  0.0000 ||--||    -0.0152     0.2921     0.7703     0.9411     0.0851"},{"path":"/articles/Definition-Variables-and-Multi-Group-SEM.html","id":"bibliography","dir":"Articles","previous_headings":"","what":"Bibliography","title":"Definition-Variables-and-Multi-Group-SEM","text":"Huang, P.-H. (2018). penalized likelihood method multi-group structural equation modelling. British Journal Mathematical Statistical Psychology, 71(3), 499–522. https://doi.org/10.1111/bmsp.12130 Huang, P.-H. (2020). lslx: Semi-confirmatory structural equation modeling via penalized likelihood. Journal Statistical Software, 93(7). https://doi.org/10.18637/jss.v093.i07","code":""},{"path":"/articles/General-Purpose-Optimization.html","id":"the-example","dir":"Articles","previous_headings":"","what":"The example","title":"General-Purpose-Optimization","text":"Let’s start setting linear regression model. end, simulate data set:","code":"set.seed(123)  # first, we simulate data for our # linear regression. N <- 100 # number of persons p <- 10 # number of predictors X <- matrix(rnorm(N*p), nrow = N, ncol = p) # design matrix b <- c(rep(1,4),        rep(0,6)) # true regression weights y <- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)"},{"path":"/articles/General-Purpose-Optimization.html","id":"the-first-approach-interfacing-from-r","dir":"Articles","previous_headings":"","what":"The first approach: Interfacing from R","title":"General-Purpose-Optimization","text":"now try implement lasso regularized linear regression using gpLasso interface. interface similar optim. use , must define fitting function R: Additionally, need labeled vector starting values: Note defined one parameter variables X. also want estimate intercept. end, extend X: Finally, need decide parameters regularized values lambda. want regularize everything except intercept: Now, ready estimate model: Note specify gradients function. case, lessSEM use numDeriv compute gradients. However, know specify gradients, can result faster estimation: short comparison running models 5 times : Runtime seconds without gradients: Runtime seconds gradients: Now, ’s quite difference! Note can also pass C++ function gpLasso similar approach : Run model : runtime seconds C++ : even lower !","code":"# defining the sum-squared-errors: sseFun <- function(par, y, X, N){   # par is the parameter vector   # y is the observed dependent variable   # X is the design matrix   # N is the sample size   pred <- X %*% matrix(par, ncol = 1) #be explicit here:   # we need par to be a column vector   sse <- sum((y - pred)^2)   # we scale with .5/N to get the same results as glmnet   return((.5/N)*sse) } par <- rep(0, p+1) names(par) <- paste0(\"b\", 0:p) print(par) #>  b0  b1  b2  b3  b4  b5  b6  b7  b8  b9 b10  #>   0   0   0   0   0   0   0   0   0   0   0 Xext <- cbind(1,X) head(Xext) #>      [,1]        [,2]        [,3]       [,4]       [,5]        [,6]        [,7] #> [1,]    1 -0.56047565 -0.71040656  2.1988103 -0.7152422 -0.07355602 -0.60189285 #> [2,]    1 -0.23017749  0.25688371  1.3124130 -0.7526890 -1.16865142 -0.99369859 #> [3,]    1  1.55870831 -0.24669188 -0.2651451 -0.9385387 -0.63474826  1.02678506 #> [4,]    1  0.07050839 -0.34754260  0.5431941 -1.0525133 -0.02884155  0.75106130 #> [5,]    1  0.12928774 -0.95161857 -0.4143399 -0.4371595  0.67069597 -1.50916654 #> [6,]    1  1.71506499 -0.04502772 -0.4762469  0.3311792 -1.65054654 -0.09514745 #>             [,8]       [,9]      [,10]      [,11] #> [1,]  1.07401226 -0.7282191  0.3562833 -1.0141142 #> [2,] -0.02734697 -1.5404424 -0.6580102 -0.7913139 #> [3,] -0.03333034 -0.6930946  0.8552022  0.2995937 #> [4,] -1.51606762  0.1188494  1.1529362  1.6390519 #> [5,]  0.79038534 -1.3647095  0.2762746  1.0846170 #> [6,] -0.21073418  0.5899827  0.1441047 -0.6245675 (regularized <- paste0(\"b\", 1:p)) #>  [1] \"b1\"  \"b2\"  \"b3\"  \"b4\"  \"b5\"  \"b6\"  \"b7\"  \"b8\"  \"b9\"  \"b10\" lambdas  <- seq(0,.1,length.out = 20) library(lessSEM) l1 <- gpLasso(par = par,                regularized = regularized,                fn = sseFun,                lambdas = lambdas,                X = Xext,               y = y,               N = length(y) ) head(l1@parameters) #>        lambda alpha         b0        b1        b2        b3       b4 #> 1 0.000000000     1 0.02738472 1.0129194 0.9991454 0.9705725 1.027626 #> 2 0.005263158     1 0.02935305 1.0043738 0.9908933 0.9626258 1.025138 #> 3 0.010526316     1 0.02995106 0.9967095 0.9846669 0.9552793 1.021892 #> 4 0.015789474     1 0.03010651 0.9897340 0.9789419 0.9481495 1.018673 #> 5 0.021052632     1 0.03029749 0.9827290 0.9732058 0.9409863 1.015363 #> 6 0.026315789     1 0.03112509 0.9753348 0.9670620 0.9338614 1.011553 #>           b5           b6          b7          b8           b9         b10 #> 1 0.01403601 -0.007460964 0.018589924 0.021930771 -0.009900077 0.027401044 #> 2 0.00336578  0.000000000 0.014341067 0.015434757 -0.007939455 0.022297544 #> 3 0.00000000  0.000000000 0.009622163 0.010707922 -0.005256532 0.017465353 #> 4 0.00000000  0.000000000 0.004932662 0.006363847 -0.002393164 0.012712886 #> 5 0.00000000  0.000000000 0.000177806 0.002036023  0.000000000 0.007969732 #> 6 0.00000000  0.000000000 0.000000000 0.000000000  0.000000000 0.003303750 sseGrad <- function(par, y, X, N){      gradients = (-2.0*t(X) %*% y + 2.0*t(X)%*%X%*%matrix(par,ncol = 1))      gradients = (.5/length(y))*gradients   return(t(gradients)) } l1 <- gpLasso(par = par,                regularized = regularized,                fn = sseFun,                gr = sseGrad,               lambdas = lambdas,                X = Xext,               y = y,               N = length(y) ) head(l1@parameters) #>        lambda alpha         b0        b1        b2        b3       b4 #> 1 0.000000000     1 0.02738485 1.0129200 0.9991452 0.9705725 1.027626 #> 2 0.005263158     1 0.02935325 1.0043732 0.9908928 0.9626258 1.025139 #> 3 0.010526316     1 0.02995023 0.9967094 0.9846669 0.9552792 1.021892 #> 4 0.015789474     1 0.03010649 0.9897330 0.9789426 0.9481493 1.018672 #> 5 0.021052632     1 0.03029729 0.9827286 0.9732062 0.9409869 1.015362 #> 6 0.026315789     1 0.03112481 0.9753368 0.9670620 0.9338616 1.011553 #>            b5           b6           b7          b8           b9         b10 #> 1 0.014034994 -0.007460252 0.0185901898 0.021930702 -0.009900699 0.027400748 #> 2 0.003364951  0.000000000 0.0143418480 0.015434447 -0.007939640 0.022297136 #> 3 0.000000000  0.000000000 0.0096217573 0.010707383 -0.005257048 0.017465134 #> 4 0.000000000  0.000000000 0.0049332838 0.006363868 -0.002393522 0.012713116 #> 5 0.000000000  0.000000000 0.0001772169 0.002036300  0.000000000 0.007969996 #> 6 0.000000000  0.000000000 0.0000000000 0.000000000  0.000000000 0.003303777 #> [1] 0.2893515 0.2759173 0.2746017 0.2737622 0.2745516 #> [1] 0.02399015 0.02863860 0.02363658 0.02460742 0.02580047 library(RcppArmadillo) library(Rcpp) linreg <- ' // [[Rcpp::depends(RcppArmadillo)]] #include <RcppArmadillo.h>  // [[Rcpp::export]] double fitfunction(const arma::colvec parameters, const arma::mat X, const arma::colvec y, const int N){      // compute the sum of squared errors:     arma::mat sse = arma::trans(y-X*parameters)*(y-X*parameters);          // other packages, such as glmnet, scale the sse with      // 1/(2*N), where N is the sample size. We will do that here as well          sse *= 1.0/(2.0 * N);          // note: We must return a double, but the sse is a matrix     // To get a double, just return the single value that is in      // this matrix:       return(sse(0,0)); }  // [[Rcpp::export]] arma::rowvec gradientfunction(const arma::colvec parameters, const arma::mat X, const arma::colvec y, const int N){      // note: we want to return our gradients as row-vector; therefore,   // we have to transpose the resulting column-vector:     arma::rowvec gradients = arma::trans(-2.0*X.t() * y + 2.0*X.t()*X*parameters);          // other packages, such as glmnet, scale the sse with      // 1/(2*N), where N is the sample size. We will do that here as well          gradients *= (.5/N);          return(gradients); }'  Rcpp::sourceCpp(code = linreg) l1 <- gpLasso(par = par,                regularized = regularized,                fn = fitfunction,                gr = gradientfunction,               lambdas = lambdas,                X = Xext,               y = y,               N = length(y) ) head(l1@parameters) #>        lambda alpha         b0        b1        b2        b3       b4 #> 1 0.000000000     1 0.02738540 1.0129199 0.9991451 0.9705723 1.027625 #> 2 0.005263158     1 0.02935207 1.0043733 0.9908930 0.9626262 1.025139 #> 3 0.010526316     1 0.02994985 0.9967091 0.9846669 0.9552794 1.021892 #> 4 0.015789474     1 0.03010681 0.9897326 0.9789426 0.9481493 1.018672 #> 5 0.021052632     1 0.03029735 0.9827286 0.9732060 0.9409868 1.015363 #> 6 0.026315789     1 0.03112467 0.9753370 0.9670622 0.9338616 1.011553 #>            b5           b6           b7          b8           b9         b10 #> 1 0.014036116 -0.007460446 0.0185897976 0.021930567 -0.009900272 0.027401106 #> 2 0.003365951  0.000000000 0.0143415907 0.015435060 -0.007939728 0.022297282 #> 3 0.000000000  0.000000000 0.0096214449 0.010707751 -0.005256894 0.017464995 #> 4 0.000000000  0.000000000 0.0049330966 0.006364175 -0.002393745 0.012713250 #> 5 0.000000000  0.000000000 0.0001769795 0.002036461  0.000000000 0.007970247 #> 6 0.000000000  0.000000000 0.0000000000 0.000000000  0.000000000 0.003304510 #> [1] 0.02534795 0.02021194 0.01996326 0.02018714 0.02033544"},{"path":"/articles/General-Purpose-Optimization.html","id":"the-second-approach-using-c-function-pointers","dir":"Articles","previous_headings":"","what":"The second approach: Using C++ function pointers","title":"General-Purpose-Optimization","text":"using Rcpp functions defined quite fast linear regression, can still fairly slow involved models (e.g., SEM). due optimizer go back forth R C++. reduce overhead, can use second approach. , instead passing Rcpp function executed R, pass pointer underlying C++ functions. approach constrained one presented : must define , fitting function gradient function Rcpp. rely numDeriv ! fitting function gradient function allowed two parameters : const Rcpp::NumericVector& (parameters) Rcpp::List& (everything else). seems restrictive, note can virtually pass anything want list. must create pointers fit gradient function. difficult, however provide guidance . may bit overwhelming first, go step step.","code":""},{"path":"/articles/General-Purpose-Optimization.html","id":"creating-a-fitting-function-and-a-gradient-function","dir":"Articles","previous_headings":"The second approach: Using C++ function pointers","what":"1. Creating a fitting function and a gradient function","title":"General-Purpose-Optimization","text":"already defined fitting function gradient function linear regression model example . However, often know gradients closed form. don’t gradient function, can try numerical approximation. details can found .","code":""},{"path":"/articles/General-Purpose-Optimization.html","id":"adapting-the-functions-to-the-constraints","dir":"Articles","previous_headings":"The second approach: Using C++ function pointers","what":"2. Adapting the functions to the constraints","title":"General-Purpose-Optimization","text":"Note fitting function gradient function comply constraints mentioned . , take two parameters arguments (const arma::colvec parameters, const arma::mat X, const arma::colvec y, const int N), arguments const Rcpp::NumericVector& Rcpp::List&. can make work? parameter vector const Rcpp::NumericVector& hold elements arma::colvec pararameters old function. Rcpp::List& must contain elements (X,y,N). Let’s start creating list, call data: Next, change functions make things work: ’s , functions transformed!","code":"data <- list(\"X\" = Xext,              \"y\" = y,              \"N\" = length(y)) linreg <- ' // [[Rcpp::depends(RcppArmadillo)]] #include <RcppArmadillo.h>  // [[Rcpp::export]] double fitfunction(const Rcpp::NumericVector& parameters, Rcpp::List& data){   // our function now only takes the two specified arguments: a   // const Rcpp::NumericVector& and an Rcpp::List&.   // We have to extract all elements from the list:   arma::colvec y = Rcpp::as<arma::colvec>(data[\"y\"]); // the dependent variable   arma::mat X = Rcpp::as<arma::mat>(data[\"X\"]); // the design matrix   int N = Rcpp::as<int>(data[\"N\"]); // the sample size      // Next, we want to get the parameters as a column-vector:     arma::colvec b = Rcpp::as<arma::colvec>(parameters);        // compute the sum of squared errors:     arma::mat sse = arma::trans(y-X*b)*(y-X*b);          // other packages, such as glmnet, scale the sse with      // 1/(2*N), where N is the sample size. We will do that here as well          sse *= 1.0/(2.0 * N);          // note: We must return a double, but the sse is a matrix     // To get a double, just return the single value that is in      // this matrix:       return(sse(0,0)); }  // [[Rcpp::export]] arma::rowvec gradientfunction(const Rcpp::NumericVector& parameters, Rcpp::List& data){     // our function now only takes the two specified arguments: a   // const Rcpp::NumericVector& and an Rcpp::List&.   // We have to extract all elements from the list:   arma::colvec y = Rcpp::as<arma::colvec>(data[\"y\"]); // the dependent variable   arma::mat X = Rcpp::as<arma::mat>(data[\"X\"]); // the design matrix   int N = Rcpp::as<int>(data[\"N\"]); // the sample size      // Next, we want to get the parameters as a column-vector:     arma::colvec b = Rcpp::as<arma::colvec>(parameters);      // note: we want to return our gradients as row-vector; therefore,   // we have to transpose the resulting column-vector:     arma::rowvec gradients = arma::trans(-2.0*X.t() * y + 2.0*X.t()*X*b);          // other packages, such as glmnet, scale the sse with      // 1/(2*N), where N is the sample size. We will do that here as well          gradients *= (.5/N);          return(gradients); } '"},{"path":"/articles/General-Purpose-Optimization.html","id":"step-3-creating-pointers-to-our-functions","dir":"Articles","previous_headings":"The second approach: Using C++ function pointers","what":"Step 3: Creating pointers to our functions","title":"General-Purpose-Optimization","text":"get’s really tricky! can’t just pass functions C++. However, can create pointers. generated C++ can tricky get right. simplify process, created function helps setting things : Let’s follow instructions add lines C++ functions: Compile functions using Rcpp: Great! Now way, can create pointers functions:","code":"cat(lessSEM::makePtrs(fitFunName = \"fitfunction\", # name of the function in C++                       gradFunName = \"gradientfunction\" # name of the function in C++ ) ) #>  #> // INSTRUCTIONS: ADD THE FOLLOWING LINES TO YOUR C++ FUNCTIONS #>  #> // IF RCPPARMADILLO IS NOT IMPORTED YET, UNCOMMENT THE FOLLOWING TWO LINES #> // // [[Rcpp::depends(RcppArmadillo)]] #> // #include <RcppArmadillo.h> #>  #> // https://gallery.rcpp.org/articles/passing-cpp-function-pointers/ #> typedef double (*fitFunPtr)(const Rcpp::NumericVector&, //parameters #>                 Rcpp::List& //additional elements #> ); #> typedef Rcpp::XPtr<fitFunPtr> fitFunPtr_t; #>  #> typedef arma::rowvec (*gradientFunPtr)(const Rcpp::NumericVector&, //parameters #>                       Rcpp::List& //additional elements #> ); #> typedef Rcpp::XPtr<gradientFunPtr> gradientFunPtr_t; #>  #> // [[Rcpp::export]] #> fitFunPtr_t fitfunctionPtr() { #>         return(fitFunPtr_t(new fitFunPtr(&fitfunction))); #> } #>  #> // [[Rcpp::export]] #> gradientFunPtr_t gradientfunctionPtr() { #>         return(gradientFunPtr_t(new gradientFunPtr(&gradientfunction))); #> } linreg <- ' // [[Rcpp::depends(RcppArmadillo)]] #include <RcppArmadillo.h>  // [[Rcpp::export]] double fitfunction(const Rcpp::NumericVector& parameters, Rcpp::List& data){   // our function now only takes the two specified arguments: a   // const Rcpp::NumericVector& and an Rcpp::List&.   // We have to extract all elements from the list:   arma::colvec y = Rcpp::as<arma::colvec>(data[\"y\"]); // the dependent variable   arma::mat X = Rcpp::as<arma::mat>(data[\"X\"]); // the design matrix   int N = Rcpp::as<int>(data[\"N\"]); // the sample size      // Next, we want to get the parameters as a column-vector:     arma::colvec b = Rcpp::as<arma::colvec>(parameters);        // compute the sum of squared errors:     arma::mat sse = arma::trans(y-X*b)*(y-X*b);          // other packages, such as glmnet, scale the sse with      // 1/(2*N), where N is the sample size. We will do that here as well          sse *= 1.0/(2.0 * N);          // note: We must return a double, but the sse is a matrix     // To get a double, just return the single value that is in      // this matrix:       return(sse(0,0)); }  // [[Rcpp::export]] arma::rowvec gradientfunction(const Rcpp::NumericVector& parameters, Rcpp::List& data){     // our function now only takes the two specified arguments: a   // const Rcpp::NumericVector& and an Rcpp::List&.   // We have to extract all elements from the list:   arma::colvec y = Rcpp::as<arma::colvec>(data[\"y\"]); // the dependent variable   arma::mat X = Rcpp::as<arma::mat>(data[\"X\"]); // the design matrix   int N = Rcpp::as<int>(data[\"N\"]); // the sample size      // Next, we want to get the parameters as a column-vector:     arma::colvec b = Rcpp::as<arma::colvec>(parameters);      // note: we want to return our gradients as row-vector; therefore,   // we have to transpose the resulting column-vector:     arma::rowvec gradients = arma::trans(-2.0*X.t() * y + 2.0*X.t()*X*b);          // other packages, such as glmnet, scale the sse with      // 1/(2*N), where N is the sample size. We will do that here as well          gradients *= (.5/N);          return(gradients); }  /// THE FOLLOWING PART IS NEW:  // INSTRUCTIONS: ADD THE FOLLOWING LINES TO YOUR C++ FUNCTIONS  // IF RCPPARMADILLO IS NOT IMPORTED YET, UNCOMMENT THE FOLLOWING TWO LINES // // [[Rcpp::depends(RcppArmadillo)]] // #include <RcppArmadillo.h>  // https://gallery.rcpp.org/articles/passing-cpp-function-pointers/ typedef double (*fitFunPtr)(const Rcpp::NumericVector&, //parameters                 Rcpp::List& //additional elements ); typedef Rcpp::XPtr<fitFunPtr> fitFunPtr_t;  typedef arma::rowvec (*gradientFunPtr)(const Rcpp::NumericVector&, //parameters                       Rcpp::List& //additional elements ); typedef Rcpp::XPtr<gradientFunPtr> gradientFunPtr_t;  // [[Rcpp::export]] fitFunPtr_t fitfunctionPtr() {         return(fitFunPtr_t(new fitFunPtr(&fitfunction))); }  // [[Rcpp::export]] gradientFunPtr_t gradientfunctionPtr() {         return(gradientFunPtr_t(new gradientFunPtr(&gradientfunction))); } ' Rcpp::sourceCpp(code = linreg) ffp <- fitfunctionPtr() # create the pointer to the fitting function # Note that the name of this function will depend on the name of your fitting function. # For instance, if your fitting function is called sse, then the pointer will be created  # with ffp <- ssePtr() gfp <- gradientfunctionPtr() # create the pointer to the gradient function # Note that the name of this function will depend on the name of your gradient function. # For instance, if your gradient function is called sseGradient, then the pointer will be created  # with gfp <- sseGradientPtr()"},{"path":"/articles/General-Purpose-Optimization.html","id":"optimizing-the-model","dir":"Articles","previous_headings":"The second approach: Using C++ function pointers","what":"Optimizing the model","title":"General-Purpose-Optimization","text":"last step call general purpose optimization. end, use gpLassoCpp function: Benchmarking approach results : , reduced run time even !","code":"l1 <- gpLassoCpp(par = par,                   regularized = regularized,                   # important: pass the poinnters!                  fn = ffp,                   gr = gfp,                   lambdas = lambdas,                   # finally, pass the list which the fitting function and the                   # gradient function need:                  additionalArguments = data ) head(l1@parameters) #>        lambda alpha         b0        b1        b2        b3       b4 #> 1 0.000000000     1 0.02738485 1.0129187 0.9991454 0.9705724 1.027625 #> 2 0.005263158     1 0.02935277 1.0043736 0.9908928 0.9626259 1.025139 #> 3 0.010526316     1 0.02995028 0.9967094 0.9846668 0.9552792 1.021892 #> 4 0.015789474     1 0.03010669 0.9897329 0.9789425 0.9481493 1.018672 #> 5 0.021052632     1 0.03029739 0.9827288 0.9732059 0.9409868 1.015363 #> 6 0.026315789     1 0.03112461 0.9753368 0.9670620 0.9338617 1.011553 #>            b5           b6           b7          b8           b9         b10 #> 1 0.014035790 -0.007461053 0.0185898207 0.021930913 -0.009900171 0.027401116 #> 2 0.003365848  0.000000000 0.0143413336 0.015434662 -0.007939397 0.022297378 #> 3 0.000000000  0.000000000 0.0096220251 0.010707434 -0.005256682 0.017464766 #> 4 0.000000000  0.000000000 0.0049333240 0.006364021 -0.002393492 0.012713145 #> 5 0.000000000  0.000000000 0.0001773114 0.002036028  0.000000000 0.007969742 #> 6 0.000000000  0.000000000 0.0000000000 0.000000000  0.000000000 0.003303730 #> [1] 0.01577735 0.01567864 0.01552200 0.01528883 0.01555324"},{"path":"/articles/General-Purpose-Optimization.html","id":"the-third-approach-including-the-header-files","dir":"Articles","previous_headings":"","what":"The third approach: Including the header files","title":"General-Purpose-Optimization","text":", far, difficult approach created whole package demonstrate . find information vignette -optimizer-interface lessLM package. come parameter estimates: run times even lower:","code":"#>              b0        b1        b2        b3       b4          b5           b6 #> [1,] 0.02734701 1.0129361 0.9991629 0.9705501 1.027728 0.013993181 -0.007491533 #> [2,] 0.02939675 1.0043635 0.9908681 0.9626493 1.025035 0.003400965  0.000000000 #> [3,] 0.02998680 0.9967117 0.9846504 0.9552960 1.021803 0.000000000  0.000000000 #> [4,] 0.03006777 0.9897315 0.9789595 0.9481301 1.018774 0.000000000  0.000000000 #> [5,] 0.03032345 0.9827556 0.9731799 0.9409662 1.015441 0.000000000  0.000000000 #> [6,] 0.03111085 0.9753325 0.9670615 0.9338506 1.011607 0.000000000  0.000000000 #>                b7          b8           b9         b10 #> [1,] 0.0186210155 0.021974963 -0.009975776 0.027466466 #> [2,] 0.0143089363 0.015388336 -0.007860992 0.022231196 #> [3,] 0.0095927088 0.010679371 -0.005183552 0.017406513 #> [4,] 0.0049636831 0.006397664 -0.002474334 0.012779758 #> [5,] 0.0001374354 0.002100692  0.000000000 0.008033475 #> [6,] 0.0000000000 0.000000000  0.000000000 0.003352431 #> [1] 0.002645731 0.001603842 0.001608372 0.001487017 0.001482248"},{"path":"/articles/Mixed-Penalties.html","id":"getting-started","dir":"Articles","previous_headings":"","what":"Getting Started","title":"Mixed Penalties","text":"following model, allow cross-loadings (c2-c4). want regularize , cross-loadings regression coefficients (r1 - r3) Next, add separate lasso penalties loadings regressions: Note can use pipe-operator add multiple penalties. don’t ; following also work: fit model, use fit- function: check parameter regularized penalty, can look penalty statement resulting object: can access best parameters according BIC : tuningParameterConfiguration refers rows lambda, theta, alpha matrices resulted best fit: case, best model cross-loadings, regressions remained unregularized: lambda cross-loadings large (1), lambda regressions 0 (regularization).","code":"model <- '    # latent variable definitions      ind60 =~ x1 + x2 + x3 + c2*y2 + c3*y3 + c4*y4      dem60 =~ y1 + y2 + y3 + y4      dem65 =~ y5 + y6 + y7 + c*y8    # regressions     dem60 ~ r1*ind60     dem65 ~ r2*ind60 + r3*dem60 '  lavaanModel <- sem(model,                    data = PoliticalDemocracy) mp <- lavaanModel |>   mixedPenalty() |>   addLasso(regularized = c(\"c2\", \"c3\", \"c4\"),             lambdas = seq(0,1,.1)) |>   addLasso(regularized = c(\"r1\", \"r2\", \"r3\"),             lambdas = seq(0,1,.2))  #>  #>  Note: Mixed penalties is a very new feature. Please note that there may still be bugs in the procedure. Use carefully! mp <- lavaanModel |>   mixedPenalty() |>   addLasso(regularized = c(\"c2\", \"c3\", \"c4\"),             lambdas = seq(0,1,.1)) |>   addScad(regularized = c(\"r1\", \"r2\", \"r3\"),            lambdas = seq(0,1,.2),           thetas = 3.7)  #>  #>  Note: Mixed penalties is a very new feature. Please note that there may still be bugs in the procedure. Use carefully! fitMp <- fit(mp) fitMp@penalty #>    ind60=~x2    ind60=~x3           c2           c3           c4    dem60=~y2  #>       \"none\"       \"none\"      \"lasso\"      \"lasso\"      \"lasso\"       \"none\"  #>    dem60=~y3    dem60=~y4    dem65=~y6    dem65=~y7            c           r1  #>       \"none\"       \"none\"       \"none\"       \"none\"       \"none\"       \"scad\"  #>           r2           r3       x1~~x1       x2~~x2       x3~~x3       y2~~y2  #>       \"scad\"       \"scad\"       \"none\"       \"none\"       \"none\"       \"none\"  #>       y3~~y3       y4~~y4       y1~~y1       y5~~y5       y6~~y6       y7~~y7  #>       \"none\"       \"none\"       \"none\"       \"none\"       \"none\"       \"none\"  #>       y8~~y8 ind60~~ind60 dem60~~dem60 dem65~~dem65  #>       \"none\"       \"none\"       \"none\"       \"none\" coef(fitMp, criterion = \"BIC\") #>                                                                       #>                        Tuning ||--||  Estimates                       #>  ---------------------------- ||--|| ---------- ---------- ---------- #>  tuningParameterConfiguration ||--||  ind60=~x2  ind60=~x3         c2 #>  ============================ ||--|| ========== ========== ========== #>                       11.0000 ||--||     2.1818     1.8188          . #>                                                                               #>                                                                               #>  ---------- ---------- ---------- ---------- ---------- ---------- ---------- #>          c3         c4  dem60=~y2  dem60=~y3  dem60=~y4  dem65=~y6  dem65=~y7 #>  ========== ========== ========== ========== ========== ========== ========== #>           .          .     1.3541     1.0441     1.2997     1.2586     1.2825 #>                                                                               #>                                                                               #>  ---------- ---------- ---------- ---------- ---------- ---------- ---------- #>           c         r1         r2         r3     x1~~x1     x2~~x2     x3~~x3 #>  ========== ========== ========== ========== ========== ========== ========== #>      1.3099     1.4740     0.4532     0.8643     0.0818     0.1184     0.4672 #>                                                                               #>                                                                               #>  ---------- ---------- ---------- ---------- ---------- ---------- ---------- #>      y2~~y2     y3~~y3     y4~~y4     y1~~y1     y5~~y5     y6~~y6     y7~~y7 #>  ========== ========== ========== ========== ========== ========== ========== #>      6.4892     5.3396     2.8861     1.9420     2.3904     4.3421     3.5090 #>                                                    #>                                                    #>  ---------- ------------ ------------ ------------ #>      y8~~y8 ind60~~ind60 dem60~~dem60 dem65~~dem65 #>  ========== ============ ============ ============ #>      2.9392       0.4482       3.8710       0.1159 getTuningParameterConfiguration(regularizedSEMMixedPenalty = fitMp,                                  tuningParameterConfiguration = 11) #>                 parameter penalty lambda theta alpha #> ind60=~x2       ind60=~x2    none      0   0.0     0 #> ind60=~x3       ind60=~x3    none      0   0.0     0 #> c2                     c2   lasso      1   0.0     0 #> c3                     c3   lasso      1   0.0     0 #> c4                     c4   lasso      1   0.0     0 #> dem60=~y2       dem60=~y2    none      0   0.0     0 #> dem60=~y3       dem60=~y3    none      0   0.0     0 #> dem60=~y4       dem60=~y4    none      0   0.0     0 #> dem65=~y6       dem65=~y6    none      0   0.0     0 #> dem65=~y7       dem65=~y7    none      0   0.0     0 #> c                       c    none      0   0.0     0 #> r1                     r1    scad      0   3.7     0 #> r2                     r2    scad      0   3.7     0 #> r3                     r3    scad      0   3.7     0 #> x1~~x1             x1~~x1    none      0   0.0     0 #> x2~~x2             x2~~x2    none      0   0.0     0 #> x3~~x3             x3~~x3    none      0   0.0     0 #> y2~~y2             y2~~y2    none      0   0.0     0 #> y3~~y3             y3~~y3    none      0   0.0     0 #> y4~~y4             y4~~y4    none      0   0.0     0 #> y1~~y1             y1~~y1    none      0   0.0     0 #> y5~~y5             y5~~y5    none      0   0.0     0 #> y6~~y6             y6~~y6    none      0   0.0     0 #> y7~~y7             y7~~y7    none      0   0.0     0 #> y8~~y8             y8~~y8    none      0   0.0     0 #> ind60~~ind60 ind60~~ind60    none      0   0.0     0 #> dem60~~dem60 dem60~~dem60    none      0   0.0     0 #> dem65~~dem65 dem65~~dem65    none      0   0.0     0"},{"path":"/articles/Mixed-Penalties.html","id":"using-glmnet","dir":"Articles","previous_headings":"","what":"Using glmnet","title":"Mixed Penalties","text":"glmnet optimizer typically considerably faster ista. However, mentioned , penalty functions currently supported. Mixed penalties glmnet can specified follows: fit model, use fit- function: tuningParameterConfiguration refers rows lambda, theta, alpha matrices resulted best fit: short run-time comparison ista glmnet lasso-regularized model : Five repetitions using ista took 2.752 seconds, glmnet took 2.917 seconds. , can use glmnet model, recommend .","code":"mp <- lavaanModel |>   # Change the optimizer and the control object:   mixedPenalty(method = \"glmnet\",                control = controlGlmnet()) |>   addLasso(regularized = c(\"c2\", \"c3\", \"c4\"),                  lambdas = seq(0,1,.1)) |>   addLasso(regularized = c(\"r1\", \"r2\", \"r3\"),                  lambdas = seq(0,1,.2))  #>  #>  Note: Mixed penalties is a very new feature. Please note that there may still be bugs in the procedure. Use carefully! fitMp <- fit(mp) coef(fitMp, criterion = \"BIC\") #>                                                                       #>                        Tuning ||--||  Estimates                       #>  ---------------------------- ||--|| ---------- ---------- ---------- #>  tuningParameterConfiguration ||--||  ind60=~x2  ind60=~x3         c2 #>  ============================ ||--|| ========== ========== ========== #>                       11.0000 ||--||     2.1817     1.8188          . #>                                                                               #>                                                                               #>  ---------- ---------- ---------- ---------- ---------- ---------- ---------- #>          c3         c4  dem60=~y2  dem60=~y3  dem60=~y4  dem65=~y6  dem65=~y7 #>  ========== ========== ========== ========== ========== ========== ========== #>           .          .     1.3540     1.0440     1.2995     1.2585     1.2825 #>                                                                               #>                                                                               #>  ---------- ---------- ---------- ---------- ---------- ---------- ---------- #>           c         r1         r2         r3     x1~~x1     x2~~x2     x3~~x3 #>  ========== ========== ========== ========== ========== ========== ========== #>      1.3098     1.4738     0.4532     0.8644     0.0818     0.1184     0.4673 #>                                                                               #>                                                                               #>  ---------- ---------- ---------- ---------- ---------- ---------- ---------- #>      y2~~y2     y3~~y3     y4~~y4     y1~~y1     y5~~y5     y6~~y6     y7~~y7 #>  ========== ========== ========== ========== ========== ========== ========== #>      6.4895     5.3399     2.8871     1.9419     2.3901     4.3428     3.5096 #>                                                    #>                                                    #>  ---------- ------------ ------------ ------------ #>      y8~~y8 ind60~~ind60 dem60~~dem60 dem65~~dem65 #>  ========== ============ ============ ============ #>      2.9403       0.4482       3.8717       0.1149 getTuningParameterConfiguration(regularizedSEMMixedPenalty = fitMp,                                  tuningParameterConfiguration = 11) #>                 parameter    penalty lambda alpha #> ind60=~x2       ind60=~x2       none      0     0 #> ind60=~x3       ind60=~x3       none      0     0 #> c2                     c2 elasticNet      1     1 #> c3                     c3 elasticNet      1     1 #> c4                     c4 elasticNet      1     1 #> dem60=~y2       dem60=~y2       none      0     0 #> dem60=~y3       dem60=~y3       none      0     0 #> dem60=~y4       dem60=~y4       none      0     0 #> dem65=~y6       dem65=~y6       none      0     0 #> dem65=~y7       dem65=~y7       none      0     0 #> c                       c       none      0     0 #> r1                     r1 elasticNet      0     1 #> r2                     r2 elasticNet      0     1 #> r3                     r3 elasticNet      0     1 #> x1~~x1             x1~~x1       none      0     0 #> x2~~x2             x2~~x2       none      0     0 #> x3~~x3             x3~~x3       none      0     0 #> y2~~y2             y2~~y2       none      0     0 #> y3~~y3             y3~~y3       none      0     0 #> y4~~y4             y4~~y4       none      0     0 #> y1~~y1             y1~~y1       none      0     0 #> y5~~y5             y5~~y5       none      0     0 #> y6~~y6             y6~~y6       none      0     0 #> y7~~y7             y7~~y7       none      0     0 #> y8~~y8             y8~~y8       none      0     0 #> ind60~~ind60 ind60~~ind60       none      0     0 #> dem60~~dem60 dem60~~dem60       none      0     0 #> dem65~~dem65 dem65~~dem65       none      0     0"},{"path":"/articles/Mixed-Penalties.html","id":"bibliography","dir":"Articles","previous_headings":"","what":"Bibliography","title":"Mixed Penalties","text":"Geminiani, E., Marra, G., & Moustaki, . (2021). Single- multiple-group penalized factor analysis: trust-region algorithm approach integrated automatic multiple tuning parameter selection. Psychometrika, 86(1), 65–95. https://doi.org/10.1007/s11336-021-09751-8","code":""},{"path":"/articles/Parameter-transformations.html","id":"motivation","dir":"Articles","previous_headings":"","what":"Motivation","title":"Parameter-transformations","text":"longitudinal SEM, important investigate parameters stay time (e.g., measurement invariance loadings). can difficult decide may require setting many different models manually. , regularization techniques can handy. instance, seminal political democracy example, model typically set follows (see ?lavaan::sem): Note loadings , b, c assumed stay time. , measurement invariance assumed! Relaxing assumption, define model follows: , loading estimated separately. results complex model. know model use? many procedures answer question (e.g., using modification indexes, setting separate models hand, etc.). following, show regularization used (see e.g., Belzak & Bauer, 2020; Jacobucci & Grimm, 2018).","code":"modelSyntax <- '    # latent variable definitions      ind60 =~ x1 + x2 + x3      dem60 =~ y1 + a*y2 + b*y3 + c*y4      dem65 =~ y5 + a*y6 + b*y7 + c*y8    # regressions     dem60 ~ ind60     dem65 ~ ind60 + dem60    # residual correlations     y1 ~~ y5     y2 ~~ y4      y3 ~~ y7     y4 ~~ y8     y6 ~~ y8 ' modelSyntax <- '    # latent variable definitions      ind60 =~ x1 + x2 + x3      dem60 =~ y1 + a1*y2 + b1*y3 + c1*y4      dem65 =~ y5 + a2*y6 + b2*y7 + c2*y8    # regressions     dem60 ~ ind60     dem65 ~ ind60 + dem60    # residual correlations     y1 ~~ y5     y2 ~~ y4      y3 ~~ y7     y4 ~~ y8     y6 ~~ y8 '"},{"path":"/articles/Parameter-transformations.html","id":"using-regularization","dir":"Articles","previous_headings":"","what":"Using Regularization","title":"Parameter-transformations","text":"First, note measurement invariance can rephrased \\(a_1-a_2 = 0\\), \\(b_1-b_2 = 0\\), \\(c_1-c_2 = 0\\). Thus, regularizing differences parameters may allow testing measurement invariance (e.g., Belzak & Bauer, 2020; Liang et al., 2018; Muthen & Asparouhov, 2013). fact, used Bayesian SEM test approximate measurement invariance (Liang et al., 2018; Muthen & Asparouhov, 2013). Similar procedures also developed Huang (2018) multi-group differences parameter estimates Fisher et al. (2022) vector autoregressive models. Furthermore, Jacobucci & Grimm (2018) proposed regularizing differences latent change score models test equivalence autoproportion parameters time using two-step procedure. end, implemented diff_lasso regsem (Jacobucci et al., 2019). diff_lasso available lessSEM. Instead, lessSEM provides flexible workaround: parameter transformations. make work, re-define parameters. Redefine: \\[ \\begin{align} a_2 &= a_1 + \\Delta a_2\\\\ b_2 &= b_1 + \\Delta b_2\\\\ c_2 &= c_1 + \\Delta c_2 \\end{align} \\] regularizing \\(\\Delta a_2\\), \\(\\Delta b_2\\), \\(\\Delta c_2\\) towards zero, can enforce measurement invariance time.","code":""},{"path":"/articles/Parameter-transformations.html","id":"setting-up-the-model","dir":"Articles","previous_headings":"","what":"Setting up the Model","title":"Parameter-transformations","text":"first start flexible model want test: Note model defined estimates parameters time-point specific. , measurement invariance assumed. Now, want redefine parameters outlined : \\[ \\begin{align} a_2 &= a_1 + \\Delta a_2\\\\ b_2 &= b_1 + \\Delta b_2\\\\ c_2 &= c_1 + \\Delta c_2 \\end{align} \\] lessSEM redefinitions called transformations can passed penalty functions (e.g., lasso) using modifyModel command. First, create definition transformations: Next, pass transformations variable penalty function: Let’s look parameter estimates: Note differences parameters get smaller larger \\(\\lambda\\) values. can also plot differences:  check measurement invariance can assumed, can select best model using information criteria: Note differences zeroed – , model full measurement invariance fit best. can also access transformed parameters: Limitation: , take account variables may different scales; thorough use method scale data first.","code":"library(lavaan) modelSyntax <- '    # latent variable definitions      ind60 =~ x1 + x2 + x3      dem60 =~ y1 + a1*y2 + b1*y3 + c1*y4      dem65 =~ y5 + a2*y6 + b2*y7 + c2*y8    # regressions     dem60 ~ ind60     dem65 ~ ind60 + dem60    # residual correlations     y1 ~~ y5     y2 ~~ y4      y3 ~~ y7     y4 ~~ y8     y6 ~~ y8 '  lavaanFit <- sem(model = modelSyntax,                  data = PoliticalDemocracy) transformations <- \" // IMPORTANT: Our transformations always have to start with the follwing line: parameters: a1, a2, b1, b2, c1, c2, delta_a2, delta_b2, delta_c2  // In the line above, we defined the names of the parameters which we // want to use in our transformations. EACH AND EVERY PARAMETER USED IN // THE FOLLOWING MUST BE STATED ABOVE. The line must always start with // the keyword 'parameters' followed by a colon. The parameters must be // separated by commata. // Comments can be added by using double backslash as shown here.  // Now we can state our transformations:  a2 = a1 + delta_a2; // Note: Each declaration must end with a semi-colon! b2 = b1 + delta_b2; c2 = c1 + delta_c2; \" lassoFit <- lasso(lavaanModel = lavaanFit,                    regularized = c(\"delta_a2\", \"delta_b2\", \"delta_c2\"),# we want to regularize                    # the differences between the parameters                   nLambdas = 100,                   # Our model modification must make use of the modifyModel - function:                   modifyModel = modifyModel(transformations = transformations) ) coef(lassoFit)@estimates[seq(1,100,10),c(\"a1\", \"b1\", \"c1\", \"delta_a2\", \"delta_b2\", \"delta_c2\")] #>             a1       b1       c1    delta_a2    delta_b2 delta_c2 #>  [1,] 1.210973 1.167917 1.233987  0.00000000 0.000000000        0 #>  [2,] 1.211087 1.166369 1.234012  0.00000000 0.002697893        0 #>  [3,] 1.212569 1.150755 1.234643  0.00000000 0.030061810        0 #>  [4,] 1.214346 1.135706 1.235561  0.00000000 0.057011496        0 #>  [5,] 1.216417 1.121165 1.236776  0.00000000 0.083706587        0 #>  [6,] 1.218770 1.107057 1.238262  0.00000000 0.110189007        0 #>  [7,] 1.221411 1.093357 1.240041  0.00000000 0.136518096        0 #>  [8,] 1.226621 1.080073 1.242027 -0.00396875 0.162494972        0 #>  [9,] 1.246399 1.067703 1.244333 -0.03213194 0.186303618        0 #> [10,] 1.266518 1.055795 1.247167 -0.06025881 0.210169907        0 plot(lassoFit) coef(lassoFit, criterion = \"BIC\") #>                                                                                #>   Tuning         ||--||  Estimates                                             #>  ------- ------- ||--|| ---------- ---------- ---------- ---------- ---------- #>   lambda   alpha ||--||  ind60=~x2  ind60=~x3         a1         b1         c1 #>  ======= ======= ||--|| ========== ========== ========== ========== ========== #>   0.2216  1.0000 ||--||     2.1825     1.8189     1.2110     1.1679     1.2340 #>                                                                       #>                                                                       #>  ----------- ----------- ----------- ---------- ---------- ---------- #>  dem60~ind60 dem65~ind60 dem65~dem60     y1~~y5     y2~~y4     y3~~y7 #>  =========== =========== =========== ========== ========== ========== #>       1.4534      0.5935      0.8659     0.5552     1.5948     0.7806 #>                                                                               #>                                                                               #>  ---------- ---------- ---------- ---------- ---------- ---------- ---------- #>      y4~~y8     y6~~y8     x1~~x1     x2~~x2     x3~~x3     y1~~y1     y2~~y2 #>  ========== ========== ========== ========== ========== ========== ========== #>      0.6537     1.5350     0.0820     0.1177     0.4675     1.7929     7.3843 #>                                                                                 #>                                                                                 #>  ---------- ---------- ---------- ---------- ---------- ---------- ------------ #>      y3~~y3     y4~~y4     y5~~y5     y6~~y6     y7~~y7     y8~~y8 ind60~~ind60 #>  ========== ========== ========== ========== ========== ========== ============ #>      5.0175     3.4074     2.2857     4.8978     3.5510     3.4512       0.4480 #>                                                             #>                                                             #>  ------------ ------------ ---------- ---------- ---------- #>  dem60~~dem60 dem65~~dem65   delta_a2   delta_b2   delta_c2 #>  ============ ============ ========== ========== ========== #>        3.9408       0.2034          .          .          . head(lassoFit@transformations) #>      lambda alpha       a2       b2       c2 #> 1 0.2411201     1 1.210973 1.167917 1.233987 #> 2 0.2386846     1 1.210976 1.167915 1.233991 #> 3 0.2362490     1 1.210981 1.167924 1.233993 #> 4 0.2338134     1 1.210980 1.167928 1.233995 #> 5 0.2313779     1 1.210980 1.167929 1.233994 #> 6 0.2289423     1 1.210979 1.167928 1.233993"},{"path":"/articles/Parameter-transformations.html","id":"some-guidelines","dir":"Articles","previous_headings":"","what":"Some Guidelines","title":"Parameter-transformations","text":"using transformations, please make sure give parameters names compatible standard naming conventions R. default names lavaan (e.g., f=~y1 loadings) supported. , parameters used transformations given names lavaan syntax. example , used following syntax: Importantly, parameters used transformation (a1, b1, c1, a2, b2, c2) labeled lavaan syntax. counter example: syntax specifies model, use lavaan-specific naming convention parameters. a1, example, named dem60=~y2. names compatible current implementation transformations used lessSEM.","code":"modelSyntax <- '    # latent variable definitions      ind60 =~ x1 + x2 + x3      dem60 =~ y1 + a1*y2 + b1*y3 + c1*y4      dem65 =~ y5 + a2*y6 + b2*y7 + c2*y8    # regressions     dem60 ~ ind60     dem65 ~ ind60 + dem60    # residual correlations     y1 ~~ y5     y2 ~~ y4      y3 ~~ y7     y4 ~~ y8     y6 ~~ y8 ' modelSyntax <- '    # latent variable definitions      ind60 =~ x1 + x2 + x3      dem60 =~ y1 + y2 + y3 + y4      dem65 =~ y5 + y6 + y7 + y8    # regressions     dem60 ~ ind60     dem65 ~ ind60 + dem60    # residual correlations     y1 ~~ y5     y2 ~~ y4      y3 ~~ y7     y4 ~~ y8     y6 ~~ y8 '"},{"path":"/articles/Parameter-transformations.html","id":"further-examples","dir":"Articles","previous_headings":"","what":"Further Examples","title":"Parameter-transformations","text":"Another example transformations useful detecting non-stationarity autoregressive cross-lagged parameters (e.g., Liang et al., 2018). following, demonstrate autoregressive model. model defined : \\[ \\begin{align} \\eta_t &= a_t\\eta_{t-1} + \\zeta_t\\\\ \\begin{bmatrix} y_{1,t}\\\\ y_{2,t}\\\\ y_{3,t}\\\\ \\end{bmatrix} &= \\begin{bmatrix} l_1\\\\ l_2\\\\ l_3\\\\ \\end{bmatrix} \\eta_t + \\pmb\\varepsilon \\end{align} \\] often assumed autoregressive effect \\(a_t\\) constant time; , autoregressive effect used time points. strong assumption may want test . One way using procedure outlined , define \\(a_t = a_1 + \\Delta a_t\\) (see Jacobucci & Grimm, 2018 similar procedure latent change score models). case, autoregressive effect composed first autoregressive effect (\\(a_1\\)) difference parameters (\\(\\Delta a_t\\)). regularizing \\(\\Delta a_t\\), can enforce stationarity. drawback approach outlined first autoregressive effect treated differently rest: , \\(a_1\\) serve baseline \\(a_2\\) \\(a_5\\)? take slightly different approach, building idea Fisher et al. (2022). similar approach also proposed Bai et al. (2020). Let’s define autoregressive effect \\[a_t = a_{t-1} + \\Delta a_t\\] Note \\(\\Delta a_t\\) longer difference respect initial autoregressive effect \\(a_1\\) difference respect directly preceding time point. regularizing \\(\\Delta a_t\\), can now detect sudden changes parameter – e.g., due intervention. can also thought regime switching model, underlying model changes time (see also Ou et al., 2019 regime switching models). regularization procedure, want detect process changes. won’t go details set model , can find source file (e.g., GitHub). simulated data set 200 individuals measured 10 time points. autoregressive effect \\(a_t\\) changes \\(t=4\\) \\(.6\\) \\(.2\\). data looks follows: lavaan model defined follows: fit model using lavaan: Note constraints autoregressive effects implemented – effect (a1-a9) estimated separately. now define transformations follows: Finally, can fit model: Extracting best fitting model: true autoregressive effects given estimates result perfect, lessSEM correctly identified change autoregressive parameter.","code":"head(data) #>           y1_t1       y2_t1       y3_t1      y1_t2      y2_t2      y3_t2 #> [1,]  0.2627294 -0.07745288 -0.06014729  1.1052896 -0.5125258  0.1352253 #> [2,] -0.5795123  0.27989039 -1.76295329 -1.1646535  0.0607009 -0.7770722 #> [3,]  0.5746526  0.67203557  1.30772224  0.5620735  1.3873269  1.2415508 #> [4,]  1.6376973  1.30880439  1.35159277  0.4681794  1.5370870  0.6961462 #> [5,]  0.4995286 -0.23722728  0.61244148  0.3469646  0.3052115  0.2639443 #> [6,] -0.9766373  0.94700260 -0.59421930 -0.3197840  0.8221090 -0.6808501 #>           y1_t3       y2_t3      y3_t3       y1_t4      y2_t4      y3_t4 #> [1,]  0.6775861  0.48382200  0.5163607  0.06936298  1.6842420  0.5604284 #> [2,] -0.4571539 -0.30575453  0.7552239  0.06618071 -1.5803092  0.1672394 #> [3,]  0.8255529 -0.09010965  1.5877450  0.75756487  1.3579232 -0.1379793 #> [4,]  0.1872212 -1.57377418  1.1081811 -0.50832183  1.1968847 -1.6473363 #> [5,]  1.3768225  0.02394048  1.1573912  1.30350064  0.3049751 -0.0146624 #> [6,] -0.3434095 -0.90348981 -1.1760506 -1.72146444 -1.2828716 -0.4379788 #>           y1_t5      y2_t5      y3_t5      y1_t6      y2_t6       y3_t6 #> [1,] -1.6171372 -1.5100658 -2.1212448 -0.9014921 -0.9732405 -0.89209997 #> [2,]  0.1045129  0.3483139 -0.3669688 -0.6347357 -1.1095700  0.01821975 #> [3,] -1.1968405 -0.3623344 -0.8732056 -0.8290071 -0.3555817 -0.98268524 #> [4,]  0.4423361 -1.0372594 -0.8153181 -1.6003121 -2.1169045 -0.46541710 #> [5,]  1.3351806  0.1229075  0.8274846  0.4508487  0.1328104  0.65883759 #> [6,]  1.0261730  0.1686185  0.6083559 -0.7427341  0.4514472 -1.15435404 #>           y1_t7      y2_t7      y3_t7      y1_t8       y2_t8      y3_t8 #> [1,] -1.4697023 -0.9088437 -2.1031570  0.9915954  0.26021125  1.2988304 #> [2,]  0.3845160  1.0140172  1.8585956 -1.1280623  0.62646559 -0.8323908 #> [3,] -1.8270782 -0.2335533 -1.6441484 -0.2693417  0.09757043  0.5752644 #> [4,]  1.3226249  1.7154104  1.8113749  0.3459617  0.29614603  0.2463911 #> [5,] -0.3915346 -0.5549143 -0.2476716 -1.3659303  1.14721685 -0.1968243 #> [6,]  1.7453347  1.2488910  0.6649188 -0.6131110 -0.27533871  0.1895920 #>            y1_t9      y2_t9      y3_t9      y1_t10      y2_t10     y3_t10 #> [1,]  1.27416201  1.2010039  1.1426722  0.04668192 -0.24123204 -0.4421784 #> [2,] -0.41574193 -0.6890046 -1.4714503 -0.01788118  0.02058262  0.3191417 #> [3,]  0.37098052 -0.3486185 -0.4273464 -0.87844796 -1.36443913 -1.0240934 #> [4,]  0.55566216  1.2444869  0.1788493  0.81351163 -0.39685221  3.3330365 #> [5,] -1.58477183 -1.8546910  0.7432728  0.89776939  1.05624257  0.3635211 #> [6,]  0.02486185 -0.5151950 -0.2018260  1.13097234  0.74034559  2.5292256 #> eta2 ~ a1*eta1 #> eta3 ~ a2*eta2 #> eta4 ~ a3*eta3 #> eta5 ~ a4*eta4 #> eta6 ~ a5*eta5 #> eta7 ~ a6*eta6 #> eta8 ~ a7*eta7 #> eta9 ~ a8*eta8 #> eta10 ~ a9*eta9 #>  #>  #> eta1 ~~ eta1 #> eta2 ~~ v*eta2 #> eta3 ~~ v*eta3 #> eta4 ~~ v*eta4 #> eta5 ~~ v*eta5 #> eta6 ~~ v*eta6 #> eta7 ~~ v*eta7 #> eta8 ~~ v*eta8 #> eta9 ~~ v*eta9 #> eta10 ~~ v*eta10 #>  #> eta1 =~ 1*y1_t1 + l2*y2_t1 + l3*y3_t1 #> y1_t1 ~~ mvar1*y1_t1 #> y2_t1 ~~ mvar2*y2_t1 #> y3_t1 ~~ mvar3*y3_t1 #> eta2 =~ 1*y1_t2 + l2*y2_t2 + l3*y3_t2 #> y1_t2 ~~ mvar1*y1_t2 #> y2_t2 ~~ mvar2*y2_t2 #> y3_t2 ~~ mvar3*y3_t2 #> eta3 =~ 1*y1_t3 + l2*y2_t3 + l3*y3_t3 #> y1_t3 ~~ mvar1*y1_t3 #> y2_t3 ~~ mvar2*y2_t3 #> y3_t3 ~~ mvar3*y3_t3 #> eta4 =~ 1*y1_t4 + l2*y2_t4 + l3*y3_t4 #> y1_t4 ~~ mvar1*y1_t4 #> y2_t4 ~~ mvar2*y2_t4 #> y3_t4 ~~ mvar3*y3_t4 #> eta5 =~ 1*y1_t5 + l2*y2_t5 + l3*y3_t5 #> y1_t5 ~~ mvar1*y1_t5 #> y2_t5 ~~ mvar2*y2_t5 #> y3_t5 ~~ mvar3*y3_t5 #> eta6 =~ 1*y1_t6 + l2*y2_t6 + l3*y3_t6 #> y1_t6 ~~ mvar1*y1_t6 #> y2_t6 ~~ mvar2*y2_t6 #> y3_t6 ~~ mvar3*y3_t6 #> eta7 =~ 1*y1_t7 + l2*y2_t7 + l3*y3_t7 #> y1_t7 ~~ mvar1*y1_t7 #> y2_t7 ~~ mvar2*y2_t7 #> y3_t7 ~~ mvar3*y3_t7 #> eta8 =~ 1*y1_t8 + l2*y2_t8 + l3*y3_t8 #> y1_t8 ~~ mvar1*y1_t8 #> y2_t8 ~~ mvar2*y2_t8 #> y3_t8 ~~ mvar3*y3_t8 #> eta9 =~ 1*y1_t9 + l2*y2_t9 + l3*y3_t9 #> y1_t9 ~~ mvar1*y1_t9 #> y2_t9 ~~ mvar2*y2_t9 #> y3_t9 ~~ mvar3*y3_t9 #> eta10 =~ 1*y1_t10 + l2*y2_t10 + l3*y3_t10 #> y1_t10 ~~ mvar1*y1_t10 #> y2_t10 ~~ mvar2*y2_t10 #> y3_t10 ~~ mvar3*y3_t10 lavaanFit <- sem(model = lavaanSyntax,                   data = data,                  orthogonal.y = TRUE,                   orthogonal.x = TRUE,                  missing = \"ml\") coef(lavaanFit) #>         a1         a2         a3         a4         a5         a6         a7  #>      0.484      0.535      0.531      0.135      0.124      0.040      0.173  #>         a8         a9 eta1~~eta1          v          v          v          v  #>      0.296      0.185      1.108      0.804      0.804      0.804      0.804  #>          v          v          v          v          v         l2         l3  #>      0.804      0.804      0.804      0.804      0.804      0.567      0.672  #>      mvar1      mvar2      mvar3         l2         l3      mvar1      mvar2  #>      0.049      0.744      0.652      0.567      0.672      0.049      0.744  #>      mvar3         l2         l3      mvar1      mvar2      mvar3         l2  #>      0.652      0.567      0.672      0.049      0.744      0.652      0.567  #>         l3      mvar1      mvar2      mvar3         l2         l3      mvar1  #>      0.672      0.049      0.744      0.652      0.567      0.672      0.049  #>      mvar2      mvar3         l2         l3      mvar1      mvar2      mvar3  #>      0.744      0.652      0.567      0.672      0.049      0.744      0.652  #>         l2         l3      mvar1      mvar2      mvar3         l2         l3  #>      0.567      0.672      0.049      0.744      0.652      0.567      0.672  #>      mvar1      mvar2      mvar3         l2         l3      mvar1      mvar2  #>      0.049      0.744      0.652      0.567      0.672      0.049      0.744  #>      mvar3         l2         l3      mvar1      mvar2      mvar3    y1_t1~1  #>      0.652      0.567      0.672      0.049      0.744      0.652     -0.155  #>    y2_t1~1    y3_t1~1    y1_t2~1    y2_t2~1    y3_t2~1    y1_t3~1    y2_t3~1  #>     -0.088     -0.175     -0.157     -0.049     -0.085     -0.150     -0.030  #>    y3_t3~1    y1_t4~1    y2_t4~1    y3_t4~1    y1_t5~1    y2_t5~1    y3_t5~1  #>     -0.157     -0.095     -0.022     -0.080      0.037     -0.023      0.058  #>    y1_t6~1    y2_t6~1    y3_t6~1    y1_t7~1    y2_t7~1    y3_t7~1    y1_t8~1  #>      0.050      0.065     -0.002     -0.047      0.068     -0.038     -0.022  #>    y2_t8~1    y3_t8~1    y1_t9~1    y2_t9~1    y3_t9~1   y1_t10~1   y2_t10~1  #>     -0.028     -0.048     -0.077     -0.058     -0.041      0.013     -0.012  #>   y3_t10~1  #>     -0.004 #> parameters: a1, a2, a3, a4, a5, a6, a7, a8, a9, delta2, delta3, delta4, delta5, delta6, delta7, delta8, delta9 #>  #> a2 = a1 + delta2; #> a3 = a2 + delta3; #> a4 = a3 + delta4; #> a5 = a4 + delta5; #> a6 = a5 + delta6; #> a7 = a6 + delta7; #> a8 = a7 + delta8; #> a9 = a8 + delta9; lassoFit <- lasso(lavaanModel = lavaanFit,                    regularized = paste0(\"delta\", 2:9),# we want to regularize                    # the differences between the parameters                   nLambdas = 100,                   # glmnet is considerably faster here:                   method = \"glmnet\",                   control = controlGlmnet(),                   # Our model modification must make use of the modifyModel - function:                   modifyModel = modifyModel(transformations = transformations) ) coef(lassoFit, criterion = \"BIC\") #>                                                                                #>   Tuning         ||--||  Estimates                                             #>  ------- ------- ||--|| ---------- ---------- ---------- ---------- ---------- #>   lambda   alpha ||--||         a1 eta1~~eta1          v         l2         l3 #>  ======= ======= ||--|| ========== ========== ========== ========== ========== #>   0.2790  1.0000 ||--||     0.4665     1.1055     0.8051     0.5696     0.6754 #>                                                                               #>                                                                               #>  ---------- ---------- ---------- ---------- ---------- ---------- ---------- #>       mvar1      mvar2      mvar3    y1_t1~1    y2_t1~1    y3_t1~1    y1_t2~1 #>  ========== ========== ========== ========== ========== ========== ========== #>      0.0533     0.7428     0.6500    -0.1546    -0.0878    -0.1746    -0.1565 #>                                                                               #>                                                                               #>  ---------- ---------- ---------- ---------- ---------- ---------- ---------- #>     y2_t2~1    y3_t2~1    y1_t3~1    y2_t3~1    y3_t3~1    y1_t4~1    y2_t4~1 #>  ========== ========== ========== ========== ========== ========== ========== #>     -0.0492    -0.0849    -0.1499    -0.0302    -0.1572    -0.0952    -0.0224 #>                                                                               #>                                                                               #>  ---------- ---------- ---------- ---------- ---------- ---------- ---------- #>     y3_t4~1    y1_t5~1    y2_t5~1    y3_t5~1    y1_t6~1    y2_t6~1    y3_t6~1 #>  ========== ========== ========== ========== ========== ========== ========== #>     -0.0798     0.0374    -0.0231     0.0576     0.0502     0.0651    -0.0019 #>                                                                               #>                                                                               #>  ---------- ---------- ---------- ---------- ---------- ---------- ---------- #>     y1_t7~1    y2_t7~1    y3_t7~1    y1_t8~1    y2_t8~1    y3_t8~1    y1_t9~1 #>  ========== ========== ========== ========== ========== ========== ========== #>     -0.0465     0.0678    -0.0383    -0.0220    -0.0279    -0.0476    -0.0775 #>                                                                               #>                                                                               #>  ---------- ---------- ---------- ---------- ---------- ---------- ---------- #>     y2_t9~1    y3_t9~1   y1_t10~1   y2_t10~1   y3_t10~1     delta2     delta3 #>  ========== ========== ========== ========== ========== ========== ========== #>     -0.0579    -0.0407     0.0132    -0.0118    -0.0042          .          . #>                                                                    #>                                                                    #>  ---------- ---------- ---------- ---------- ---------- ---------- #>      delta4     delta5     delta6     delta7     delta8     delta9 #>  ========== ========== ========== ========== ========== ========== #>     -0.2807          .          .          .          .          . #> [1] 0.6 0.6 0.6 0.2 0.2 0.2 0.2 0.2 0.2 #> [1] 0.4665091 0.4665091 0.4665091 0.1858552 0.1858552 0.1858552 0.1858552 #> [8] 0.1858552 0.1858552"},{"path":"/articles/Parameter-transformations.html","id":"looking-under-the-hood","dir":"Articles","previous_headings":"","what":"Looking under the hood","title":"Parameter-transformations","text":"transformations used implemented using RcppArmadillo. allows lot complicated transformations outlined . general, lessSEM take transformations try translate C++. Let’s assume model given first example: defined transformations : transformation passed lessSEM, lessSEM first try figure parameters already model ones new. case a1, a2, b1, b2, c1, c2 already known, delta_a2, delta_b2, delta_c2 new. lessSEM now add new parameters internal parameter vector. Next, lessSEM scan names parameters appear left hand side equation (a2, b2, c2) case. tell lessSEM parameters functions parameters (.e., transformations). Knowing a2 function parameters tell lessSEM, a2 longer estimated. Instead, parameters make a2 estimated: a1 delta_a2. see action, can create C++ function without compilation: First, let’s look extended parameter vector: Note delta_a2, delta_b2, delta_c2 added. transformations parameters: estimated computed based model parameters. Finally, C++ function returned: importantly, note first step extract required parameters parameter vector (e.g., double a1 = parameterValues[\"a1\"];). Next, parameters directly available use transformations. can simply write a2 = a1 + delta_a2;. lessSEM also added semicolons required C++. Finally, transformed parameters returned. pass function lessSEM, also create pointer function, beyond scope . point may wondering complicated transformations promised . Importantly, can use functions implemented Rcpp RcppArmadillo can applied variables type double within transformations without -depth knowledge C++. instance, RcppArmadillo comes exponential-function (exp), pow log function. Making use , can implement univariate continuous time SEM (e.g., Voelkle & Oud, 2012). Far superior versions model implemented ctsem dynr) Arnold et al. (submission) recently derived close form solutions gradients models outperform lessSEM considerably terms runtime. use model , remove change autoregressive effect. code simulate data set can found source file (e.g., GitHub). initial model , however autoregressive effect constrained equality time manifest means. also added initial mean latent variable \\(\\eta\\) changed names variables make using ctsem data easier: Now, define transformations latent variables turn model continuous time SEM: Let’s look parameter estimates: comparison, run model ctsem: parameter ctA model corresponds DRIFT parameter ctsem summary parameter ctV corresponds root DIFFUSION parameter ctsem summary:","code":"modelSyntax <- '    # latent variable definitions      ind60 =~ x1 + x2 + x3      dem60 =~ y1 + a1*y2 + b1*y3 + c1*y4      dem65 =~ y5 + a2*y6 + b2*y7 + c2*y8    # regressions     dem60 ~ ind60     dem65 ~ ind60 + dem60    # residual correlations     y1 ~~ y5     y2 ~~ y4      y3 ~~ y7     y4 ~~ y8     y6 ~~ y8 '  lavaanFit <- sem(model = modelSyntax,                   data = PoliticalDemocracy) transformations <- \" parameters: a1, a2, b1, b2, c1, c2, delta_a2, delta_b2, delta_c2  a2 = a1 + delta_a2; b2 = b1 + delta_b2; c2 = c1 + delta_c2; \" transformationFunction <- lessSEM:::.compileTransformations(syntax = transformations,                                                              parameterLabels = names(getLavaanParameters(lavaanFit)),                                                             compile = FALSE) transformationFunction$parameters #> [1] \"a1\"       \"a2\"       \"b1\"       \"b2\"       \"c1\"       \"c2\"       \"delta_a2\" #> [8] \"delta_b2\" \"delta_c2\" transformationFunction$isTransformation #> [1] \"a2\" \"b2\" \"c2\" cat(transformationFunction$armaFunction) #>  #>   // [[Rcpp::depends(RcppArmadillo)]] #>   #include <RcppArmadillo.h> #>   // [[Rcpp::export]] #>   Rcpp::NumericVector transformationFunction(Rcpp::NumericVector& parameterValues, Rcpp::List transformationList) #>   { #>   using namespace Rcpp; #>   using namespace arma; #>    #>   // extract required parameters from parameterValues #>    #> double a1 = parameterValues[\"a1\"]; #> double a2 = parameterValues[\"a2\"]; #> double b1 = parameterValues[\"b1\"]; #> double b2 = parameterValues[\"b2\"]; #> double c1 = parameterValues[\"c1\"]; #> double c2 = parameterValues[\"c2\"]; #> double delta_a2 = parameterValues[\"delta_a2\"]; #> double delta_b2 = parameterValues[\"delta_b2\"]; #> double delta_c2 = parameterValues[\"delta_c2\"]; #>  #>  #> // add user defined functions #>  #>  #> a2 = a1 + delta_a2; #> b2 = b1 + delta_b2; #> c2 = c1 + delta_c2; #>  #>  #>  #> // update parameters #> parameterValues[\"a1\"] = a1; #> parameterValues[\"a2\"] = a2; #> parameterValues[\"b1\"] = b1; #> parameterValues[\"b2\"] = b2; #> parameterValues[\"c1\"] = c1; #> parameterValues[\"c2\"] = c2; #> parameterValues[\"delta_a2\"] = delta_a2; #> parameterValues[\"delta_b2\"] = delta_b2; #> parameterValues[\"delta_c2\"] = delta_c2; #>  #>    #>   return(parameterValues); #>   } #>  #>    #>    #>   // https://gallery.rcpp.org/articles/passing-cpp-function-pointers/ #> typedef Rcpp::NumericVector (*transformationFunctionPtr)(Rcpp::NumericVector&, //parameters #> Rcpp::List // transformationList #> ); #>  #> typedef Rcpp::XPtr<transformationFunctionPtr> transformationFunctionPtr_t; #>  #> // [[Rcpp::export]] #> transformationFunctionPtr_t getPtr() { #>         return(transformationFunctionPtr_t(new transformationFunctionPtr(&transformationFunction))); #> } cat(lavaanSyntax) #> eta1 ~ a*eta0 #> eta2 ~ a*eta1 #> eta3 ~ a*eta2 #> eta4 ~ a*eta3 #> eta5 ~ a*eta4 #> eta6 ~ a*eta5 #> eta7 ~ a*eta6 #> eta8 ~ a*eta7 #> eta9 ~ a*eta8 #>  #>  #> eta0 ~~ eta0 #> eta1 ~~ v*eta1 #> eta2 ~~ v*eta2 #> eta3 ~~ v*eta3 #> eta4 ~~ v*eta4 #> eta5 ~~ v*eta5 #> eta6 ~~ v*eta6 #> eta7 ~~ v*eta7 #> eta8 ~~ v*eta8 #> eta9 ~~ v*eta9 #>  #>  #> eta0~1 #>  #> eta0 =~ 1*y1_T0 + l2*y2_T0 + l3*y3_T0 #> y1_T0 ~~ mvar1*y1_T0 #> y2_T0 ~~ mvar2*y2_T0 #> y3_T0 ~~ mvar3*y3_T0 #> y1_T0 ~ mMean1*1 #> y2_T0 ~ mMean2*1 #> y3_T0 ~ mMean3*1 #> eta1 =~ 1*y1_T1 + l2*y2_T1 + l3*y3_T1 #> y1_T1 ~~ mvar1*y1_T1 #> y2_T1 ~~ mvar2*y2_T1 #> y3_T1 ~~ mvar3*y3_T1 #> y1_T1 ~ mMean1*1 #> y2_T1 ~ mMean2*1 #> y3_T1 ~ mMean3*1 #> eta2 =~ 1*y1_T2 + l2*y2_T2 + l3*y3_T2 #> y1_T2 ~~ mvar1*y1_T2 #> y2_T2 ~~ mvar2*y2_T2 #> y3_T2 ~~ mvar3*y3_T2 #> y1_T2 ~ mMean1*1 #> y2_T2 ~ mMean2*1 #> y3_T2 ~ mMean3*1 #> eta3 =~ 1*y1_T3 + l2*y2_T3 + l3*y3_T3 #> y1_T3 ~~ mvar1*y1_T3 #> y2_T3 ~~ mvar2*y2_T3 #> y3_T3 ~~ mvar3*y3_T3 #> y1_T3 ~ mMean1*1 #> y2_T3 ~ mMean2*1 #> y3_T3 ~ mMean3*1 #> eta4 =~ 1*y1_T4 + l2*y2_T4 + l3*y3_T4 #> y1_T4 ~~ mvar1*y1_T4 #> y2_T4 ~~ mvar2*y2_T4 #> y3_T4 ~~ mvar3*y3_T4 #> y1_T4 ~ mMean1*1 #> y2_T4 ~ mMean2*1 #> y3_T4 ~ mMean3*1 #> eta5 =~ 1*y1_T5 + l2*y2_T5 + l3*y3_T5 #> y1_T5 ~~ mvar1*y1_T5 #> y2_T5 ~~ mvar2*y2_T5 #> y3_T5 ~~ mvar3*y3_T5 #> y1_T5 ~ mMean1*1 #> y2_T5 ~ mMean2*1 #> y3_T5 ~ mMean3*1 #> eta6 =~ 1*y1_T6 + l2*y2_T6 + l3*y3_T6 #> y1_T6 ~~ mvar1*y1_T6 #> y2_T6 ~~ mvar2*y2_T6 #> y3_T6 ~~ mvar3*y3_T6 #> y1_T6 ~ mMean1*1 #> y2_T6 ~ mMean2*1 #> y3_T6 ~ mMean3*1 #> eta7 =~ 1*y1_T7 + l2*y2_T7 + l3*y3_T7 #> y1_T7 ~~ mvar1*y1_T7 #> y2_T7 ~~ mvar2*y2_T7 #> y3_T7 ~~ mvar3*y3_T7 #> y1_T7 ~ mMean1*1 #> y2_T7 ~ mMean2*1 #> y3_T7 ~ mMean3*1 #> eta8 =~ 1*y1_T8 + l2*y2_T8 + l3*y3_T8 #> y1_T8 ~~ mvar1*y1_T8 #> y2_T8 ~~ mvar2*y2_T8 #> y3_T8 ~~ mvar3*y3_T8 #> y1_T8 ~ mMean1*1 #> y2_T8 ~ mMean2*1 #> y3_T8 ~ mMean3*1 #> eta9 =~ 1*y1_T9 + l2*y2_T9 + l3*y3_T9 #> y1_T9 ~~ mvar1*y1_T9 #> y2_T9 ~~ mvar2*y2_T9 #> y3_T9 ~~ mvar3*y3_T9 #> y1_T9 ~ mMean1*1 #> y2_T9 ~ mMean2*1 #> y3_T9 ~ mMean3*1 lavaanFit <- sem(model = lavaanSyntax,                   data = data,                  orthogonal.y = TRUE,                   orthogonal.x = TRUE,                  missing = \"ml\") getLavaanParameters(lavaanFit) #>            a   eta0~~eta0            v       eta0~1           l2           l3  #>  0.592770935  0.873870399  0.387065602 -0.091482670  0.578004929  0.745298413  #>        mvar1        mvar2        mvar3       mMean1       mMean2       mMean3  #>  0.127999058  0.750360430  0.577566347 -0.026015718  0.005858067 -0.027032832 transformations <- \" parameters: a, ctA, v, ctV // NOTE: We can define starting values for our parameters. This // is implemented with the 'start:' keyword: start: ctA = -.1, ctV = .1  // We changed the starting values for the ct parameters // because the auto-effect ctA should be negative.  a = exp(ctA); v = log((1.0/(2.0*ctA))*(exp(2.0*ctA)-1)*pow(ctV,2.0)); // we take // the log because lessSEM internally takes the exponential of // any variance parameter (v in our case) to avoid negative variances. \" lessSEMFit <- bfgs(lavaanModel = lavaanFit,                     # Our model modification must make use of the modifyModel - function:                    modifyModel = modifyModel(transformations = transformations) ) coef(lessSEMFit) #>                                                                                #>   Tuning         ||--||  Estimates                                             #>  ------- ------- ||--|| ---------- ---------- ---------- ---------- ---------- #>   lambda   alpha ||--|| eta0~~eta0     eta0~1         l2         l3      mvar1 #>  ======= ======= ||--|| ========== ========== ========== ========== ========== #>   0.0000  0.0000 ||--||     0.8736    -0.0902     0.5780     0.7453     0.1280 #>                                                                               #>                                                                               #>  ---------- ---------- ---------- ---------- ---------- ---------- ---------- #>       mvar2      mvar3     mMean1     mMean2     mMean3        ctA        ctV #>  ========== ========== ========== ========== ========== ========== ========== #>      0.7504     0.5775    -0.0267     0.0054    -0.0276    -0.5228     0.7899 library(ctsemOMX) dataCt <- cbind(data,                 data.frame(\"dT1\" = rep(1,nrow(data)),                            \"dT2\" = rep(1,nrow(data)),                            \"dT3\" = rep(1,nrow(data)),                            \"dT4\" = rep(1,nrow(data)),                            \"dT5\" = rep(1,nrow(data)),                            \"dT6\" = rep(1,nrow(data)),                            \"dT7\" = rep(1,nrow(data)),                            \"dT8\" = rep(1,nrow(data)),                            \"dT9\" = rep(1,nrow(data)))) cModel <- ctModel(type = \"omx\",                    n.manifest = 3,                    n.latent = 1,                    Tpoints = 10,                   manifestNames = c(\"y1\",\"y2\", \"y3\"),                    latentNames = \"eta\",                   LAMBDA = matrix(c(1,                                     \"l2\",                                     \"l3\"),3,1,TRUE),                    DRIFT = matrix(\"a\",1,1) )  cFit <- ctFit(dat = dataCt, ctmodelobj = cModel) ctSummary <- summary(cFit) coef(lessSEMFit)@estimates[,c(\"ctA\", \"ctV\")] #>        ctA        ctV  #> -0.5228340  0.7899115  # drift value from ctsem: ctSummary$DRIFT #>            eta #> eta -0.5229469 # sqrt(diffusion) value from ctsem: sqrt(ctSummary$DIFFUSION) #>           eta #> eta 0.7900236"},{"path":"/articles/Parameter-transformations.html","id":"making-use-of-c","dir":"Articles","previous_headings":"Looking under the hood","what":"Making use of C++","title":"Parameter-transformations","text":"example , used univariate ctsem. , functions fairly simple needed log, exp, pow functions single variables. However, lessSEM creates C++ function, can build much powerful transformations familiar RcppArmadillo. following, therefore extend example multivariate continuous time SEM. use AnomAuth data set ctsem. data included ctsemOMX package: five measurement occasions unequally spaced. discrete time model, take care implementing model different autoregressive cross-lagged effects different time intervals: Setting model lavaan: transform parameters continuous time model, define transformations . go details . See Voelkle et al. (2012) underlying transformations. importantly, used matrices transformations. possible, lessSEM uses RcppArmadillo background therefore also provides users functions implemented therein. , can now fit model lessSEM comparison, also fit model ctsemOMX: following matrices drifts lessSEM model ctsemOMX model: lessSEM: ctsemOMX: diffusions given : lessSEM: ctsemOMX: Regularization used enforce sparsity (Orzek & Voelkle, review):  , steps easier using dedicated packages ctsem ctsemOMX continuous time SEM regCtsem (Orzek & Voelkle, review) regularized continuous time SEM.","code":"data(\"AnomAuth\") head(AnomAuth) #>   Y1_T0 Y2_T0 Y1_T1 Y2_T1 Y1_T2 Y2_T2 Y1_T3 Y2_T3 Y1_T4 Y2_T4 dT1 dT2 dT3 dT4 #> 1  2.67  3.50  3.33   3.5    NA    NA    NA    NA    NA    NA   1   1   2   2 #> 2  3.33  3.25    NA    NA    NA    NA    NA    NA    NA    NA   1   1   2   2 #> 3  3.33  2.75  3.33   3.0  3.33   2.5  2.33     3  2.33     3   1   1   2   2 #> 4  3.33  3.25    NA    NA    NA    NA    NA    NA    NA    NA   1   1   2   2 #> 5  4.00  4.00    NA    NA    NA    NA    NA    NA    NA    NA   1   1   2   2 #> 6  3.67  4.00    NA    NA    NA    NA  4.00     4  4.00     4   1   1   2   2 # initial time point lavaanSyntax <-    \"eta1_T0 =~ 1 * Y1_T0 eta2_T0 =~ 1 * Y2_T0 Y1_T0 ~~ 0*Y1_T0 Y2_T0 ~~ 0*Y2_T0\\n\"  # variances lavaanSyntax <- c(lavaanSyntax,                   \"eta1_T0 ~~ v0_11 * eta1_T0 + v0_12 * eta2_T0\\neta2_T0 ~~ v0_22 * eta2_T0\\n\" )  # means lavaanSyntax <- c(lavaanSyntax,                   \"eta1_T0 ~ 1\\neta2_T0 ~ 1\\nY1_T0~mMean1*1\\nY2_T0~mMean2*1\\n\"  )  for(tp in c(0,1,2,3)){   if(tp < 2) {     a <- \"a1\"     v <- \"v1\"   }else{     a <-\"a2\"     v <- \"v2\"   }      # autoregressive and cross-lagged   lavaanSyntax <- c(lavaanSyntax,                     paste0(                       \"eta1_T\", tp+1, \" ~ \", a, \"_11 * eta1_T\", tp, \" + \", a, \"_12 * eta2_T\", tp,\"\\n\",                       \"eta2_T\", tp+1, \" ~ \", a, \"_21 * eta1_T\", tp, \" + \", a, \"_22 * eta2_T\", tp, \"\\n\"                     )   )      # variances   lavaanSyntax <- c(lavaanSyntax,                     paste0(                       \"eta1_T\", tp+1, \" ~~ \", v, \"_11 * eta1_T\", tp+1, \" + \", v, \"_12 * eta2_T\", tp+1,\"\\n\",                       \"eta2_T\", tp+1, \" ~~ \", v, \"_22 * eta2_T\", tp+1, \"\\n\"                     )   )      # loadings   lavaanSyntax <- c(lavaanSyntax,                     paste0(                       \"eta1_T\", tp+1, \" =~ 1 * Y1_T\", tp+1,\"\\n\",                       \"eta2_T\", tp+1, \" =~ 1 * Y2_T\", tp+1,\"\\n\"                     )   )      # manifest variances   lavaanSyntax <- c(lavaanSyntax,                     paste0(                       \"Y1_T\", tp+1, \" ~~ 0* Y1_T\", tp+1, \"\\n\",                       \"Y2_T\", tp+1, \" ~~ 0* Y2_T\", tp+1, \"\\n\"                     )   )      # manifest means   lavaanSyntax <- c(lavaanSyntax,                     paste0(                       \"Y1_T\", tp+1, \" ~ mMean1 * 1\\n\",                       \"Y2_T\", tp+1, \" ~ mMean2 * 1\\n\"                     )   ) } lavaanSyntax <- paste0(lavaanSyntax, collapse = \"\") cat(lavaanSyntax) #> eta1_T0 =~ 1 * Y1_T0 #> eta2_T0 =~ 1 * Y2_T0 #> Y1_T0 ~~ 0*Y1_T0 #> Y2_T0 ~~ 0*Y2_T0 #> eta1_T0 ~~ v0_11 * eta1_T0 + v0_12 * eta2_T0 #> eta2_T0 ~~ v0_22 * eta2_T0 #> eta1_T0 ~ 1 #> eta2_T0 ~ 1 #> Y1_T0~mMean1*1 #> Y2_T0~mMean2*1 #> eta1_T1 ~ a1_11 * eta1_T0 + a1_12 * eta2_T0 #> eta2_T1 ~ a1_21 * eta1_T0 + a1_22 * eta2_T0 #> eta1_T1 ~~ v1_11 * eta1_T1 + v1_12 * eta2_T1 #> eta2_T1 ~~ v1_22 * eta2_T1 #> eta1_T1 =~ 1 * Y1_T1 #> eta2_T1 =~ 1 * Y2_T1 #> Y1_T1 ~~ 0* Y1_T1 #> Y2_T1 ~~ 0* Y2_T1 #> Y1_T1 ~ mMean1 * 1 #> Y2_T1 ~ mMean2 * 1 #> eta1_T2 ~ a1_11 * eta1_T1 + a1_12 * eta2_T1 #> eta2_T2 ~ a1_21 * eta1_T1 + a1_22 * eta2_T1 #> eta1_T2 ~~ v1_11 * eta1_T2 + v1_12 * eta2_T2 #> eta2_T2 ~~ v1_22 * eta2_T2 #> eta1_T2 =~ 1 * Y1_T2 #> eta2_T2 =~ 1 * Y2_T2 #> Y1_T2 ~~ 0* Y1_T2 #> Y2_T2 ~~ 0* Y2_T2 #> Y1_T2 ~ mMean1 * 1 #> Y2_T2 ~ mMean2 * 1 #> eta1_T3 ~ a2_11 * eta1_T2 + a2_12 * eta2_T2 #> eta2_T3 ~ a2_21 * eta1_T2 + a2_22 * eta2_T2 #> eta1_T3 ~~ v2_11 * eta1_T3 + v2_12 * eta2_T3 #> eta2_T3 ~~ v2_22 * eta2_T3 #> eta1_T3 =~ 1 * Y1_T3 #> eta2_T3 =~ 1 * Y2_T3 #> Y1_T3 ~~ 0* Y1_T3 #> Y2_T3 ~~ 0* Y2_T3 #> Y1_T3 ~ mMean1 * 1 #> Y2_T3 ~ mMean2 * 1 #> eta1_T4 ~ a2_11 * eta1_T3 + a2_12 * eta2_T3 #> eta2_T4 ~ a2_21 * eta1_T3 + a2_22 * eta2_T3 #> eta1_T4 ~~ v2_11 * eta1_T4 + v2_12 * eta2_T4 #> eta2_T4 ~~ v2_22 * eta2_T4 #> eta1_T4 =~ 1 * Y1_T4 #> eta2_T4 =~ 1 * Y2_T4 #> Y1_T4 ~~ 0* Y1_T4 #> Y2_T4 ~~ 0* Y2_T4 #> Y1_T4 ~ mMean1 * 1 #> Y2_T4 ~ mMean2 * 1 lavaanFit <- sem(model = lavaanSyntax, data = AnomAuth,                  orthogonal.y = TRUE,                   orthogonal.x = TRUE,                  missing = \"ml\",                  do.fit = FALSE) transformations <- \" // Define all parameters which we want to use: parameters: a1_11, a1_12, a1_21, a1_22, a2_11, a2_12, a2_21, a2_22,  ctA_11, ctA_12, ctA_21, ctA_22,  v1_11, v1_12, v1_22, v2_11, v2_12, v2_22,  ctV_11, ctV_12, ctV_22  // Define the starting values for the continuous time parameters: start: ctA_11 = -1, ctA_12 = 0, ctA_21 = 0, ctA_22 = -1,  ctV_11 = .1, ctV_12 = 0, ctV_22 = .1  // transformations: arma::mat drift(2,2); arma::mat ARCL1(2,2); arma::mat ARCL2(2,2); arma::mat driftHash(4,4); drift(0,0) = ctA_11; drift(1,0) = ctA_21; drift(0,1) = ctA_12; drift(1,1) = ctA_22; ARCL1 = expmat(drift); ARCL2 = expmat(drift*2.0);  driftHash = kron(drift, arma::eye(2,2)) + kron(arma::eye(2,2), drift);  arma::mat diffusion(2,2); arma::mat discreteDiff1(2,2); arma::mat discreteDiff2(2,2); diffusion(0,0) = ctV_11; diffusion(1,0) = ctV_12; diffusion(0,1) = ctV_12; diffusion(1,1) = ctV_22; discreteDiff1 = arma::reshape(arma::inv(driftHash) *    (expmat(driftHash) - arma::eye(arma::size(expmat(driftHash))))*   arma::vectorise(diffusion),2,2); discreteDiff2 = arma::reshape(arma::inv(driftHash) *    (expmat(driftHash*2.0) - arma::eye(arma::size(expmat(driftHash*2.0))))*   arma::vectorise(diffusion),2,2);  // extract parameters  a1_11 = ARCL1(0,0); a1_12 = ARCL1(0,1); a1_21 = ARCL1(1,0); a1_22 = ARCL1(1,1);  a2_11 = ARCL2(0,0); a2_12 = ARCL2(0,1); a2_21 = ARCL2(1,0); a2_22 = ARCL2(1,1);  v1_11 = log(discreteDiff1(0,0)); // we take the log because of the internal  // transformation in lessSEM v1_12 = discreteDiff1(0,1); v1_22 = log(discreteDiff1(1,1)); // we take the log because of the internal  // transformation in lessSEM  v2_11 = log(discreteDiff2(0,0)); // we take the log because of the internal  // transformation in lessSEM v2_12 = discreteDiff2(0,1); v2_22 = log(discreteDiff2(1,1)); // we take the log because of the internal  // transformation in lessSEM \" lessSEMFit <- bfgs(lavaanModel = lavaanFit,                    # Our model modification must make use of the modifyModel - function:                    modifyModel = modifyModel(transformations = transformations) ) AnomAuthmodel <- ctModel(LAMBDA = matrix(c(1, 0, 0, 1), nrow = 2, ncol = 2),                           Tpoints = 5, n.latent = 2, n.manifest = 2, MANIFESTVAR=diag(0, 2), TRAITVAR = NULL)  AnomAuthfit <- ctFit(AnomAuth, AnomAuthmodel) matrix(coef(lessSEMFit)@estimates[,c(\"ctA_11\", \"ctA_21\", \"ctA_12\", \"ctA_22\")],2,2) #>             [,1]       [,2] #> [1,] -0.44814616  0.2320881 #> [2,]  0.04260196 -0.1171318 AnomAuthfit$mxobj$DRIFT$values #>             [,1]       [,2] #> [1,] -0.44728184  0.2324980 #> [2,]  0.04329283 -0.1174662 matrix(coef(lessSEMFit)@estimates[,c(\"ctV_11\", \"ctV_12\", \"ctV_12\", \"ctV_22\")],2,2) #>              [,1]         [,2] #> [1,]  0.473528285 -0.003850882 #> [2,] -0.003850882  0.154513444 AnomAuthfit$mxobj$DIFFUSIONchol$result%*%t(AnomAuthfit$mxobj$DIFFUSIONchol$result) #>              [,1]         [,2] #> [1,]  0.473241884 -0.004610149 #> [2,] -0.004610149  0.154509547 lassoFit <- lasso(lavaanModel = lavaanFit,                    regularized = \"ctA_21\",                   nLambdas = 30,                   method = \"glmnet\",                   control = controlGlmnet(),                   # Our model modification must make use of the modifyModel - function:                   modifyModel = modifyModel(transformations = transformations) ) plot(lassoFit)"},{"path":"/articles/Parameter-transformations.html","id":"how-it-is-implemented","dir":"Articles","previous_headings":"","what":"How it is implemented","title":"Parameter-transformations","text":"basic idea behind transformations follows: Assume SEM parameters given \\(\\pmb\\theta\\). 2-log-likelihood model given \\(f(\\pmb\\theta)\\). using transformations, redefine \\(\\pmb\\theta\\) function parameters, say \\(\\pmb\\gamma\\). , \\(\\pmb\\theta =\\pmb g(\\pmb\\gamma)\\), \\(\\pmb g\\) function returning vector. result, can re-write fitting function \\(f(\\pmb\\theta) = f(\\pmb g(\\pmb\\gamma))\\). Instead optimizing \\(\\pmb\\theta\\), now optimize \\(\\pmb\\gamma\\). Within lessSEM, gradients \\(f(\\pmb\\theta)\\) respect \\(\\pmb\\theta\\) implemented closed form results considerably faster run time. get gradients \\(f(\\pmb\\theta) = f(\\pmb g(\\pmb\\gamma))\\) respect \\(\\pmb\\gamma\\), lessSEM makes use chain rule. gradients transformation \\(\\pmb g(\\pmb\\gamma)\\) approximated numerically. similar procedure Arnold et al. (submission) continuous time SEM, derivative matrix exponential approximated numerically, elements derived closed form solutions.","code":""},{"path":"/articles/Parameter-transformations.html","id":"bibliography","dir":"Articles","previous_headings":"","what":"Bibliography","title":"Parameter-transformations","text":"Arnold, M., Cancér, P. F., Estrada, E., & Voelkle, M. C. (submission). Score-Guided Recursive Partitioning Continuous-Time Structural Equation Models. Bai, P., Safikhani, ., & Michailidis, G. (2020). Multiple Change Points Detection Low Rank Sparse High Dimensional Vector Autoregressive Models. IEEE Transactions Signal Processing, 68, 3074–3089. https://doi.org/10.1109/TSP.2020.2993145 Belzak, W. C. M., & Bauer, D. J. (2020). Improving assessment measurement invariance: Using regularization select anchor items identify differential item functioning. Psychological Methods, 25(6), 673–690. https://doi.org/10.1037/met0000253 Driver, C. C., Oud, J. H. L., & Voelkle, M. C. (2017). Continuous time structural equation modelling R package ctsem. Journal Statistical Software, 77(5), 1–36. https://doi.org/10.18637/jss.v077.i05 Fisher, Z. F., Kim, Y., Fredrickson, B. L., & Pipiras, V. (2022). Penalized Estimation Forecasting Multiple Subject Intensive Longitudinal Data. Psychometrika, 87(2), 1–29. https://doi.org/10.1007/s11336-021-09825-7 Huang, P.-H. (2018). penalized likelihood method multi-group structural equation modelling. British Journal Mathematical Statistical Psychology, 71(3), 499–522. https://doi.org/10.1111/bmsp.12130 Jacobucci, R., & Grimm, K. J. (2018). Regularized Estimation Multivariate Latent Change Score Models. E. Ferrer, S. M. Boker, & K. J. Grimm (Eds.), Longitudinal Multivariate Psychology (1st ed., pp. 109–125). Routledge. https://doi.org/10.4324/9781315160542-6 Jacobucci, R., Grimm, K. J., Brandmaier, . M., Serang, S., Kievit, R. ., & Scharf, F. (2019). regsem: Regularized structural equation modeling. https://CRAN.R-project.org/package=regsem Liang, X., Yang, Y., & Huang, J. (2018). Evaluation Structural Relationships Autoregressive Cross-Lagged Models Longitudinal Approximate Invariance:Bayesian Analysis. Structural Equation Modeling: Multidisciplinary Journal, 25(4), 558–572. https://doi.org/10.1080/10705511.2017.1410706 Muthen, B., & Asparouhov, T. (2013). BSEM Measurement Invariance Analysis. Mplus Web Notes: . 17. Ou, L., Hunter, M., D., & Chow, S.-M. (2019). Whats dynr: package linear nonlinear dynamic modeling r. R Journal, 11(1), 91–111. https://doi.org/10.32614/RJ-2019-012 Orzek, J. H., & Voelkle, M. C. (review). Regularized continuous time structural equation models: network perspective. Voelkle, M. C., Oud, J. H. L., Davidov, E., & Schmidt, P. (2012). sem approach continuous time modeling panel data: Relating authoritarianism anomia. Psychological Methods, 17(2), 176–192. https://doi.org/10.1037/a0027543","code":""},{"path":"/articles/SCAD-and-MCP.html","id":"scad","dir":"Articles","previous_headings":"","what":"SCAD","title":"SCAD-and-MCP","text":"scad penalty given : \\[p(x) = \\begin{cases} \\lambda |x| & \\text{} |x| \\leq \\theta\\\\ \\frac{-x^2 + 2\\theta\\lambda |x| - \\lambda^2}{2(\\theta -1)} & \\text{} \\lambda < |x| < \\lambda\\theta\\\\ (\\theta + 1) \\lambda^2/2 & \\text{} |x| \\geq \\theta\\lambda\\\\ \\end{cases}\\] \\(\\theta > 2\\) \\(\\lambda \\geq 0\\). proximal operator searching solution function \\(\\hat x = \\arg\\min_x \\frac 12 (x-u)^2 + \\frac 1t p(x)\\). idea Gong et al. (2013) minimze function regions mentioned compare minima find global minimum.","code":""},{"path":"/articles/SCAD-and-MCP.html","id":"assume-x-leq-lambda","dir":"Articles","previous_headings":"SCAD","what":"Assume: \\(|x| \\leq \\lambda\\)","title":"SCAD-and-MCP","text":"Assuming solution region \\(|x| \\leq \\lambda\\), scad identical lasso. follows: \\[\\hat x = \\text{sign} (u)\\max(0,|u|-\\lambda / t)\\] also take border \\(|x| \\leq \\lambda\\) account, follows: \\(x \\geq 0\\): \\(\\hat x = \\min(\\lambda, \\text{sign} (u)\\max(0,|u|-\\lambda / t))\\) \\(x \\leq 0\\): \\(\\hat x = \\max(-\\lambda, \\text{sign} (u)\\max(0,|u|-\\lambda / t))\\) Combined: \\[\\hat x = \\text{sign}(u)\\max(\\lambda, \\max(0,|u|-\\lambda / t))\\]","code":""},{"path":"/articles/SCAD-and-MCP.html","id":"assume-lambda-x-leq-lambdatheta","dir":"Articles","previous_headings":"SCAD","what":"Assume: \\(\\lambda < |x| \\leq \\lambda\\theta\\)","title":"SCAD-and-MCP","text":"Assuming solution region \\(\\lambda < |x| \\leq \\lambda\\theta\\), critical section absolute value function (\\(x=0\\)) avoided. Therefore, derivative respect \\(x\\) defined can set zero. solution given : \\[ \\hat x = \\begin{cases} \\frac{u}{v} - \\frac{\\theta\\lambda}{t(\\theta-1)v} & \\text{} \\lambda < x <= \\lambda\\theta\\\\ \\frac{u}{v} + \\frac{\\theta\\lambda}{t(\\theta-1)v} & \\text{}  -\\theta > x > -\\lambda\\theta \\\\ \\end{cases} \\] \\(v = (1-\\frac{1}{t(\\theta-1)})\\). Also accounting borders gives: \\[ \\hat x = \\begin{cases} \\min(\\lambda\\theta, \\max(\\lambda, \\frac{u}{v} - \\frac{\\theta\\lambda}{t(\\theta-1)v}) & \\text{} x \\geq 0\\\\ \\max(-\\lambda\\theta, \\min(-\\lambda, \\frac{u}{v} + \\frac{\\theta\\lambda}{t(\\theta-1)v}) & \\text{} x \\leq 0\\\\ \\end{cases} \\] Derivation: penalty given \\(\\frac{-x^2 + 2\\theta\\lambda |x| - \\lambda^2}{2(\\theta -1)}\\). Differentiation respect \\(x\\) gives: \\[ \\begin{aligned} & \\frac{(-2x + 2\\theta\\lambda \\text{sign}(x))*2(\\theta -1)}{(2(\\theta -1))^2} \\\\ &= \\frac{-x + \\theta\\lambda \\text{sign}(x)}{(\\theta -1)} \\end{aligned} \\] (Note: indicated , \\(x \\neq 0\\) \\(\\lambda < |x| \\leq \\lambda\\theta\\). \\(\\partial |x| = \\text{sign}(x)*1\\).) Now combine differentiation penalty differentiation \\(\\frac 12 (x-u)^2\\) set 0: \\[x-u + \\frac 1t \\frac{-x + \\theta\\lambda \\text{sign}(x)}{(\\theta -1)} := 0\\] get equations solution.","code":""},{"path":"/articles/SCAD-and-MCP.html","id":"assume-x-geq-thetalambda","dir":"Articles","previous_headings":"SCAD","what":"Assume: \\(|x| \\geq \\theta\\lambda\\)","title":"SCAD-and-MCP","text":"\\(x \\neq 0\\), differentiation respect \\(x\\) defined. penalty given \\((\\theta + 1) \\lambda^2/2\\). Differentiating respect \\(x\\) gives \\(0\\) solution. follows: \\[x-u + \\frac 1t 0 := 0 \\Rightarrow \\hat x = u\\] Respecting borders: \\[\\hat x = \\text{sign}(u) \\min(\\theta\\lambda, |u|)\\]","code":""},{"path":"/articles/SCAD-and-MCP.html","id":"combining-the-solutions","dir":"Articles","previous_headings":"SCAD","what":"combining the solutions","title":"SCAD-and-MCP","text":"now minima section penalty function. find global minimum, compute \\(\\frac 12 (x-u)^2 + \\frac 1t p(x)\\) proposed solution select one results smallest value.","code":""},{"path":"/articles/SCAD-and-MCP.html","id":"mcp","dir":"Articles","previous_headings":"","what":"MCP","title":"SCAD-and-MCP","text":"MCP defined \\[ p(x) = \\begin{cases} \\lambda |x| - x^2/(2\\theta) & \\text{} |x| \\leq \\theta\\lambda\\\\ \\theta\\lambda^2/2 & \\text{} |x| > \\lambda\\theta \\end{cases}; \\theta > 0 \\]","code":""},{"path":[]},{"path":"/articles/SCAD-and-MCP.html","id":"assume-that-the-solution-is-given-by-x-geq-0-","dir":"Articles","previous_headings":"MCP > Assume: \\(|x| \\leq \\theta\\lambda\\)","what":"Assume that the solution is given by \\(x \\geq 0\\).","title":"SCAD-and-MCP","text":"\\(\\frac{\\partial}{\\partial x}p(x) = \\lambda - \\frac x\\theta\\). follows minimum \\(f(x) = \\frac 12 (x-u)^2 + \\frac 1t p(x)\\) given \\[ \\begin{aligned} x-u + \\frac 1t (\\lambda - \\frac x\\theta) &:= 0\\\\ \\Rightarrow x = \\frac u v - \\frac{1}{tv}\\lambda \\end{aligned} \\] \\(v = 1-\\frac{1}{\\theta t}\\) Respecting borders: \\[x = \\max(0,\\min(\\frac u v - \\frac{1}{tv}\\lambda, \\theta\\lambda))\\]","code":""},{"path":"/articles/SCAD-and-MCP.html","id":"assume-that-the-solution-is-given-by-x-leq-0-","dir":"Articles","previous_headings":"MCP > Assume: \\(|x| \\leq \\theta\\lambda\\)","what":"Assume that the solution is given by \\(x \\leq 0\\).","title":"SCAD-and-MCP","text":"\\(\\frac{\\partial}{\\partial x}p(x) = -\\lambda - \\frac x\\theta\\). follows minimum \\(f(x) = \\frac 12 (x-u)^2 + \\frac 1t p(x)\\) given \\[ \\begin{aligned} x-u + \\frac 1t (-\\lambda - \\frac x\\theta) &:= 0\\\\ \\Rightarrow x = \\frac u v + \\frac{1}{tv}\\lambda \\end{aligned} \\] \\(v = 1-\\frac{1}{\\theta t}\\) Respecting borders: \\[x = \\min(0,\\max(\\frac u v + \\frac{1}{tv}\\lambda, -\\theta\\lambda))\\]","code":""},{"path":"/articles/SCAD-and-MCP.html","id":"assume-x-geq-thetalambda-1","dir":"Articles","previous_headings":"MCP","what":"Assume \\(|x| \\geq \\theta\\lambda\\)","title":"SCAD-and-MCP","text":"case, differentiation respect \\(x\\) well defined. get \\(\\frac{\\partial}{\\partial x}p(x) = 0\\) \\(x = u\\) minimum function. Respecting borders: \\[x = \\text{sign}(u)\\max(\\theta\\lambda, |u|)\\] Finally, going compare proposed minima select one actually minimizes function.","code":""},{"path":"/articles/SCAD-and-MCP.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"SCAD-and-MCP","text":"Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). general iterative shrinkage thresholding algorithm non-convex regularized optimization problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/articles/The-Structural-Equation-Model.html","id":"from-lavaan-to-lesssem","dir":"Articles","previous_headings":"","what":"From lavaan to lessSEM","title":"The-Structural-Equation-Model","text":"translate model lavaan lessSEM, use lessSEM:::.SEMFromLavaan function. Importantly, function exported lessSEM. , must use three colons shown access function! lessSEM:::.SEMFromLavaan function comes additional arguments fine tune initialization model. whichPars: whichPars arguments, can change parameters used mySEM created . default, use estimates (whichPars = \"est\") lavaan model, also use starting values (whichPars = \"start\") supply custom parameter values fit: fit = TRUE, lessSEM fit model compare fitting function value lavaanModel. supplied parameters “est”, set fit = FALSE addMeans: mean structure added? currenlty recomended set TRUE activeSet: allows using part data set. can useful cross-validation. dataSet: allows passing different data set mySEM. can useful cross-validation. cases, recommend setting model shown , none additional arguments used.","code":"library(lessSEM)  # won't work: mySEM <- .SEMFromLavaan(lavaanModel = lavaanModel) #> Error in .SEMFromLavaan(lavaanModel = lavaanModel): could not find function \".SEMFromLavaan\"  # will work: mySEM <- lessSEM:::.SEMFromLavaan(lavaanModel = lavaanModel) show(mySEM) #> Internal C++ model representation of lessSEM #> Parameters: #>    ind60=~x2    ind60=~x3            a            b            c  dem60~ind60  #>    2.1796566    1.8182100    1.1907820    1.1745407    1.2509789    1.4713302  #>  dem65~ind60  dem65~dem60       y1~~y5       y2~~y4       y2~~y6       y3~~y7  #>    0.6004746    0.8650430    0.5825389    1.4402477    2.1829448    0.7115901  #>       y4~~y8       y6~~y8       x1~~x1       x2~~x2       x3~~x3       y1~~y1  #>    0.3627964    1.3717741    0.0813878    0.1204271    0.4666596    1.8546417  #>       y2~~y2       y3~~y3       y4~~y4       y5~~y5       y6~~y6       y7~~y7  #>    7.5813926    4.9556766    3.2245521    2.3130404    4.9681408    3.5600367  #>       y8~~y8 ind60~~ind60 dem60~~dem60 dem65~~dem65         x1~1         x2~1  #>    3.3076854    0.4485989    3.8753039    0.1644633    5.0543838    4.7921946  #>         x3~1         y1~1         y2~1         y3~1         y4~1         y5~1  #>    3.5576898    5.4646667    4.2564429    6.5631103    4.4525330    5.1362519  #>         y6~1         y7~1         y8~1  #>    2.9780741    6.1962639    4.0433897  #>  #> -2 log-Likelihood: 3097.6361581071"},{"path":"/articles/The-Structural-Equation-Model.html","id":"working-with-the-rcpp_semcpp-class","dir":"Articles","previous_headings":"","what":"Working with the Rcpp_SEMCpp class","title":"The-Structural-Equation-Model","text":"mySEM object implemented C++ make everything run faster. underlying class Rcpp_SEMCpp created using wonderful Rcpp RcppArmadillo packages. can access elements using dollar-operator: Note , identical regsem, model implemented RAM notation (McArdle & McDonald, 1984). familiar notation, Fox (2006) provides short introduction. However, won’t need know details time . Instead, focus get set parameters, fit model, get gradients, etc.","code":"class(mySEM) #> [1] \"Rcpp_SEMCpp\" #> attr(,\"package\") #> [1] \"lessSEM\" mySEM$A #>            [,1]     [,2]     [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] #>  [1,] 0.0000000 0.000000 0.000000    0    0    0    0    0    0     0     0 #>  [2,] 1.4713302 0.000000 0.000000    0    0    0    0    0    0     0     0 #>  [3,] 0.6004746 0.865043 0.000000    0    0    0    0    0    0     0     0 #>  [4,] 1.0000000 0.000000 0.000000    0    0    0    0    0    0     0     0 #>  [5,] 2.1796566 0.000000 0.000000    0    0    0    0    0    0     0     0 #>  [6,] 1.8182100 0.000000 0.000000    0    0    0    0    0    0     0     0 #>  [7,] 0.0000000 1.000000 0.000000    0    0    0    0    0    0     0     0 #>  [8,] 0.0000000 1.190782 0.000000    0    0    0    0    0    0     0     0 #>  [9,] 0.0000000 1.174541 0.000000    0    0    0    0    0    0     0     0 #> [10,] 0.0000000 1.250979 0.000000    0    0    0    0    0    0     0     0 #> [11,] 0.0000000 0.000000 1.000000    0    0    0    0    0    0     0     0 #> [12,] 0.0000000 0.000000 1.190782    0    0    0    0    0    0     0     0 #> [13,] 0.0000000 0.000000 1.174541    0    0    0    0    0    0     0     0 #> [14,] 0.0000000 0.000000 1.250979    0    0    0    0    0    0     0     0 #>       [,12] [,13] [,14] #>  [1,]     0     0     0 #>  [2,]     0     0     0 #>  [3,]     0     0     0 #>  [4,]     0     0     0 #>  [5,]     0     0     0 #>  [6,]     0     0     0 #>  [7,]     0     0     0 #>  [8,]     0     0     0 #>  [9,]     0     0     0 #> [10,]     0     0     0 #> [11,]     0     0     0 #> [12,]     0     0     0 #> [13,]     0     0     0 #> [14,]     0     0     0"},{"path":"/articles/The-Structural-Equation-Model.html","id":"accessing-the-parameters","dir":"Articles","previous_headings":"Working with the Rcpp_SEMCpp class","what":"Accessing the Parameters","title":"The-Structural-Equation-Model","text":"parameters model can accessed lessSEM:::.getParameters function: naming identical lavaanModel. default, parameters returned transformed format. requires explanation: lessSEM assume negative variances outside parameter space. , negative variances allowed (different lavaan!). ensure variances positive, use transformation: Say interested variance ind60~~ind60. Internally, parameter called x1~~x1 parameter rawValue transformed value (called just value). can access values : parameters variances, rawValue identical value. variances, rawValue can real value. value computed \\(e^{\\text{rawValue}}\\); ensures value always positive. can access raw values follows: Note raw value ind60~~ind60 negative transformed value positive.","code":"(myParameters <- lessSEM:::.getParameters(mySEM)) #>    ind60=~x2    ind60=~x3            a            b            c  dem60~ind60  #>    2.1796566    1.8182100    1.1907820    1.1745407    1.2509789    1.4713302  #>  dem65~ind60  dem65~dem60       y1~~y5       y2~~y4       y2~~y6       y3~~y7  #>    0.6004746    0.8650430    0.5825389    1.4402477    2.1829448    0.7115901  #>       y4~~y8       y6~~y8       x1~~x1       x2~~x2       x3~~x3       y1~~y1  #>    0.3627964    1.3717741    0.0813878    0.1204271    0.4666596    1.8546417  #>       y2~~y2       y3~~y3       y4~~y4       y5~~y5       y6~~y6       y7~~y7  #>    7.5813926    4.9556766    3.2245521    2.3130404    4.9681408    3.5600367  #>       y8~~y8 ind60~~ind60 dem60~~dem60 dem65~~dem65         x1~1         x2~1  #>    3.3076854    0.4485989    3.8753039    0.1644633    5.0543838    4.7921946  #>         x3~1         y1~1         y2~1         y3~1         y4~1         y5~1  #>    3.5576898    5.4646667    4.2564429    6.5631103    4.4525330    5.1362519  #>         y6~1         y7~1         y8~1  #>    2.9780741    6.1962639    4.0433897 mySEM$getParameters() #>           label     value   rawValue location isTransformation #> 1     ind60=~x2 2.1796566  2.1796566  Amatrix            FALSE #> 2     ind60=~x3 1.8182100  1.8182100  Amatrix            FALSE #> 3             a 1.1907820  1.1907820  Amatrix            FALSE #> 4             b 1.1745407  1.1745407  Amatrix            FALSE #> 5             c 1.2509789  1.2509789  Amatrix            FALSE #> 6   dem60~ind60 1.4713302  1.4713302  Amatrix            FALSE #> 7   dem65~ind60 0.6004746  0.6004746  Amatrix            FALSE #> 8   dem65~dem60 0.8650430  0.8650430  Amatrix            FALSE #> 9        y1~~y5 0.5825389  0.5825389  Smatrix            FALSE #> 10       y2~~y4 1.4402477  1.4402477  Smatrix            FALSE #> 11       y2~~y6 2.1829448  2.1829448  Smatrix            FALSE #> 12       y3~~y7 0.7115901  0.7115901  Smatrix            FALSE #> 13       y4~~y8 0.3627964  0.3627964  Smatrix            FALSE #> 14       y6~~y8 1.3717741  1.3717741  Smatrix            FALSE #> 15       x1~~x1 0.0813878 -2.5085299  Smatrix            FALSE #> 16       x2~~x2 0.1204271 -2.1167106  Smatrix            FALSE #> 17       x3~~x3 0.4666596 -0.7621551  Smatrix            FALSE #> 18       y1~~y1 1.8546417  0.6176915  Smatrix            FALSE #> 19       y2~~y2 7.5813926  2.0256969  Smatrix            FALSE #> 20       y3~~y3 4.9556766  1.6005337  Smatrix            FALSE #> 21       y4~~y4 3.2245521  1.1707941  Smatrix            FALSE #> 22       y5~~y5 2.3130404  0.8385629  Smatrix            FALSE #> 23       y6~~y6 4.9681408  1.6030457  Smatrix            FALSE #> 24       y7~~y7 3.5600367  1.2697708  Smatrix            FALSE #> 25       y8~~y8 3.3076854  1.1962487  Smatrix            FALSE #> 26 ind60~~ind60 0.4485989 -0.8016262  Smatrix            FALSE #> 27 dem60~~dem60 3.8753039  1.3546241  Smatrix            FALSE #> 28 dem65~~dem65 0.1644633 -1.8050678  Smatrix            FALSE #> 29         x1~1 5.0543838  5.0543838  Mvector            FALSE #> 30         x2~1 4.7921946  4.7921946  Mvector            FALSE #> 31         x3~1 3.5576898  3.5576898  Mvector            FALSE #> 32         y1~1 5.4646667  5.4646667  Mvector            FALSE #> 33         y2~1 4.2564429  4.2564429  Mvector            FALSE #> 34         y3~1 6.5631103  6.5631103  Mvector            FALSE #> 35         y4~1 4.4525330  4.4525330  Mvector            FALSE #> 36         y5~1 5.1362519  5.1362519  Mvector            FALSE #> 37         y6~1 2.9780741  2.9780741  Mvector            FALSE #> 38         y7~1 6.1962639  6.1962639  Mvector            FALSE #> 39         y8~1 4.0433897  4.0433897  Mvector            FALSE lessSEM:::.getParameters(mySEM, raw = TRUE) #>    ind60=~x2    ind60=~x3            a            b            c  dem60~ind60  #>    2.1796566    1.8182100    1.1907820    1.1745407    1.2509789    1.4713302  #>  dem65~ind60  dem65~dem60       y1~~y5       y2~~y4       y2~~y6       y3~~y7  #>    0.6004746    0.8650430    0.5825389    1.4402477    2.1829448    0.7115901  #>       y4~~y8       y6~~y8       x1~~x1       x2~~x2       x3~~x3       y1~~y1  #>    0.3627964    1.3717741   -2.5085299   -2.1167106   -0.7621551    0.6176915  #>       y2~~y2       y3~~y3       y4~~y4       y5~~y5       y6~~y6       y7~~y7  #>    2.0256969    1.6005337    1.1707941    0.8385629    1.6030457    1.2697708  #>       y8~~y8 ind60~~ind60 dem60~~dem60 dem65~~dem65         x1~1         x2~1  #>    1.1962487   -0.8016262    1.3546241   -1.8050678    5.0543838    4.7921946  #>         x3~1         y1~1         y2~1         y3~1         y4~1         y5~1  #>    3.5576898    5.4646667    4.2564429    6.5631103    4.4525330    5.1362519  #>         y6~1         y7~1         y8~1  #>    2.9780741    6.1962639    4.0433897"},{"path":"/articles/The-Structural-Equation-Model.html","id":"changing-the-parameters","dir":"Articles","previous_headings":"Working with the Rcpp_SEMCpp class","what":"Changing the Parameters","title":"The-Structural-Equation-Model","text":"able change parameters essential fitting model. lessSEM, facilitated lessSEM:::.setParameters function: Note specify parameters myParameters given raw format. , already used transformed parameters, set raw = FALSE. Using raw parameters instead look follows: Let’s check parameters: Note now value 1.","code":"# first, let's change one of the parameters: myParameters[\"a\"] <- 1  # now, let's change the parameters of the model mySEM <- lessSEM:::.setParameters(SEM = mySEM, # the model                                   labels = names(myParameters), # names of the parameters                                   values = myParameters, # values of the parameters                                    raw = FALSE) myParameters <- lessSEM:::.getParameters(mySEM, raw = TRUE) # first, let's change one of the parameters: myParameters[\"a\"] <- 1  # now, let's change the parameters of the model mySEM <- lessSEM:::.setParameters(SEM = mySEM, # the model                                   labels = names(myParameters), # names of the parameters                                   values = myParameters, # values of the parameters                                   raw = TRUE) lessSEM:::.getParameters(mySEM) #>    ind60=~x2    ind60=~x3            a            b            c  dem60~ind60  #>    2.1796566    1.8182100    1.0000000    1.1745407    1.2509789    1.4713302  #>  dem65~ind60  dem65~dem60       y1~~y5       y2~~y4       y2~~y6       y3~~y7  #>    0.6004746    0.8650430    0.5825389    1.4402477    2.1829448    0.7115901  #>       y4~~y8       y6~~y8       x1~~x1       x2~~x2       x3~~x3       y1~~y1  #>    0.3627964    1.3717741    0.0813878    0.1204271    0.4666596    1.8546417  #>       y2~~y2       y3~~y3       y4~~y4       y5~~y5       y6~~y6       y7~~y7  #>    7.5813926    4.9556766    3.2245521    2.3130404    4.9681408    3.5600367  #>       y8~~y8 ind60~~ind60 dem60~~dem60 dem65~~dem65         x1~1         x2~1  #>    3.3076854    0.4485989    3.8753039    0.1644633    5.0543838    4.7921946  #>         x3~1         y1~1         y2~1         y3~1         y4~1         y5~1  #>    3.5576898    5.4646667    4.2564429    6.5631103    4.4525330    5.1362519  #>         y6~1         y7~1         y8~1  #>    2.9780741    6.1962639    4.0433897"},{"path":"/articles/The-Structural-Equation-Model.html","id":"fitting-the-model","dir":"Articles","previous_headings":"","what":"Fitting the model","title":"The-Structural-Equation-Model","text":"compute -2-log-likelihood model, use lessSEM:::.fit function: -2-log-likelihood can accessed :","code":"mySEM <- lessSEM:::.fit(SEM = mySEM) mySEM$m2LL #> [1] 3100.741"},{"path":"/articles/The-Structural-Equation-Model.html","id":"computing-the-gradients","dir":"Articles","previous_headings":"","what":"Computing the gradients","title":"The-Structural-Equation-Model","text":"compute gradients, use lessSEM:::.getGradients function. Gradients can computed transformed parameters raw parameters","code":"lessSEM:::.getGradients(mySEM, raw = FALSE) #>     ind60=~x2     ind60=~x3             a             b             c  #>   0.361097622   0.105564095 -32.837359814   2.453232158  17.076222049  #>   dem60~ind60   dem65~ind60   dem65~dem60        y1~~y5        y2~~y4  #>  -0.450648533  -1.078272542  -5.357920036   0.004650747  -0.157923486  #>        y2~~y6        y3~~y7        y4~~y8        y6~~y8        x1~~x1  #>  -0.567076177   0.161163293   0.266869495  -0.271533827   0.230054158  #>        x2~~x2        x3~~x3        y1~~y1        y2~~y2        y3~~y3  #>   0.306685898  -0.093753843  -0.015516535  -0.343353554   0.099359383  #>        y4~~y4        y5~~y5        y6~~y6        y7~~y7        y8~~y8  #>   0.137753880   0.131454473  -0.330083693   0.073331567   0.148964628  #>  ind60~~ind60  dem60~~dem60  dem65~~dem65          x1~1          x2~1  #>  -0.103960291  -0.252921392  -1.955349241   0.000000000   0.000000000  #>          x3~1          y1~1          y2~1          y3~1          y4~1  #>   0.000000000   0.000000000   0.000000000   0.000000000   0.000000000  #>          y5~1          y6~1          y7~1          y8~1  #>   0.000000000   0.000000000   0.000000000   0.000000000 lessSEM:::.getGradients(mySEM, raw = TRUE) #>     ind60=~x2     ind60=~x3             a             b             c  #>   0.361097622   0.105564095 -32.837359814   2.453232158  17.076222049  #>   dem60~ind60   dem65~ind60   dem65~dem60        y1~~y5        y2~~y4  #>  -0.450648533  -1.078272542  -5.357920036   0.004650747  -0.157923486  #>        y2~~y6        y3~~y7        y4~~y8        y6~~y8        x1~~x1  #>  -0.567076177   0.161163293   0.266869495  -0.271533827   0.018723602  #>        x2~~x2        x3~~x3        y1~~y1        y2~~y2        y3~~y3  #>   0.036933297  -0.043751134  -0.028777612  -2.603098086   0.492392971  #>        y4~~y4        y5~~y5        y6~~y6        y7~~y7        y8~~y8  #>   0.444194569   0.304059508  -1.639902248   0.261063066   0.492728130  #>  ind60~~ind60  dem60~~dem60  dem65~~dem65          x1~1          x2~1  #>  -0.046636468  -0.980147244  -0.321583201   0.000000000   0.000000000  #>          x3~1          y1~1          y2~1          y3~1          y4~1  #>   0.000000000   0.000000000   0.000000000   0.000000000   0.000000000  #>          y5~1          y6~1          y7~1          y8~1  #>   0.000000000   0.000000000   0.000000000   0.000000000"},{"path":"/articles/The-Structural-Equation-Model.html","id":"computing-the-hessian","dir":"Articles","previous_headings":"","what":"Computing the Hessian","title":"The-Structural-Equation-Model","text":"compute Hessian, use lessSEM:::.getHessian function. Hessian can computed transformed parameters raw parameters","code":"lessSEM:::.getHessian(mySEM, raw = FALSE) lessSEM:::.getHessian(mySEM, raw = TRUE)"},{"path":"/articles/The-Structural-Equation-Model.html","id":"computing-the-scores","dir":"Articles","previous_headings":"","what":"Computing the Scores","title":"The-Structural-Equation-Model","text":"compute scores (derivative -2-log-likelihood person), use lessSEM:::.getScores function. scores can computed transformed parameters raw parameters","code":"lessSEM:::.getScores(mySEM, raw = FALSE) lessSEM:::.getScores(mySEM, raw = TRUE)"},{"path":"/articles/The-Structural-Equation-Model.html","id":"using-lesssem-with-general-purpose-optimizers","dir":"Articles","previous_headings":"","what":"Using lessSEM with general purpose optimizers","title":"The-Structural-Equation-Model","text":"important part whole SEM implementation mentioned can use flexibly different optimizers. instance, may want try BFGS optimizer optim. Important: highly recommend use raw parameters optimization. Using non-raw parameters can cause errors unnecessary headaches! Let’s look optim function: Note function requires par argument - parameter estimates - fn argument - fitting function - also allows gradients passed function using gr argument. build functions based lessSEM:::.fit lessSEM:::.getGradients functions shown , however convenience wrappers already implemented lessSEM. fitting function called lessSEM:::.fitFunction gradient function called lessSEM:::.gradientFunction. expect vector parameters, SEM, argument specifying parameters raw format. can use optim follows: Note parameter now back maximum likelihood estimate . However, parameters still raw format. get transformed parameters, let’s take one step: Compare parameter estimates lavaan: Finally, can compute standard errors: Let’s compare lavaan :","code":"args(optim) #> function (par, fn, gr = NULL, ..., method = c(\"Nelder-Mead\",  #>     \"BFGS\", \"CG\", \"L-BFGS-B\", \"SANN\", \"Brent\"), lower = -Inf,  #>     upper = Inf, control = list(), hessian = FALSE)  #> NULL # let's get the starting values: par <- lessSEM:::.getParameters(mySEM, raw = TRUE) # important: Use raw = TRUE!  print(par) #>    ind60=~x2    ind60=~x3            a            b            c  dem60~ind60  #>    2.1796566    1.8182100    1.0000000    1.1745407    1.2509789    1.4713302  #>  dem65~ind60  dem65~dem60       y1~~y5       y2~~y4       y2~~y6       y3~~y7  #>    0.6004746    0.8650430    0.5825389    1.4402477    2.1829448    0.7115901  #>       y4~~y8       y6~~y8       x1~~x1       x2~~x2       x3~~x3       y1~~y1  #>    0.3627964    1.3717741   -2.5085299   -2.1167106   -0.7621551    0.6176915  #>       y2~~y2       y3~~y3       y4~~y4       y5~~y5       y6~~y6       y7~~y7  #>    2.0256969    1.6005337    1.1707941    0.8385629    1.6030457    1.2697708  #>       y8~~y8 ind60~~ind60 dem60~~dem60 dem65~~dem65         x1~1         x2~1  #>    1.1962487   -0.8016262    1.3546241   -1.8050678    5.0543838    4.7921946  #>         x3~1         y1~1         y2~1         y3~1         y4~1         y5~1  #>    3.5576898    5.4646667    4.2564429    6.5631103    4.4525330    5.1362519  #>         y6~1         y7~1         y8~1  #>    2.9780741    6.1962639    4.0433897  opt <- optim(par = par,               fn = lessSEM:::.fitFunction, # use the fitting function wrapper              gr = lessSEM:::.gradientFunction, # use the gradient function wrapper              SEM = mySEM, # use the SEM we created above              raw = TRUE, # make sure to tell the functions that we are using raw parameters              method = \"BFGS\" # use the BFGS optimizer ) print(opt$par) #>    ind60=~x2    ind60=~x3            a            b            c  dem60~ind60  #>    2.1791276    1.8180458    1.1909397    1.1740909    1.2511328    1.4725867  #>  dem65~ind60  dem65~dem60       y1~~y5       y2~~y4       y2~~y6       y3~~y7  #>    0.6007137    0.8649836    0.5817910    1.4336940    2.1828828    0.7229781  #>       y4~~y8       y6~~y8       x1~~x1       x2~~x2       x3~~x3       y1~~y1  #>    0.3605874    1.3774602   -2.5093044   -2.1126718   -0.7633056    0.6178333  #>       y2~~y2       y3~~y3       y4~~y4       y5~~y5       y6~~y6       y7~~y7  #>    2.0246995    1.6022529    1.1690474    0.8385775    1.6039528    1.2711405  #>       y8~~y8 ind60~~ind60 dem60~~dem60 dem65~~dem65         x1~1         x2~1  #>    1.1971146   -0.8013687    1.3542390   -1.8057174    5.0543838    4.7921946  #>         x3~1         y1~1         y2~1         y3~1         y4~1         y5~1  #>    3.5576898    5.4646667    4.2564429    6.5631103    4.4525330    5.1362519  #>         y6~1         y7~1         y8~1  #>    2.9780741    6.1962639    4.0433897 mySEM <- lessSEM:::.setParameters(SEM = mySEM, # the model                                   labels = names(opt$par), # names of the parameters                                   values = opt$par, # values of the parameters                                   raw = TRUE) print(lessSEM:::.getParameters(mySEM, raw = FALSE)) #>    ind60=~x2    ind60=~x3            a            b            c  dem60~ind60  #>   2.17912764   1.81804575   1.19093968   1.17409087   1.25113276   1.47258673  #>  dem65~ind60  dem65~dem60       y1~~y5       y2~~y4       y2~~y6       y3~~y7  #>   0.60071368   0.86498364   0.58179103   1.43369401   2.18288278   0.72297808  #>       y4~~y8       y6~~y8       x1~~x1       x2~~x2       x3~~x3       y1~~y1  #>   0.36058737   1.37746019   0.08132479   0.12091448   0.46612308   1.85490471  #>       y2~~y2       y3~~y3       y4~~y4       y5~~y5       y6~~y6       y7~~y7  #>   7.57383439   4.96420359   3.21892497   2.31307420   4.97264951   3.56491599  #>       y8~~y8 ind60~~ind60 dem60~~dem60 dem65~~dem65         x1~1         x2~1  #>   3.31055101   0.44871438   3.87381195   0.16435650   5.05438384   4.79219463  #>         x3~1         y1~1         y2~1         y3~1         y4~1         y5~1  #>   3.55768979   5.46466667   4.25644288   6.56311025   4.45253304   5.13625192  #>         y6~1         y7~1         y8~1  #>   2.97807408   6.19626389   4.04338968 coef(lavaanModel) #>    ind60=~x2    ind60=~x3            a            b            c            a  #>        2.180        1.818        1.191        1.175        1.251        1.191  #>            b            c  dem60~ind60  dem65~ind60  dem65~dem60       y1~~y5  #>        1.175        1.251        1.471        0.600        0.865        0.583  #>       y2~~y4       y2~~y6       y3~~y7       y4~~y8       y6~~y8       x1~~x1  #>        1.440        2.183        0.712        0.363        1.372        0.081  #>       x2~~x2       x3~~x3       y1~~y1       y2~~y2       y3~~y3       y4~~y4  #>        0.120        0.467        1.855        7.581        4.956        3.225  #>       y5~~y5       y6~~y6       y7~~y7       y8~~y8 ind60~~ind60 dem60~~dem60  #>        2.313        4.968        3.560        3.308        0.449        3.875  #> dem65~~dem65  #>        0.164 lessSEM:::.standardErrors(SEM = mySEM, raw = FALSE) #>    ind60=~x2    ind60=~x3            a            b            c  dem60~ind60  #>   0.13885220   0.15204331   0.14166120   0.11987057   0.12295637   0.39139696  #>  dem65~ind60  dem65~dem60       y1~~y5       y2~~y4       y2~~y6       y3~~y7  #>   0.23828913   0.07567860   0.36462028   0.68977248   0.73096921   0.62119518  #>       y4~~y8       y6~~y8       x1~~x1       x2~~x2       x3~~x3       y1~~y1  #>   0.46062833   0.57969390   0.01968652   0.06991196   0.08897395   0.45717112  #>       y2~~y2       y3~~y3       y4~~y4       y5~~y5       y6~~y6       y7~~y7  #>   1.34332169   0.96373267   0.74092220   0.48364102   0.89600780   0.73922565  #>       y8~~y8 ind60~~ind60 dem60~~dem60 dem65~~dem65         x1~1         x2~1  #>   0.71425329   0.08675480   0.88802934   0.23331748   0.08406657   0.17326967  #>         x3~1         y1~1         y2~1         y3~1         y4~1         y5~1  #>   0.16121433   0.29892606   0.43891242   0.39404806   0.37957637   0.30446534  #>         y6~1         y7~1         y8~1  #>   0.39247640   0.36442149   0.37545879 parameterEstimates(lavaanModel)[,1:6] #>      lhs op   rhs label   est    se #> 1  ind60 =~    x1       1.000 0.000 #> 2  ind60 =~    x2       2.180 0.138 #> 3  ind60 =~    x3       1.818 0.152 #> 4  dem60 =~    y1       1.000 0.000 #> 5  dem60 =~    y2     a 1.191 0.139 #> 6  dem60 =~    y3     b 1.175 0.120 #> 7  dem60 =~    y4     c 1.251 0.117 #> 8  dem65 =~    y5       1.000 0.000 #> 9  dem65 =~    y6     a 1.191 0.139 #> 10 dem65 =~    y7     b 1.175 0.120 #> 11 dem65 =~    y8     c 1.251 0.117 #> 12 dem60  ~ ind60       1.471 0.392 #> 13 dem65  ~ ind60       0.600 0.226 #> 14 dem65  ~ dem60       0.865 0.075 #> 15    y1 ~~    y5       0.583 0.356 #> 16    y2 ~~    y4       1.440 0.689 #> 17    y2 ~~    y6       2.183 0.737 #> 18    y3 ~~    y7       0.712 0.611 #> 19    y4 ~~    y8       0.363 0.444 #> 20    y6 ~~    y8       1.372 0.577 #> 21    x1 ~~    x1       0.081 0.019 #> 22    x2 ~~    x2       0.120 0.070 #> 23    x3 ~~    x3       0.467 0.090 #> 24    y1 ~~    y1       1.855 0.433 #> 25    y2 ~~    y2       7.581 1.366 #> 26    y3 ~~    y3       4.956 0.956 #> 27    y4 ~~    y4       3.225 0.723 #> 28    y5 ~~    y5       2.313 0.479 #> 29    y6 ~~    y6       4.968 0.921 #> 30    y7 ~~    y7       3.560 0.710 #> 31    y8 ~~    y8       3.308 0.704 #> 32 ind60 ~~ ind60       0.449 0.087 #> 33 dem60 ~~ dem60       3.875 0.866 #> 34 dem65 ~~ dem65       0.164 0.227"},{"path":"/articles/The-Structural-Equation-Model.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"The-Structural-Equation-Model","text":"Fox, J. (2006). Teacher’s corner: Structural equation modeling sem package R. Structural Equation Modeling: Multidisciplinary Journal, 13(3), 465–486. https://doi.org/10.1207/s15328007sem1303_7 McArdle, J. J., & McDonald, R. P. (1984). algebraic properties Reticular Action Model moment structures. British Journal Mathematical Statistical Psychology, 37(2), 234–251. https://doi.org/10.1111/j.2044-8317.1984.tb00802.x","code":""},{"path":"/articles/The-optimizer-interface.html","id":"the-fitting-function","dir":"Articles","previous_headings":"","what":"The Fitting Function","title":"The-optimizer-interface","text":"objective optimizers minimize fitting function. , glmnet ista assume fitting function given differentiable part non-differentiable part. specific, fitting function given : \\[f(\\pmb{\\theta}) = \\underbrace{l(\\pmb\\theta) + s(\\pmb\\theta,\\pmb{t}_s)}_{\\text{differentiable}} + \\underbrace{p(\\pmb\\theta,\\pmb{t}_p)}_{\\text{non-differentiable}}\\] \\(l(\\pmb\\theta)\\) unregularized fitting function (e.g., -2log-likelihood) SEMs implemented lessSEM. \\(s(\\pmb\\theta,\\pmb{t}_s)\\) differentiable (smooth) penalty function (e.g., ridge penalty) \\(p(\\pmb\\theta,\\pmb{t}_p)\\) non-differentiable penalty function (e.g., lasso penalty). three functions take parameter estimates \\(\\pmb\\theta\\) input return single value output. penalty functions \\(s(\\pmb\\theta,\\pmb{t}_s)\\) \\(p(\\pmb\\theta,\\pmb{t}_p)\\) additionally expect vectors tuning parameters–\\(\\pmb{t}_s\\) case smooth penalty \\(\\pmb{t}_p\\) case non-differentiable penalty. Thus, theory penalty functions can use different tuning parameters. prototypical example fitting functions form defined elastic net. , \\[f(\\pmb{\\theta}) = \\underbrace{l(\\pmb\\theta) + (1-\\alpha)\\lambda \\sum_j\\theta_j^2}_{\\text{differentiable}} + \\underbrace{\\alpha\\lambda\\sum_j| \\theta_j|}_{\\text{non-differentiable}}\\] elastic net combination ridge penalty lasso penalty. Note case, penalties take tuning parameters (\\(\\lambda\\) \\(\\alpha\\)). mind, can now closer look optimization functions. start glmnet (see function glmnet file inst/include/glmnet_class.hpp). function called follows: first argument model. model created must inherit lessSEM::model class (see lessLM example). importantly, model must provide means get value first part fitting function: \\(l(\\pmb\\theta)\\). must also provide means compute gradients fitting function. next argument starting values given Rcpp::NumericVector. type chosen can labels current implementation lessSEM expects give startingValues names. Now come actual penalty functions. first one non-differentiable penalty: lasso \\(p(\\pmb\\theta)\\). must object class penaltyLASSOGlmnet can created lessSEM::penaltyLASSOGlmnet(). Next comes differentiable ridge penalty must class penaltyRidgeGlmnet can created lessSEM::penaltyRidgeGlmnet. Now, tuning parameters deviate bit discussed . said differentiable non-differentiable penalty functions take vector tuning parameters (\\(\\pmb t_s\\) \\(\\pmb t_p\\) respectively). Note, however, elastic net uses tuning parameters , ridge lasso penalty. glmnet optimizer therefore decided combine one: tuningParametersEnetGlmnet struct. tuningParametersEnetGlmnet three slots: alpha (set \\(\\alpha\\)), lambda (set \\(\\lambda\\)) slot called weights. Now, weights allow us switch penalty selected parameters. instance, linear regression want penalize intercept. end, fitting function actually implemented internally given \\[f(\\pmb{\\theta}) = \\underbrace{l(\\pmb\\theta) + (1-\\alpha)\\lambda \\sum_j\\omega_j \\theta_j^2}_{\\text{differentiable}} + \\underbrace{\\alpha\\lambda\\sum_j\\omega_j| \\theta_j|}_{\\text{non-differentiable}}\\] set \\(\\omega_j = 0\\) specific parameter, parameter unregularized. Setting \\(\\omega_j = 1\\) means parameter \\(j\\) penalized. \\(\\omega_j\\) can also take value (e.g., \\(\\omega_j = .4123\\)) allows penalties adaptive lasso. Importantly, weights vector must length startingValuesRcpp. , parameter must weight associated weights vector. Finally, controlGLMNET argument. argument lets us fine tune optimizer. use control argument, create new control object C++ follows: Now, can tweak element myControlObject; e.g., Currently, following control elements can changed: initialHessian: matrix initial Hessian approximation stepSize: initial step size used line search sigma: Controls sigma parameter Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421. gamma: Controls gamma parameter Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421. maxIterOut: maximal number outer iterations maxIterIn: maximal number inner iterations maxIterLine: maximal number iterations given line search procedure breakOuter: small value; outer convergence criterion falls threshold, assumed model converged breakInner: small value; inner convergence criterion falls threshold, assumed inner optimization verbose: set value > 0, fit every verbose iterations printed. take closer look two penalty functions handled within glmnet, realize basically absorb differentiable penalty normal fitting function. , non-differentiable part gets special treatment, differentiable part simply added differntiable \\(l(\\pmb\\theta)\\). give example: Note gradients \\(l(\\pmb\\theta)\\) \\(s(\\pmb\\theta,\\pmb{t}_s)\\) combined one.","code":"inline lessSEM::fitResults glmnet(model& model_,                                    Rcpp::NumericVector startingValuesRcpp,                                   penaltyLASSOGlmnet& penalty_,                                   penaltyRidgeGlmnet& smoothPenalty_,                                    const tuningParametersEnetGlmnet tuningParameters,                                   const controlGLMNET& control_ = controlGlmnetDefault()) {...} lessSEM::controlGLMNET myControlObject = lessSEM::controlGlmnetDefault(); myControlObject.maxIterOut = 100 // only 100 outer iterations gradients_kMinus1 = model_.gradients(parameters_kMinus1, parameterLabels) +       smoothPenalty_.getGradients(parameters_kMinus1, parameterLabels, tuningParameters); // ridge part"},{"path":"/articles/The-optimizer-interface.html","id":"the-ista-variants","dir":"Articles","previous_headings":"","what":"The ista variants","title":"The-optimizer-interface","text":"Besides glmnet optimizer, also implemented variants ista. based publications mentioned . fitting function given \\[f(\\pmb{\\theta}) = \\underbrace{l(\\pmb\\theta) + s(\\pmb\\theta,\\pmb{t}_s)}_{\\text{differentiable}} + \\underbrace{p(\\pmb\\theta,\\pmb{t}_p)}_{\\text{non-differentiable}}\\] However, ista lot flexible glmnet moment allows penalties subsumed elastic net umbrella. following, build lot ’ve already discussed regarding glmnet optimizer . First, let’s look ista function located inst/include/ista_class.hpp. function called : function complicated glmnet function discussed . , let’s start part stays : First, still pass model function. model must fit gradients function return fit gradient respectively. Next, function expects us provide starting values Rcpp::NumericVector. skip proximalOperator penalty moment (relate \\(p(\\pmb\\theta,\\pmb t_p)\\)) concentrate smoothPenalty first. function \\(s(\\pmb\\theta,\\pmb t_s)\\). previous example, looked elastic net penalty, smooth penalty ridge penalty function. Now, ista optimizer, can also pass ridge penalty smooth penalty. fact, exactly use ista fit elastic net (see class istaEnet src/elasticNet.cpp). smooth penalty tuning parameters \\(\\pmb t_s\\) called smoothTuningParameters function call. case elastic net, \\(\\alpha\\) \\(\\lambda\\) (weights vector). Similar glmnet procedure outlined , differentiable penalty \\(s(\\pmb\\theta,\\pmb t_s)\\) simply absorbed unregularized fitting function \\(l(\\pmb\\theta)\\). Now, non-differentiable part \\(p(\\pmb\\theta,\\pmb p_s)\\), ista optimizer uses -called proximal operators. details beyond scope , Parikh et al. (2013) provide good overview algorithms. make things work ista, must pass proximal operator optimizer. Within lessSEM, prepared proximal operators well-known penalty functions. instance, find proximal operator mcp penalty inst/include/mcp.hpp. Additionally, need function returns acutal penalty value. penalty object function call. Finally, penalty \\(p(\\pmb\\theta,\\pmb t_p)\\) gets tuning parameters \\(\\pmb t_p\\). tuningParameters object . make things concrete, let’s look elastic net . case, penalty class lessSEM::penaltyLASSO proximal operator type lessSEM::proximalOperatorLasso. tuning parameters \\(\\alpha\\) \\(\\lambda\\) (weights vector). example provided class istaEnet src/elasticNet.cpp. Note many penalty function implemented lessSEM typically combined smooth penalty (e.g., scad, mcp, …). case, must still pass smoothPenalty object ista. end, created lessSEM::noSmoothPenalty class can used instead smooth penalty like ridge. See class istaMcp src/mcp.cpp example. Finally, control argument. argument lets us fine tune optimizer. use control argument, create new control object C++ follows: Now, can tweak element myControlObject; e.g., Currently, following control elements can changed: L0: controls step size used first iteration eta: controls much step size changes inner iterations \\((eta^)*L\\), \\(\\) inner iteration accelerate: true, extrapolation parameter used accelerate ista (see, e.g., Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231., p. 152) maxIterOut: maximal number outer iterations maxIterIn: maximal number inner iterations breakOuter: small value; outer convergence criterion falls threshold, assumed model converged convCritInner: related inner breaking condition. set ista, one presented Beck & Teboulle (2009) used; see Remark 3.1 p. 191 (ISTA backtracking). set gist, one presented Gong et al. (2013) (Equation 3) used. sigma: (0,1); used gist convergence criterion. larger sigma# enforce larger improvement fit stepSizeIn: step sizes carried forward iteration iteration? set initial, step size reset L0 iteration, set istaStepInheritance, previous step size used initial value next iteration. set barzilaiBorwein, use Barzilai-Borwein procedure. Finally, set stochasticBarzilaiBorwein, also use Barzilai-Borwein procedure, sometimes resets step size; can – experience – help optimizer caught bad spot. sampleSize: can used scale fitting function verbose: set value > 0, fit every verbose iterations printed.","code":"template<typename T, typename U> // T is the type of the tuning parameters inline lessSEM::fitResults ista(     model& model_,      Rcpp::NumericVector startingValuesRcpp,     proximalOperator<T>& proximalOperator_, // proximalOperator takes the tuning parameters     // as input -> <T>     penalty<T>& penalty_, // penalty takes the tuning parameters     smoothPenalty<U>& smoothPenalty_, // smoothPenalty takes the smooth tuning parameters     // as input -> <U>     const T& tuningParameters, // tuning parameters are of type T     const U& smoothTuningParameters, // tuning parameters are of type U     const control& control_ = controlDefault() ) {...} lessSEM::control myControlObject = lessSEM::controlDefault(); myControlObject.L0 = .9"},{"path":"/articles/lessSEM.html","id":"regularized-structural-equation-modeling","dir":"Articles","previous_headings":"","what":"Regularized Structural Equation Modeling","title":"lessSEM","text":"Regularized structural equation modeling (REGSEM) proposed Jacobucci et al. (2016) Huang et al. (2017). objective reduce overfitting small samples allow flexibility. general idea regularize parameters towards zero. end, penalty function \\(p(\\pmb\\theta)\\) added vanilla objective function. lessSEM, objective function given full information maximum likelihood function \\(F_{\\text{ML}}(\\pmb\\theta)\\). new objective function defined : \\[F_{\\text{REGSEM},\\lambda}(\\pmb\\theta) = F_{\\text{ML}}(\\pmb\\theta)+ \\lambda N p(\\pmb\\theta)\\] \\(F_{\\text{ML}}(\\pmb\\theta)\\) wants parameters close ordinary maximum likelihood estimates \\(p(\\pmb\\theta)\\) wants regularized parameters close zero \\(\\lambda\\) allows us fine tune two forces mentioned gets influence final parameter estimates \\(N\\) sample size. Scaling \\(N\\) done stay consistent results returned regsem lslx. many different penalty functions used. lessSEM, implemented following functions: \\[ \\begin{array}{l|ll} \\text{penalty} & \\text{function} & \\text{reference}\\\\ \\hline \\text{ridge} & p( x_j) = \\lambda x_j^2 & \\text{(Hoerl & Kennard, 1970)}\\\\ \\text{lasso} & p( x_j) = \\lambda| x_j| & \\text{(Tibshirani, 1996)}\\\\ \\text{adaptiveLasso} & p( x_j) = \\frac{1}{w_j}\\lambda| x_j| & \\text{(Zou, 2006)}\\\\ \\text{elasticNet} & p( x_j) = \\alpha\\lambda|x_j| + (1-\\alpha)\\lambda x_j^2 & \\text{(Zou & Hastie, 2005)}\\\\ \\text{cappedL1} & p( x_j) = \\lambda \\min(| x_j|, \\theta); \\theta > 0 & \\text{(Zhang, 2010)}\\\\ \\text{lsp} & p( x_j) = \\lambda \\log(1 + |x_j|\\theta); \\theta > 0 & \\text{(Candès et al., 2008)} \\\\ \\text{scad} & p( x_j) = \\begin{cases} \\lambda |x_j| & \\text{} |x_j| \\leq \\lambda\\\\ \\frac{-x_j^2 + 2\\theta\\lambda |x_j| - \\lambda^2}{2(\\theta -1)} & \\text{} \\lambda < |x_j| \\leq \\lambda\\theta \\\\ (\\theta + 1) \\lambda^2/2 & \\text{} |x_j| \\geq \\theta\\lambda\\\\ \\end{cases}; \\theta > 2 & \\text{(Fan & Li, 2001)} \\\\ \\text{mcp} & p( x_j) = \\begin{cases} \\lambda |x_j| - x_j^2/(2\\theta) & \\text{} |x_j| \\leq \\theta\\lambda\\\\ \\theta\\lambda^2/2 & \\text{} |x_j| > \\lambda\\theta \\end{cases}; \\theta > 0 & \\text{(Zhang, 2010)} \\end{array} \\]","code":""},{"path":"/articles/lessSEM.html","id":"objectives","dir":"Articles","previous_headings":"","what":"Objectives","title":"lessSEM","text":"objectives lessSEM provide … flexible framework regularizing SEM optimizers SEM packages can used interface similar optim.","code":""},{"path":"/articles/lessSEM.html","id":"regularizing-sem","dir":"Articles","previous_headings":"","what":"Regularizing SEM","title":"lessSEM","text":"lessSEM heavily inspired regsem package. also builds lavaan set model.","code":""},{"path":"/articles/lessSEM.html","id":"setting-up-a-model","dir":"Articles","previous_headings":"Regularizing SEM","what":"Setting up a model","title":"lessSEM","text":"First, start lavaan: Next, decide parameters regularized. Let’s go l5-l7. lessSEM, always use parameter labels specify parameters regularized! Finally, set regularized model. end, must first decide penalty function want use. want shrink parameters without setting zero, can use ridge regularization. Otherwise, must use penalty functions mentioned . lessSEM, dedicated function penalties. names functions identical “penalty” column table . instance, let’s look lasso penalty: Plot paths see going :  Note parameters pulled towards zero \\(\\lambda\\) increases. Note also specify specific values \\(\\lambda\\) lasso function . Instead, specified many \\(\\lambda\\)s want (nLambdas=50). use lasso adaptive lasso, lessSEM can automatically compute \\(\\lambda\\) necessary set parameters zero. currently supported penalties. plots returned lessSEM either ggplot2 elements (case single tuning parameter), created plotly (case 2 tuning parameters). can change plot post-hoc:  coef function gives access parameter estimates: Now, let’s assume also want try scad penalty. case, replace lasso() function scad() function: scad penalty two tuning parmeters \\(\\lambda\\) \\(\\theta\\). naming follows used Gong et al. (2013). can plot results , however requires plotly package currently supported Rmarkdown. parameter estimates can accessed coef() function:","code":"library(lavaan) #> This is lavaan 0.6-14 #> lavaan is FREE software! Please report any bugs. library(lessSEM) set.seed(4321) # let's simulate data for a simple  # cfa with 7 observed variables data <- lessSEM::simulateExampleData(N = 50,                                       loadings = c(rep(1,4),                                                   rep(0,3)) ) head(data) #>              y1         y2         y3         y4          y5         y6 #> [1,] -0.1737175 -0.1970204  1.1888412  1.8520403  0.16257957  1.8825526 #> [2,] -1.5179940  0.9029781 -0.1726986 -0.3596920 -0.02092956 -0.5798953 #> [3,]  0.6136418  0.2578986 -0.1359237  0.7703602  0.23502463  0.2001872 #> [4,] -0.5920933  0.2157830  1.6784758  1.8568433 -0.60458482  0.2219578 #> [5,]  0.0763996 -1.1442382 -2.8122156  0.4899892  0.03453494  2.0457604 #> [6,]  2.2504896  2.9742206  0.4353705  1.2338364  0.04693253 -0.6438847 #>              y7 #> [1,]  1.1383999 #> [2,]  0.9020861 #> [3,]  0.7986506 #> [4,]  0.4736751 #> [5,] -2.6721417 #> [6,] -1.1386235  # we assume a single factor structure lavaanSyntax <- \"       f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 + l6*y6 + l7*y7        f ~~ 1*f       \" # estimate the model with lavaan lavaanModel <- cfa(lavaanSyntax,                     data = data) regularized <- c(\"l5\", \"l6\", \"l7\") # tip: we can use paste to make this easier: regularized <- paste0(\"l\", 5:7) fitLasso <- lasso(lavaanModel = lavaanModel,                    regularized = regularized,                   nLambdas = 5) plot(fitLasso) plot(fitLasso) + ggplot2::theme_bw() coef(fitLasso) #>                                                                                #>   Tuning         ||--||  Estimates                                             #>  ------- ------- ||--|| ---------- ---------- ---------- ---------- ---------- #>   lambda   alpha ||--||         l2         l3         l4         l5         l6 #>  ======= ======= ||--|| ========== ========== ========== ========== ========== #>   0.1034  1.0000 ||--||     0.7523     0.7536     0.5742          .          . #>   0.0776  1.0000 ||--||     0.7477     0.7480     0.5720    -0.0104          . #>   0.0517  1.0000 ||--||     0.7399     0.7396     0.5688    -0.0266          . #>   0.0259  1.0000 ||--||     0.7301     0.7332     0.5677    -0.0418          . #>   0.0000  1.0000 ||--||     0.7239     0.7318     0.5688    -0.0562     0.0166 #>                                                                               #>                                                                               #>  ---------- ---------- ---------- ---------- ---------- ---------- ---------- #>          l7     y1~~y1     y2~~y2     y3~~y3     y4~~y4     y5~~y5     y6~~y6 #>  ========== ========== ========== ========== ========== ========== ========== #>           .     0.8812     1.1477     1.9274     1.0804     0.5710     0.9628 #>           .     0.8742     1.1523     1.9331     1.0818     0.5705     0.9628 #>     -0.0090     0.8632     1.1602     1.9416     1.0838     0.5697     0.9628 #>     -0.0479     0.8529     1.1706     1.9481     1.0841     0.5690     0.9628 #>     -0.0894     0.8491     1.1779     1.9496     1.0830     0.5685     0.9626 #>             #>             #>  ---------- #>      y7~~y7 #>  ========== #>      1.5320 #>      1.5320 #>      1.5312 #>      1.5282 #>      1.5255 fitScad <- scad(lavaanModel = lavaanModel,                  regularized = regularized,                 lambdas = seq(0,1,length.out = 4),                 thetas = seq(2.1, 5,length.out = 2)) plot(fitScad) coef(fitScad) #>                                                                                #>   Tuning         ||--||  Estimates                                             #>  ------- ------- ||--|| ---------- ---------- ---------- ---------- ---------- #>   lambda   theta ||--||         l2         l3         l4         l5         l6 #>  ======= ======= ||--|| ========== ========== ========== ========== ========== #>   0.0000  2.1000 ||--||     0.7240     0.7320     0.5689    -0.0562     0.0166 #>   0.3333  2.1000 ||--||     0.7520     0.7533     0.5741          .          . #>   0.6667  2.1000 ||--||     0.7522     0.7535     0.5742          .          . #>   1.0000  2.1000 ||--||     0.7523     0.7536     0.5742          .          . #>   0.0000  5.0000 ||--||     0.7244     0.7326     0.5690    -0.0561     0.0167 #>   0.3333  5.0000 ||--||     0.7520     0.7533     0.5741          .          . #>   0.6667  5.0000 ||--||     0.7522     0.7535     0.5742          .          . #>   1.0000  5.0000 ||--||     0.7523     0.7536     0.5742          .          . #>                                                                               #>                                                                               #>  ---------- ---------- ---------- ---------- ---------- ---------- ---------- #>          l7     y1~~y1     y2~~y2     y3~~y3     y4~~y4     y5~~y5     y6~~y6 #>  ========== ========== ========== ========== ========== ========== ========== #>     -0.0894     0.8492     1.1778     1.9495     1.0829     0.5684     0.9626 #>           .     0.8808     1.1480     1.9277     1.0804     0.5710     0.9628 #>           .     0.8812     1.1478     1.9274     1.0804     0.5710     0.9628 #>           .     0.8812     1.1478     1.9274     1.0804     0.5710     0.9628 #>     -0.0894     0.8499     1.1774     1.9489     1.0828     0.5685     0.9626 #>           .     0.8808     1.1480     1.9277     1.0804     0.5710     0.9628 #>           .     0.8812     1.1478     1.9274     1.0804     0.5710     0.9628 #>           .     0.8812     1.1478     1.9274     1.0804     0.5710     0.9628 #>             #>             #>  ---------- #>      y7~~y7 #>  ========== #>      1.5255 #>      1.5320 #>      1.5320 #>      1.5320 #>      1.5255 #>      1.5320 #>      1.5320 #>      1.5320"},{"path":"/articles/lessSEM.html","id":"selecting-a-model","dir":"Articles","previous_headings":"Regularizing SEM","what":"Selecting a model","title":"lessSEM","text":"select model report final parameter estimates, can use AIC BIC (information criteria also possible, currently implemented). two ways use information criteria. First, can compute select model : easier way use coef() function :","code":"AICs <- AIC(fitLasso) head(AICs) #>       lambda alpha     m2LL  regM2LL nonZeroParameters convergence      AIC #> 1 0.10340364     1 1071.078 1071.078                10        TRUE 1091.078 #> 2 0.07755273     1 1071.033 1071.074                11        TRUE 1093.033 #> 3 0.05170182     1 1070.956 1071.048                12        TRUE 1094.956 #> 4 0.02585091     1 1070.851 1070.967                12        TRUE 1094.851 #> 5 0.00000000     1 1070.810 1070.810                13        TRUE 1096.810  fitLasso@parameters[which.min(AICs$AIC),] #>      lambda alpha        l2        l3       l4 l5 l6 l7    y1~~y1   y2~~y2 #> 1 0.1034036     1 0.7522981 0.7536087 0.574216  0  0  0 0.8812245 1.147738 #>     y3~~y3   y4~~y4    y5~~y5    y6~~y6   y7~~y7 #> 1 1.927356 1.080356 0.5710042 0.9628056 1.531997 coef(fitLasso, criterion = \"AIC\") #>                                                                                #>   Tuning         ||--||  Estimates                                             #>  ------- ------- ||--|| ---------- ---------- ---------- ---------- ---------- #>   lambda   alpha ||--||         l2         l3         l4         l5         l6 #>  ======= ======= ||--|| ========== ========== ========== ========== ========== #>   0.1034  1.0000 ||--||     0.7523     0.7536     0.5742          .          . #>                                                                               #>                                                                               #>  ---------- ---------- ---------- ---------- ---------- ---------- ---------- #>          l7     y1~~y1     y2~~y2     y3~~y3     y4~~y4     y5~~y5     y6~~y6 #>  ========== ========== ========== ========== ========== ========== ========== #>           .     0.8812     1.1477     1.9274     1.0804     0.5710     0.9628 #>             #>             #>  ---------- #>      y7~~y7 #>  ========== #>      1.5320"},{"path":"/articles/lessSEM.html","id":"cross-validation","dir":"Articles","previous_headings":"Regularizing SEM > Selecting a model","what":"Cross-Validation","title":"lessSEM","text":"good alternative information criteria use cross-validation. lessSEM, dedicated cross-validation function penalties discussed . Let’s look lsp() penalty time. Now, non-cross-validated lsp, use use cross-validated version lsp, simply use cv prefix. function called cvLsp(): best model can now accessed ","code":"fitLsp <- lsp(lavaanModel = lavaanModel,                regularized = regularized,               lambdas = seq(0,1,.1),               thetas = seq(.1,2,length.out = 4)) fitCvLsp <- cvLsp(lavaanModel = lavaanModel,                    regularized = regularized,                   lambdas = seq(0,1,.1),                   thetas = seq(.1,2,length.out = 4)) coef(fitCvLsp) #>                                                                                #>   Tuning         ||--||  Estimates                                             #>  ------- ------- ||--|| ---------- ---------- ---------- ---------- ---------- #>   lambda   theta ||--||         l2         l3         l4         l5         l6 #>  ======= ======= ||--|| ========== ========== ========== ========== ========== #>   1.0000  0.1000 ||--||     0.7520     0.7533     0.5741          .          . #>                                                                               #>                                                                               #>  ---------- ---------- ---------- ---------- ---------- ---------- ---------- #>          l7     y1~~y1     y2~~y2     y3~~y3     y4~~y4     y5~~y5     y6~~y6 #>  ========== ========== ========== ========== ========== ========== ========== #>           .     0.8808     1.1480     1.9277     1.0804     0.5710     0.9628 #>             #>             #>  ---------- #>      y7~~y7 #>  ========== #>      1.5320"},{"path":"/articles/lessSEM.html","id":"missing-data","dir":"Articles","previous_headings":"Regularizing SEM","what":"Missing Data","title":"lessSEM","text":"psychological data sets missing data. lessSEM, follow example regsem use full information maximum likelihood function account missingness. Identical regsem, lessSEM expects already use full information maximum likelihood method lavaan. Note added argument missing = 'ml' lavaan model. tells lavaan use full information maximum likelihood function. Next, pass model penalty functions lessSEM. lessSEM automatically switch full information maximum likelihood function well: check lessSEM actually use full information maximum likelihood, can compare 2log-likelihood lavaan lessSEM penalty used (\\(\\lambda = 0\\)): Compare :","code":"# let's simulate data for a simple  # cfa with 7 observed variables # and 10 % missing data data <- lessSEM::simulateExampleData(N = 100,                                       loadings = c(rep(1,4),                                                   rep(0,3)),                                      percentMissing = 10 ) head(data) #>               y1         y2         y3          y4         y5        y6 #> [1,]  0.60367543 -0.3206755 -0.5712115  0.36626658  0.6138552 0.8207451 #> [2,]  0.37497661  2.0100766 -1.5925242 -0.02983920  0.2409065 1.1250778 #> [3,]          NA  0.8134143  1.7803075  3.27710938 -0.3651732        NA #> [4,] -0.04379503  0.1369219 -1.9424719  0.40304282 -0.6435542 1.5412868 #> [5,] -0.32969221         NA -1.6536493 -2.20991516  1.2462449 0.6725163 #> [6,]  0.61738032  0.9116425  0.9196841  0.03340633  0.5553805 0.1209500 #>              y7 #> [1,]  0.6346473 #> [2,]  0.8865902 #> [3,] -0.8283463 #> [4,]  0.0635044 #> [5,]         NA #> [6,]  2.0956358  # we assume a single factor structure lavaanSyntax <- \"       f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 + l6*y6 + l7*y7        f ~~ 1*f       \" # estimate the model with lavaan lavaanModel <- cfa(lavaanSyntax,                     data = data,                    missing = \"ml\") # important: use fiml for missing data fitLasso <- lasso(lavaanModel = lavaanModel,                    regularized = regularized,                   nLambdas = 10) fitLasso <- lasso(lavaanModel = lavaanModel,                    regularized = regularized,                   lambdas = 0) fitLasso@fits$m2LL #> [1] 2034.104 -2*logLik(lavaanModel) #> 'log Lik.' 2034.104 (df=20)"},{"path":"/articles/lessSEM.html","id":"using-multiple-cores","dir":"Articles","previous_headings":"","what":"Using multiple cores","title":"lessSEM","text":"default, lessSEM use one computer core. However, model many parameters, parallel computations can considerably faster. Multi-Core support therefore provided using RcppParallel package (Allaire et. al, 2023). make use multiple cores, number cores must specified control argument (see ). , makes sense check many cores computer : Note using cores can block computer resources left tasks R. use 2 cores, can set nCores = 2 follows: Note multi-core support provided SEM. Using optimizers implemented lessSEM models SEM (e.g., lessLM package) automatically allow multi-core execution.","code":"library(RcppParallel) # Print the number of threads (we call them cores for simplicity, but technically they are threads) RcppParallel::defaultNumThreads() #> [1] 2 fitLasso <- lasso(lavaanModel = lavaanModel,                    regularized = regularized,                   nLambdas = 10,                   control = controlGlmnet(nCores = 2))"},{"path":"/articles/lessSEM.html","id":"changing-the-optimizer","dir":"Articles","previous_headings":"","what":"Changing the optimizer","title":"lessSEM","text":"lessSEM comes two specialized optimization procedures elastic-net-type penalties: ista glmnet. Currently, default glmnet elastic net variants (ridge, lasso, adaptive lasso, elastic net) ista penalties. Ista require computation Hessian matrix. However, comes price: ista optimization tends call fit gradient function lot glment. using elastic-net-type penalty (ridge, lasso, adaptive lasso, elastic net), recommend first test glmnet optimizer switch ista glmnet results errors due Hessian matrix. Switching ista done follows:","code":"fitLasso <- lasso(lavaanModel = lavaanModel,                    regularized = regularized,                   nLambdas = 10,                   method = \"ista\", # change the method                   control = controlIsta() # change the control argument                   )"},{"path":"/articles/lessSEM.html","id":"parameter-transformations","dir":"Articles","previous_headings":"","what":"Parameter transformations","title":"lessSEM","text":"lessSEM allows parameter transformations. explained detail vignette Parameter-transformations (see vignette(\"Parameter-transformations\", package = \"lessSEM\")). provide short example, let’s look political democracy data set: Note model estimated , loadings latent variables constrained equality time. also relax assumption allowing time point specific loadings: Deciding approaches can difficult may parameters equality time holds, others violate assumption. , transformations can used regularize differences parameters. end, define transformations: Next, pass transformations variable penalty function: check measurement invariance can assumed, can select best model using information criteria: details provided vignette(\"Parameter-transformations\", package = \"lessSEM\").","code":"# example from ?lavaan::sem library(lavaan) modelSyntax <- '    # latent variable definitions      ind60 =~ x1 + x2 + x3      dem60 =~ y1 + a*y2 + b*y3 + c*y4      dem65 =~ y5 + a*y6 + b*y7 + c*y8    # regressions     dem60 ~ ind60     dem65 ~ ind60 + dem60    # residual correlations     y1 ~~ y5     y2 ~~ y4      y3 ~~ y7     y4 ~~ y8     y6 ~~ y8 '  lavaanFit <- sem(model = modelSyntax,                  data = PoliticalDemocracy) library(lavaan) modelSyntax <- '    # latent variable definitions      ind60 =~ x1 + x2 + x3      dem60 =~ y1 + a1*y2 + b1*y3 + c1*y4      dem65 =~ y5 + a2*y6 + b2*y7 + c2*y8    # regressions     dem60 ~ ind60     dem65 ~ ind60 + dem60    # residual correlations     y1 ~~ y5     y2 ~~ y4      y3 ~~ y7     y4 ~~ y8     y6 ~~ y8 '  lavaanFit <- sem(model = modelSyntax,                  data = PoliticalDemocracy) transformations <- \" // IMPORTANT: Our transformations always have to start with the follwing line: parameters: a1, a2, b1, b2, c1, c2, delta_a2, delta_b2, delta_c2  // In the line above, we defined the names of the parameters which we // want to use in our transformations. EACH AND EVERY PARAMETER USED IN // THE FOLLOWING MUST BE STATED ABOVE. The line must always start with // the keyword 'parameters' followed by a colon. The parameters must be // separated by commata. // Comments are added with double-backslash  // Now we can state our transformations:  a2 = a1 + delta_a2; // statements must end with semicolon b2 = b1 + delta_b2; c2 = c1 + delta_c2; \" coef(lassoFit, criterion = \"BIC\") #>                                                                                #>   Tuning         ||--||  Estimates                                             #>  ------- ------- ||--|| ---------- ---------- ---------- ---------- ---------- #>   lambda   alpha ||--||  ind60=~x2  ind60=~x3         a1         b1         c1 #>  ======= ======= ||--|| ========== ========== ========== ========== ========== #>   0.2216  1.0000 ||--||     2.1825     1.8189     1.2110     1.1679     1.2340 #>                                                                       #>                                                                       #>  ----------- ----------- ----------- ---------- ---------- ---------- #>  dem60~ind60 dem65~ind60 dem65~dem60     y1~~y5     y2~~y4     y3~~y7 #>  =========== =========== =========== ========== ========== ========== #>       1.4534      0.5935      0.8659     0.5552     1.5947     0.7807 #>                                                                               #>                                                                               #>  ---------- ---------- ---------- ---------- ---------- ---------- ---------- #>      y4~~y8     y6~~y8     x1~~x1     x2~~x2     x3~~x3     y1~~y1     y2~~y2 #>  ========== ========== ========== ========== ========== ========== ========== #>      0.6537     1.5350     0.0820     0.1177     0.4675     1.7929     7.3843 #>                                                                                 #>                                                                                 #>  ---------- ---------- ---------- ---------- ---------- ---------- ------------ #>      y3~~y3     y4~~y4     y5~~y5     y6~~y6     y7~~y7     y8~~y8 ind60~~ind60 #>  ========== ========== ========== ========== ========== ========== ============ #>      5.0175     3.4074     2.2857     4.8977     3.5510     3.4511       0.4480 #>                                                             #>                                                             #>  ------------ ------------ ---------- ---------- ---------- #>  dem60~~dem60 dem65~~dem65   delta_a2   delta_b2   delta_c2 #>  ============ ============ ========== ========== ========== #>        3.9408       0.2034          .          .          ."},{"path":"/articles/lessSEM.html","id":"experimental-features","dir":"Articles","previous_headings":"","what":"Experimental Features","title":"lessSEM","text":"following features relatively new may still experience bugs. Please aware using features.","code":""},{"path":"/articles/lessSEM.html","id":"from-lesssem-to-lavaan","dir":"Articles","previous_headings":"Experimental Features","what":"From lessSEM to lavaan","title":"lessSEM","text":"lessSEM supports exporting specific models lavaan. can useful plotting final model. case, best model given : can get lavaan model parameters corresponding regularized model lambda = lambdaBest follows: result can plotted , instance, semPlot:","code":"lambdaBest <- coef(rsem, criterion = \"BIC\")$lambda lavaanModel <- lessSEM2Lavaan(regularizedSEM = rsem,                                lambda = lambdaBest) library(semPlot) semPaths(lavaanModel,          what = \"est\",          fade = FALSE)"},{"path":"/articles/lessSEM.html","id":"multi-group-models-and-definition-variables","dir":"Articles","previous_headings":"Experimental Features","what":"Multi-Group Models and Definition Variables","title":"lessSEM","text":"lessSEM supports multi-group SEM , degree, definition variables. Regularized multi-group SEM proposed Huang (2018) implemented lslx (Huang, 2020). , differences groups regularized. detailed introduction can found vignette(topic = \"Definition-Variables--Multi-Group-SEM\", package = \"lessSEM\"). Therein also explained multi-group SEM can used implement definition variables (e.g., latent growth curve models).","code":""},{"path":"/articles/lessSEM.html","id":"mixed-penalties","dir":"Articles","previous_headings":"Experimental Features","what":"Mixed Penalties","title":"lessSEM","text":"lessSEM allows defining different penalties different parts model. feature new experimental. Please keep mind using procedure. detailed introduction can found vignette(topic = \"Mixed-Penalties\", package = \"lessSEM\"). provide short example, regularize loadings regression parameters Political Democracy data set different penalties. following script adapted ?lavaan::sem. best model according BIC can extracted :","code":"model <- '    # latent variable definitions      ind60 =~ x1 + x2 + x3 + c2*y2 + c3*y3 + c4*y4      dem60 =~ y1 + y2 + y3 + y4      dem65 =~ y5 + y6 + y7 + c*y8    # regressions     dem60 ~ r1*ind60     dem65 ~ r2*ind60 + r3*dem60 '  lavaanModel <- sem(model,                    data = PoliticalDemocracy)  # Let's add a lasso penalty on the cross-loadings c2 - c4 and  # scad penalty on the regressions r1-r3 mp <- lavaanModel |>   mixedPenalty() |>   addLasso(regularized = c(\"c2\", \"c3\", \"c4\"),             lambdas = seq(0,1,.1)) |>   addLasso(regularized = c(\"r1\", \"r2\", \"r3\"),             lambdas = seq(0,1,.2)) |>   fit() coef(fitMp, criterion = \"BIC\")"},{"path":"/articles/lessSEM.html","id":"more-information","dir":"Articles","previous_headings":"","what":"More information","title":"lessSEM","text":"provide information documentation individual functions. instance, see ?lessSEM::lasso details lasso penalty. interested general purpose interface, look ?lessEM::gpLasso, ?lesssEM::gpMcp, etc. get details implementing lessSEM optimizers package, look vignettes vignette('General-Purpose-Optimization') vignette('-optimizer-interface') lessLM package.","code":""},{"path":"/articles/lessSEM.html","id":"table-of-the-most-relevant-functions","dir":"Articles","previous_headings":"","what":"Table of the most relevant functions","title":"lessSEM","text":"Fitting regularized SEM: Using optimizers lessSEM general purpose optimization: Using optimizers lessSEM general purpose optimization C++ functions:","code":""},{"path":[]},{"path":"/articles/lessSEM.html","id":"r---packages-software","dir":"Articles","previous_headings":"References","what":"R - Packages / Software","title":"lessSEM","text":"lavaan Rosseel, Y. (2012). lavaan: R Package Structural Equation Modeling. Journal Statistical Software, 48(2), 1–36. https://doi.org/10.18637/jss.v048.i02 regsem: Jacobucci, R. (2017). regsem: Regularized Structural Equation Modeling. ArXiv:1703.08489 [Stat]. http://arxiv.org/abs/1703.08489 lslx: Huang, P.-H. (2020). lslx: Semi-confirmatory structural equation modeling via penalized likelihood. Journal Statistical Software, 93(7). https://doi.org/10.18637/jss.v093.i07 fasta: Another implementation fista algorithm (Beck & Teboulle, 2009) ensmallen: Curtin, R. R., Edel, M., Prabhu, R. G., Basak, S., Lou, Z., & Sanderson, C. (2021). ensmallen library ﬂexible numerical optimization. Journal Machine Learning Research, 22, 1–6. RcppParallel Allaire J, Francois R, Ushey K, Vandenbrouck G, Geelnard M, Intel (2023). RcppParallel: Parallel Programming Tools ‘Rcpp’. R package version 5.1.6, https://CRAN.R-project.org/package=RcppParallel.","code":""},{"path":"/articles/lessSEM.html","id":"regularized-structural-equation-modeling-1","dir":"Articles","previous_headings":"References","what":"Regularized Structural Equation Modeling","title":"lessSEM","text":"Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Huang, P.-H. (2018). penalized likelihood method multi-group structural equation modelling. British Journal Mathematical Statistical Psychology, 71(3), 499–522. https://doi.org/10.1111/bmsp.12130 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793","code":""},{"path":"/articles/lessSEM.html","id":"penalty-functions","dir":"Articles","previous_headings":"References","what":"Penalty Functions","title":"lessSEM","text":"Candès, E. J., Wakin, M. B., & Boyd, S. P. (2008). Enhancing Sparsity Reweighted l1 Minimization. Journal Fourier Analysis Applications, 14(5–6), 877–905. https://doi.org/10.1007/s00041-008-9045-x Fan, J., & Li, R. (2001). Variable selection via nonconcave penalized likelihood oracle properties. Journal American Statistical Association, 96(456), 1348–1360. https://doi.org/10.1198/016214501753382273 Hoerl, . E., & Kennard, R. W. (1970). Ridge Regression: Biased Estimation Nonorthogonal Problems. Technometrics, 12(1), 55–67. https://doi.org/10.1080/00401706.1970.10488634 Tibshirani, R. (1996). Regression shrinkage selection via lasso. Journal Royal Statistical Society. Series B (Methodological), 58(1), 267–288. Zhang, C.-H. (2010). Nearly unbiased variable selection minimax concave penalty. Annals Statistics, 38(2), 894–942. https://doi.org/10.1214/09-AOS729 Zhang, T. (2010). Analysis Multi-stage Convex Relaxation Sparse Regularization. Journal Machine Learning Research, 11, 1081–1107. Zou, H. (2006). adaptive lasso oracle properties. Journal American Statistical Association, 101(476), 1418–1429. https://doi.org/10.1198/016214506000000735 Zou, H., & Hastie, T. (2005). Regularization variable selection via elastic net. Journal Royal Statistical Society: Series B, 67(2), 301–320. https://doi.org/10.1111/j.1467-9868.2005.00503.x","code":""},{"path":[]},{"path":"/articles/lessSEM.html","id":"glmnet","dir":"Articles","previous_headings":"References > Optimizer","what":"GLMNET","title":"lessSEM","text":"Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization paths generalized linear models via coordinate descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421","code":""},{"path":"/articles/lessSEM.html","id":"variants-of-ista","dir":"Articles","previous_headings":"References > Optimizer","what":"Variants of ISTA","title":"lessSEM","text":"Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). general iterative shrinkage thresholding algorithm non-convex regularized optimization problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/articles/lessSEM.html","id":"important-notes","dir":"Articles","previous_headings":"","what":"Important Notes","title":"lessSEM","text":"SOFTWARE PROVIDED ‘’, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"/articles/log-likelihood-gradients.html","id":"ableitung-der-log-likelihood","dir":"Articles","previous_headings":"","what":"Ableitung der Log-Likelihood","title":"log-likelihood-gradients","text":"\\[L(\\pmb\\theta) = \\underbrace{k\\ln(2\\pi)}_{1} + \\underbrace{\\ln(|\\pmb\\Sigma(\\pmb\\theta)|)}_{2} +  \\underbrace{(\\pmb x - \\pmb \\mu(\\pmb\\theta))^T\\pmb\\Sigma(\\pmb\\theta)^{-1}(\\pmb x - \\pmb \\mu(\\pmb\\theta))}_{3}\\] Wir wollen nach \\(\\pmb \\theta\\) ableiten.","code":""},{"path":"/articles/log-likelihood-gradients.html","id":"element-1","dir":"Articles","previous_headings":"Ableitung der Log-Likelihood","what":"Element 1","title":"log-likelihood-gradients","text":"Es gilt \\(\\frac{\\partial}{\\partial \\theta_j} k\\ln(2\\pi)= 0\\)","code":""},{"path":"/articles/log-likelihood-gradients.html","id":"element-2","dir":"Articles","previous_headings":"Ableitung der Log-Likelihood","what":"Element 2","title":"log-likelihood-gradients","text":"Es gilt: \\[\\frac{\\partial}{\\partial \\theta_j}\\ln(|\\pmb\\Sigma(\\pmb\\theta)|) = \\frac{1}{|\\pmb\\Sigma(\\pmb\\theta)|}\\frac{\\partial}{\\partial \\theta_j}|\\pmb\\Sigma(\\pmb\\theta)|\\] Jacobis Formel: \\[\\frac{\\partial}{\\partial \\theta_j}|\\pmb\\Sigma(\\pmb\\theta)| = |\\pmb\\Sigma(\\pmb\\theta)|tr(\\pmb\\Sigma(\\pmb\\theta)^{-1}\\frac{\\partial}{\\partial \\theta_j}\\pmb\\Sigma(\\pmb\\theta))\\] und somit: \\[\\frac{\\partial}{\\partial \\theta_j}\\ln(|\\pmb\\Sigma(\\pmb\\theta)|) = \\frac{1}{|\\pmb\\Sigma(\\pmb\\theta)|}|\\pmb\\Sigma(\\pmb\\theta)|tr(\\pmb\\Sigma(\\pmb\\theta)^{-1}\\frac{\\partial}{\\partial \\theta_j}\\pmb\\Sigma(\\pmb\\theta)) = tr(\\pmb\\Sigma(\\pmb\\theta)^{-1}\\frac{\\partial}{\\partial \\theta_j}\\pmb\\Sigma(\\pmb\\theta))\\] Wir brauchen also die Ableitung der modell-implizierten Kovarianzmatrix nach den Parametern: \\(\\frac{\\partial}{\\partial \\theta_j}\\pmb\\Sigma(\\pmb\\theta)\\). Dabei gilt: \\(\\pmb\\Sigma(\\pmb\\theta) = \\pmb F (\\pmb - \\pmb )^{-1} \\pmb S ((\\pmb - \\pmb )^{-1})^T \\pmb F^T\\).","code":""},{"path":"/articles/log-likelihood-gradients.html","id":"fall-1-der-parameter-theta_j-ist-in-pmb-s-","dir":"Articles","previous_headings":"Ableitung der Log-Likelihood > Element 2","what":"Fall 1: Der Parameter \\(\\theta_j\\) ist in \\(\\pmb S\\).","title":"log-likelihood-gradients","text":"Dann gilt: Außer \\(\\pmb S\\) kann alles andere als Konstante behandelt werden. Es folgt: \\[\\frac{\\partial}{\\partial \\theta_j}\\pmb\\Sigma(\\pmb\\theta) = \\pmb F (\\pmb - \\pmb )^{-1} \\frac{\\partial}{\\partial \\theta_j}\\pmb S ((\\pmb - \\pmb )^{-1})^T \\pmb F^T\\] wobei \\(\\frac{\\partial}{\\partial \\theta_j}\\pmb S\\) eine sparse Matrix mit einsen den Stellen ist, denen \\(\\theta_j\\) vorkommt. Zusammenfassung: \\[\\frac{\\partial}{\\partial \\theta_j}\\ln(|\\pmb\\Sigma(\\pmb\\theta)|) = tr(\\pmb\\Sigma(\\pmb\\theta)^{-1}\\pmb F (\\pmb - \\pmb )^{-1} \\frac{\\partial}{\\partial \\theta_j}\\pmb S ((\\pmb - \\pmb )^{-1})^T \\pmb F^T)\\] Achtung: Wenn die Person Missings hat, kann man die Matrix \\(\\pmb F\\) anpassen, dass die entsprechenden Zeilen und Spalten herausfallen.","code":""},{"path":"/articles/log-likelihood-gradients.html","id":"fall-2-der-parameter-theta_j-ist-in-pmb-a-","dir":"Articles","previous_headings":"Ableitung der Log-Likelihood > Element 2","what":"Fall 2: Der Parameter \\(\\theta_j\\) ist in \\(\\pmb A\\).","title":"log-likelihood-gradients","text":"Dann gilt: Außer \\(\\pmb \\) kann alles andere als Konstante behandelt werden. Zudem gilt: \\(\\frac{\\partial}{\\partial a_i}\\pmb ^{-1} = \\pmb ^{-1}\\frac{\\partial \\pmb }{\\partial a_i} \\pmb ^{-1}\\) (https://math.stackexchange.com/questions/4074265/derivative-involving-inverse-matrix?noredirect=1&lq=1). Es folgt: \\[\\frac{\\partial}{\\partial \\theta_j}\\pmb\\Sigma(\\pmb\\theta) = \\pmb F[(\\pmb - \\pmb )^{-1} \\frac{\\partial\\pmb }{\\partial \\theta_j}(\\pmb - \\pmb )^{-1}][\\pmb S ((\\pmb - \\pmb )^{-1})^T \\pmb F^T] + \\pmb F(\\pmb - \\pmb )^{-1} \\pmb S[(\\pmb - \\pmb )^{-1} \\frac{\\partial\\pmb }{\\partial \\theta_j}(\\pmb - \\pmb )^{-1}]^T\\pmb F^T\\] Zusammenfassung: \\[\\frac{\\partial}{\\partial \\theta_j}\\ln(|\\pmb\\Sigma(\\pmb\\theta)|) = tr(\\pmb\\Sigma(\\pmb\\theta)^{-1}[\\pmb F[(\\pmb - \\pmb )^{-1} \\frac{\\partial\\pmb }{\\partial \\theta_j}(\\pmb - \\pmb )^{-1}][\\pmb S ((\\pmb - \\pmb )^{-1})^T \\pmb F^T] + \\pmb F(\\pmb - \\pmb )^{-1} \\pmb S[(\\pmb - \\pmb )^{-1} \\frac{\\partial\\pmb }{\\partial \\theta_j}(\\pmb - \\pmb )^{-1}]^T\\pmb F^T])\\]","code":""},{"path":"/articles/log-likelihood-gradients.html","id":"fall-3-der-parameter-theta_j-ist-in-pmb-m-wobei-pmb-m-die-mittelwertstruktur-des-sem-ist-","dir":"Articles","previous_headings":"Ableitung der Log-Likelihood > Element 2","what":"Fall 3: Der Parameter \\(\\theta_j\\) ist in \\(\\pmb m\\), wobei \\(\\pmb m\\) die Mittelwertstruktur des SEM ist.","title":"log-likelihood-gradients","text":"Dann gilt: Die Ableitung ist \\(0\\). Hinweis: Element 2 ist unabhängig vom Datensatz!","code":""},{"path":"/articles/log-likelihood-gradients.html","id":"element-3","dir":"Articles","previous_headings":"Ableitung der Log-Likelihood","what":"Element 3","title":"log-likelihood-gradients","text":"\\[\\frac{\\partial}{\\partial \\theta_j}(\\pmb x - \\pmb \\mu(\\pmb\\theta))^T\\pmb\\Sigma(\\pmb\\theta)^{-1}(\\pmb x - \\pmb \\mu(\\pmb\\theta))\\] Es gilt: \\[\\begin{aligned} &\\frac{\\partial}{\\partial \\theta_j}(\\pmb x - \\pmb \\mu(\\pmb\\theta))^T\\pmb\\Sigma(\\pmb\\theta)^{-1}(\\pmb x - \\pmb \\mu(\\pmb\\theta))\\\\ =& [\\frac{\\partial}{\\partial \\theta_j}(\\pmb x - \\pmb \\mu(\\pmb\\theta))^T]\\pmb\\Sigma(\\pmb\\theta)^{-1}(\\pmb x - \\pmb \\mu(\\pmb\\theta)) + (\\pmb x - \\pmb \\mu(\\pmb\\theta))^T\\frac{\\partial}{\\partial \\theta_j}[\\pmb\\Sigma(\\pmb\\theta)^{-1}(\\pmb x - \\pmb \\mu(\\pmb\\theta))] \\\\ =& [\\frac{\\partial}{\\partial \\theta_j}(\\pmb x - \\pmb \\mu(\\pmb\\theta))^T]\\pmb\\Sigma(\\pmb\\theta)^{-1}(\\pmb x - \\pmb \\mu(\\pmb\\theta)) + (\\pmb x - \\pmb \\mu(\\pmb\\theta))^T[\\frac{\\partial}{\\partial \\theta_j}\\pmb\\Sigma(\\pmb\\theta)^{-1}](\\pmb x - \\pmb \\mu(\\pmb\\theta)) + (\\pmb x - \\pmb \\mu(\\pmb\\theta))^T\\pmb\\Sigma(\\pmb\\theta)^{-1}\\frac{\\partial}{\\partial \\theta_j}[(\\pmb x - \\pmb \\mu(\\pmb\\theta))] \\end{aligned}\\] mit \\(\\pmb\\mu (\\pmb\\theta) = \\pmb F(\\pmb - \\pmb )^{-1}\\pmb m\\) wobei \\(\\pmb m\\) die Mittelwertstruktur des SEMs ist.","code":""},{"path":"/articles/log-likelihood-gradients.html","id":"fall-1-der-parameter-theta_j-ist-in-pmb-s--1","dir":"Articles","previous_headings":"Ableitung der Log-Likelihood > Element 3","what":"Fall 1: Der Parameter \\(\\theta_j\\) ist in \\(\\pmb S\\).","title":"log-likelihood-gradients","text":"Dann gilt: Außer \\(\\pmb S\\) kann alles andere als Konstante behandelt werden. Es folgt: \\([\\frac{\\partial}{\\partial \\theta_j}(\\pmb x - \\pmb \\mu(\\pmb\\theta))^T] = 0\\) und somit \\[\\begin{aligned} &[\\frac{\\partial}{\\partial \\theta_j}(\\pmb x - \\pmb \\mu(\\pmb\\theta))^T]\\pmb\\Sigma(\\pmb\\theta)^{-1}(\\pmb x - \\pmb \\mu(\\pmb\\theta)) + (\\pmb x - \\pmb \\mu(\\pmb\\theta))^T[\\frac{\\partial}{\\partial \\theta_j}\\pmb\\Sigma(\\pmb\\theta)^{-1}](\\pmb x - \\pmb \\mu(\\pmb\\theta)) + (\\pmb x - \\pmb \\mu(\\pmb\\theta))^T\\pmb\\Sigma(\\pmb\\theta)^{-1}\\frac{\\partial}{\\partial \\theta_j}[(\\pmb x - \\pmb \\mu(\\pmb\\theta))] \\\\ =&(\\pmb x - \\pmb \\mu(\\pmb\\theta))^T[\\frac{\\partial}{\\partial \\theta_j}\\pmb\\Sigma(\\pmb\\theta)^{-1}](\\pmb x - \\pmb \\mu(\\pmb\\theta)) \\end{aligned}\\] Es gilt (https://math.stackexchange.com/questions/4074265/derivative-involving-inverse-matrix?noredirect=1&lq=1): \\[\\frac{\\partial}{\\partial \\theta_j} \\pmb \\Sigma(\\pmb\\theta)^{-1} = -\\pmb \\Sigma(\\pmb\\theta)^{-1}\\frac{\\partial}{\\partial \\theta_j}\\pmb \\Sigma(\\pmb\\theta)\\Sigma(\\pmb\\theta)^{-1}\\] und somit: \\[\\begin{aligned} &\\frac{\\partial}{\\partial \\theta_j}(\\pmb x - \\pmb \\mu(\\pmb\\theta))^T\\pmb\\Sigma(\\pmb\\theta)^{-1}(\\pmb x - \\pmb \\mu(\\pmb\\theta))\\\\ =&(\\pmb x - \\pmb \\mu(\\pmb\\theta))^T[\\frac{\\partial}{\\partial \\theta_j}\\pmb\\Sigma(\\pmb\\theta)^{-1}](\\pmb x - \\pmb \\mu(\\pmb\\theta))\\\\ =& (\\pmb x - \\pmb \\mu(\\pmb\\theta))^T[-\\pmb \\Sigma(\\pmb\\theta)^{-1}\\frac{\\partial}{\\partial \\theta_j}\\pmb \\Sigma(\\pmb\\theta)\\Sigma(\\pmb\\theta)^{-1}](\\pmb x - \\pmb \\mu(\\pmb\\theta))\\\\ =& (\\pmb x - \\pmb \\mu(\\pmb\\theta))^T[-\\pmb \\Sigma(\\pmb\\theta)^{-1}\\pmb F (\\pmb - \\pmb )^{-1} \\frac{\\partial}{\\partial \\theta_j}\\pmb S ((\\pmb - \\pmb )^{-1})^T \\pmb F^T\\pmb\\Sigma(\\pmb\\theta)^{-1}](\\pmb x - \\pmb \\mu(\\pmb\\theta))\\\\ \\end{aligned}\\] Hinweis: Der letzte Schritt wurde bei Element 2 besprochen.","code":""},{"path":"/articles/log-likelihood-gradients.html","id":"fall-2-der-parameter-theta_j-ist-in-pmb-a--1","dir":"Articles","previous_headings":"Ableitung der Log-Likelihood > Element 3","what":"Fall 2: Der Parameter \\(\\theta_j\\) ist in \\(\\pmb A\\).","title":"log-likelihood-gradients","text":"\\(\\pmb \\) findet sich auch der Mittelwertstruktur wieder. Hier gilt \\[\\begin{aligned} &\\frac{\\partial}{\\partial \\theta_j}(\\pmb x - \\pmb \\mu(\\pmb\\theta))^T\\pmb\\Sigma(\\pmb\\theta)^{-1}(\\pmb x - \\pmb \\mu(\\pmb\\theta))\\\\ =& [\\frac{\\partial}{\\partial \\theta_j}(\\pmb x - \\pmb \\mu(\\pmb\\theta))^T]\\pmb\\Sigma(\\pmb\\theta)^{-1}(\\pmb x - \\pmb \\mu(\\pmb\\theta)) + (\\pmb x - \\pmb \\mu(\\pmb\\theta))^T[\\frac{\\partial}{\\partial \\theta_j}\\pmb\\Sigma(\\pmb\\theta)^{-1}](\\pmb x - \\pmb \\mu(\\pmb\\theta)) + (\\pmb x - \\pmb \\mu(\\pmb\\theta))^T\\pmb\\Sigma(\\pmb\\theta)^{-1}\\frac{\\partial}{\\partial \\theta_j}[(\\pmb x - \\pmb \\mu(\\pmb\\theta))] \\end{aligned}\\] mit \\([\\frac{\\partial}{\\partial \\theta_j}(\\pmb x - \\pmb \\mu(\\pmb\\theta))] = [- \\frac{\\partial}{\\partial \\theta_j}\\pmb \\mu(\\pmb\\theta))] = -\\frac{\\partial}{\\partial \\theta_j}\\pmb F(\\pmb - \\pmb )^{-1}\\pmb m = -\\pmb F(\\pmb - \\pmb )^{-1}\\frac{\\partial (\\pmb - \\pmb )}{\\partial \\theta_j}(\\pmb - \\pmb )^{-1}\\pmb m\\) Es folgt: \\[\\begin{aligned} &\\frac{\\partial}{\\partial \\theta_j}(\\pmb x - \\pmb \\mu(\\pmb\\theta))^T\\pmb\\Sigma(\\pmb\\theta)^{-1}(\\pmb x - \\pmb \\mu(\\pmb\\theta))\\\\ =& 2*[-\\pmb F(\\pmb - \\pmb )^{-1}\\frac{\\partial (\\pmb - \\pmb )}{\\partial \\theta_j}(\\pmb - \\pmb )^{-1}\\pmb m]^T\\pmb\\Sigma(\\pmb\\theta)^{-1}(\\pmb x - \\pmb \\mu(\\pmb\\theta)) + (\\pmb x - \\pmb \\mu(\\pmb\\theta))^T[\\frac{\\partial}{\\partial \\theta_j}\\pmb\\Sigma(\\pmb\\theta)^{-1}](\\pmb x - \\pmb \\mu(\\pmb\\theta))\\\\ =& 2*[-\\pmb F(\\pmb - \\pmb )^{-1}\\frac{\\partial (\\pmb - \\pmb )}{\\partial \\theta_j}(\\pmb - \\pmb )^{-1}\\pmb m]^T\\pmb\\Sigma(\\pmb\\theta)^{-1}(\\pmb x - \\pmb \\mu(\\pmb\\theta)) \\\\ &+ (\\pmb x - \\pmb \\mu(\\pmb\\theta))^T[-\\pmb \\Sigma(\\pmb\\theta)^{-1}[\\pmb F[(\\pmb - \\pmb )^{-1} \\frac{\\partial\\pmb }{\\partial \\theta_j}(\\pmb - \\pmb )^{-1}][\\pmb S ((\\pmb - \\pmb )^{-1})^T \\pmb F^T] \\\\ &+ \\pmb F(\\pmb - \\pmb )^{-1} \\pmb S[(\\pmb - \\pmb )^{-1} \\frac{\\partial\\pmb }{\\partial \\theta_j}(\\pmb - \\pmb )^{-1}]^T\\pmb F^T]\\pmb \\Sigma(\\pmb\\theta)^{-1}](\\pmb x - \\pmb \\mu(\\pmb\\theta))\\\\ \\end{aligned}\\] Hinweis: Der letzte Schritt wurde bei Element 3 besprochen.","code":""},{"path":"/articles/log-likelihood-gradients.html","id":"fall-3-der-parameter-theta_j-ist-in-pmb-m-","dir":"Articles","previous_headings":"Ableitung der Log-Likelihood > Element 3","what":"Fall 3: Der Parameter \\(\\theta_j\\) ist in \\(\\pmb m\\).","title":"log-likelihood-gradients","text":"Dann gilt: Außer \\(\\pmb\\mu (\\pmb\\theta) = \\pmb F(\\pmb - \\pmb )^{-1}\\pmb m\\) kann alles andere als Konstante behandelt werden. \\[\\begin{aligned} &\\frac{\\partial}{\\partial \\theta_j}(\\pmb x - \\pmb \\mu(\\pmb\\theta))^T\\pmb\\Sigma(\\pmb\\theta)^{-1}(\\pmb x - \\pmb \\mu(\\pmb\\theta))\\\\ =& [\\frac{\\partial}{\\partial \\theta_j}(\\pmb x - \\pmb \\mu(\\pmb\\theta))^T]\\pmb\\Sigma(\\pmb\\theta)^{-1}(\\pmb x - \\pmb \\mu(\\pmb\\theta)) + (\\pmb x - \\pmb \\mu(\\pmb\\theta))^T\\frac{\\partial}{\\partial \\theta_j}[\\pmb\\Sigma(\\pmb\\theta)^{-1}(\\pmb x - \\pmb \\mu(\\pmb\\theta))] \\\\ =& [\\frac{\\partial}{\\partial \\theta_j}(\\pmb x - \\pmb \\mu(\\pmb\\theta))^T]\\pmb\\Sigma(\\pmb\\theta)^{-1}(\\pmb x - \\pmb \\mu(\\pmb\\theta)) + (\\pmb x - \\pmb \\mu(\\pmb\\theta))^T\\pmb\\Sigma(\\pmb\\theta)^{-1}\\frac{\\partial}{\\partial \\theta_j}[(\\pmb x - \\pmb \\mu(\\pmb\\theta))] \\\\ =& (-\\pmb F(\\pmb - \\pmb )^{-1}\\pmb e)^T\\pmb\\Sigma(\\pmb\\theta)^{-1}(\\pmb x - \\pmb \\mu(\\pmb\\theta)) + (\\pmb x - \\pmb \\mu(\\pmb\\theta))^T\\pmb\\Sigma(\\pmb\\theta)^{-1}(-\\pmb F(\\pmb - \\pmb )^{-1}\\pmb e)\\\\ =& 2*(- \\pmb F(\\pmb - \\pmb )^{-1}\\pmb e)^T\\pmb\\Sigma(\\pmb\\theta)^{-1}(\\pmb x - \\pmb \\mu(\\pmb\\theta)) \\end{aligned}\\] wobei \\(\\pmb e = \\begin{bmatrix} 0 & 0 & ... & 1 & ... &0\\end{bmatrix}^T\\) ein Vektor ist, der eine eins der Stelle hat, der \\(\\theta_j\\) \\(\\pmb m\\) sitzt.","code":""},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Jannik H. Orzek. Maintainer.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Orzek JH (2023). lessSEM: Regularized structural equation models non-smooth penalties. R package version 1.4.3.","code":"@Manual{,   title = {lessSEM: Regularized structural equation models with non-smooth penalties},   author = {Jannik H. Orzek},   year = {2023},   note = {R package version 1.4.3}, }"},{"path":"/index.html","id":"lesssem-","dir":"","previous_headings":"","what":"Regularized structural equation models with non-smooth penalties","title":"Regularized structural equation models with non-smooth penalties","text":"lessSEM (lessSEM estimates sparse SEM) R package regularized structural equation modeling (regularized SEM) non-smooth penalty functions (e.g., lasso) building lavaan. lessSEM heavily inspired regsem package lslx packages similar functionality. objectives lessSEM provide … flexible framework regularizing SEM optimizers SEM packages can used interface similar optim. Note: Please also check implementations regularized SEM mature R packages regsem lslx. Finally, may want check julia package StructuralEquationModels.jl. following penalty functions currently implemented lessSEM:  column “penalty” refers name function call lessSEM package (e.g., lasso called lasso() function). Smooth functions called smoothLasso, smoothAdaptiveLasso, smoothElasticNet. implemented comparison exact approximate optimization used. marked deprecated soon. best model can selected AIC BIC. want use cross-validation, use cvLasso, cvAdaptiveLasso, etc. instead (see, e.g., ?lessSEM::cvLasso). smooth versions called cvSmoothLasso, etc. Currently, lessSEM following optimizers: (variants ) iterative shrinkage thresholding (e.g., Beck & Teboulle, 2009; Gong et al., 2013; Parikh & Boyd, 2013); optimization cappedL1, lsp, scad, mcp based Gong et al. (2013) glmnet (Friedman et al., 2010; Yuan et al., 2012; Huang, 2020) optimizers implemented based regCtsem package. importantly, optimizers lessSEM available packages. three ways implement documented vignette(\"General-Purpose-Optimization\", package = \"lessSEM\"). short, : using R interface: general purpose implementations functions called prefix “gp” (gpLasso, gpScad, …). information examples can found documentation functions (e.g., ?lessSEM::gpLasso, ?lessSEM::gpAdaptiveLasso, ?lessSEM::gpElasticNet). interface similar optim optimizers R. using Rcpp, can pass C++ function pointers general purpose optimizers gpLassoCpp, gpScadCpp, … (e.g., ?lessSEM::gpLassoCpp) optimizers implemented C++ header-files lessSEM. Thus, can accessed packages using C++. interface similar ensmallen library. implemented simple example elastic net regularization linear regressions lessLM package. can also find details general design optimizer interface vignette(\"-optimizer-interface\", package = \"lessSEM\"). Identical regsem, lessSEM specified using model built lavaan. lessSEM can handle missing data means full information maximum likelihood estimation allows equality constraints parameters. may, however, also want check regsem lslx offer features still missing lessSEM. distinct feature lessSEM parameter transformations (see example). Additionally, lessSEM allows mixed penalties. Finally, lessSEM can make use multiple cores.","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Regularized structural equation models with non-smooth penalties","text":"want install lessSEM GitHub, use following commands R:","code":"if(!require(devtools))install.packages(\"devtools\") devtools::install_github(\"jhorzek/lessSEM\")"},{"path":"/index.html","id":"introduction","dir":"","previous_headings":"","what":"Introduction","title":"Regularized structural equation models with non-smooth penalties","text":"find short introduction regularized SEM lessSEM package vignette('lessSEM', package = 'lessSEM'). information also provided documentation individual functions (e.g., see ?lessSEM::scad). Finally, find templates selection models can used lessSEM (e.g., cross-lagged panel model) package lessTemplates.","code":""},{"path":"/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"Regularized structural equation models with non-smooth penalties","text":"","code":"library(lessSEM) #> Loading required package: lavaan #> This is lavaan 0.6-14 #> lavaan is FREE software! Please report any bugs. library(lavaan)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \"       f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +             l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +             l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15       f ~~ 1*f       \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Optional: Plot the model # semPlot::semPaths(lavaanModel,  #                   what = \"est\", #                   fade = FALSE)  lsem <- lasso(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   # names of the regularized parameters:   regularized = c(\"l6\", \"l7\", \"l8\", \"l9\", \"l10\",                   \"l11\", \"l12\", \"l13\", \"l14\", \"l15\"),   # in case of lasso and adaptive lasso, we can specify the number of lambda   # values to use. lessSEM will automatically find lambda_max and fit   # models for nLambda values between 0 and lambda_max. For the other   # penalty functions, lambdas must be specified explicitly   nLambdas = 50)  # use the plot-function to plot the regularized parameters: plot(lsem)  # use the coef-function to show the estimates coef(lsem)  # The best parameters can be extracted with: coef(lsem, criterion = \"AIC\") coef(lsem, criterion = \"BIC\")  # elements of lsem can be accessed with the @ operator: lsem@parameters[1,]  # AIC and BIC for all tuning parameter configurations: AIC(lsem) BIC(lsem)  # cross-validation cv <- cvLasso(lavaanModel = lavaanModel,               regularized = c(\"l6\", \"l7\", \"l8\", \"l9\", \"l10\",                               \"l11\", \"l12\", \"l13\", \"l14\", \"l15\"),               lambdas = seq(0,1,.1),               standardize = TRUE)  # get best model according to cross-validation: coef(cv)  #### Advanced ### # Switching the optimizer #  # Use the \"method\" argument to switch the optimizer. The control argument # must also be changed to the corresponding function: lsemIsta <- lasso(   lavaanModel = lavaanModel,   regularized = paste0(\"l\", 6:15),   nLambdas = 50,   method = \"ista\",   control = controlIsta(     # Here, we can also specify that we want to use multiple cores:     nCores = 2))  # Note: The results are basically identical: lsemIsta@parameters - lsem@parameters"},{"path":"/index.html","id":"transformations","dir":"","previous_headings":"","what":"Transformations","title":"Regularized structural equation models with non-smooth penalties","text":"lessSEM allows parameter transformations , instance, used test measurement invariance longitudinal models (e.g., Liang, 2018; Bauer et al., 2020). thorough introduction provided vignette('Parameter-transformations', package = 'lessSEM'). example, test measurement invariance PoliticalDemocracy data set. Finally, can extract best parameters: differences (delta_a2, delta_b2, delta_c2) zeroed, can assume measurement invariance.","code":"library(lessSEM) library(lavaan) # we will use the PoliticalDemocracy from lavaan (see ?lavaan::sem) model <- '    # latent variable definitions      ind60 =~ x1 + x2 + x3      # assuming different loadings for different time points:      dem60 =~ y1 + a1*y2 + b1*y3 + c1*y4      dem65 =~ y5 + a2*y6 + b2*y7 + c2*y8    # regressions     dem60 ~ ind60     dem65 ~ ind60 + dem60    # residual correlations     y1 ~~ y5     y2 ~~ y4 + y6     y3 ~~ y7     y4 ~~ y8     y6 ~~ y8 '  fit <- sem(model, data = PoliticalDemocracy)  # We will define a transformation which regularizes differences # between loadings over time:  transformations <- \" // which parameters do we want to use? parameters: a1, a2, b1, b2, c1, c2, delta_a2, delta_b2, delta_c2  // transformations: a2 = a1 + delta_a2; b2 = b1 + delta_b2; c2 = c1 + delta_c2; \"  # setting delta_a2, delta_b2, or delta_c2 to zero implies measurement invariance # for the respective parameters (a1, b1, c1) lassoFit <- lasso(lavaanModel = fit,                    # we want to regularize the differences between the parameters                   regularized = c(\"delta_a2\", \"delta_b2\", \"delta_c2\"),                   nLambdas = 100,                   # Our model modification must make use of the modifyModel - function:                   modifyModel = modifyModel(transformations = transformations) ) coef(lassoFit, criterion = \"BIC\")"},{"path":"/index.html","id":"experimental-features","dir":"","previous_headings":"","what":"Experimental Features","title":"Regularized structural equation models with non-smooth penalties","text":"following features relatively new may still experience bugs. Please aware using features.","code":""},{"path":"/index.html","id":"from-lesssem-to-lavaan","dir":"","previous_headings":"","what":"From lessSEM to lavaan","title":"Regularized structural equation models with non-smooth penalties","text":"lessSEM supports exporting specific models lavaan. can useful plotting final model. case, best model given : can get lavaan model parameters corresponding regularized model lambda = lambdaBest follows: result can plotted , instance, semPlot:","code":"lambdaBest <- coef(lsem, criterion = \"BIC\")@tuningParameters$lambda lavaanModel <- lessSEM2Lavaan(regularizedSEM = lsem,                                lambda = lambdaBest) library(semPlot) semPaths(lavaanModel,          what = \"est\",          fade = FALSE)"},{"path":"/index.html","id":"multi-group-models-and-definition-variables","dir":"","previous_headings":"","what":"Multi-Group Models and Definition Variables","title":"Regularized structural equation models with non-smooth penalties","text":"lessSEM supports multi-group SEM , degree, definition variables. Regularized multi-group SEM proposed Huang (2018) implemented lslx (Huang, 2020). , differences groups regularized. detailed introduction can found vignette(topic = \"Definition-Variables--Multi-Group-SEM\", package = \"lessSEM\"). Therein also explained multi-group SEM can used implement definition variables (e.g., latent growth curve models).","code":""},{"path":"/index.html","id":"mixed-penalties","dir":"","previous_headings":"","what":"Mixed Penalties","title":"Regularized structural equation models with non-smooth penalties","text":"lessSEM allows defining different penalties different parts model. feature new experimental. Please keep mind using procedure. detailed introduction can found vignette(topic = \"Mixed-Penalties\", package = \"lessSEM\"). provide short example, regularize loadings regression parameters Political Democracy data set different penalties. following script adapted ?lavaan::sem. best model according BIC can extracted :","code":"model <- '    # latent variable definitions      ind60 =~ x1 + x2 + x3 + c2*y2 + c3*y3 + c4*y4      dem60 =~ y1 + y2 + y3 + y4      dem65 =~ y5 + y6 + y7 + c*y8    # regressions     dem60 ~ r1*ind60     dem65 ~ r2*ind60 + r3*dem60 '  lavaanModel <- sem(model,                    data = PoliticalDemocracy)  # Let's add a lasso penalty on the cross-loadings c2 - c4 and  # scad penalty on the regressions r1-r3 fitMp <- lavaanModel |>   mixedPenalty() |>   addLasso(regularized = c(\"c2\", \"c3\", \"c4\"),             lambdas = seq(0,1,.1)) |>   addScad(regularized = c(\"r1\", \"r2\", \"r3\"),            lambdas = seq(0,1,.2),           thetas = 3.7) |>   fit() coef(fitMp, criterion = \"BIC\")"},{"path":[]},{"path":"/index.html","id":"r---packages--software","dir":"","previous_headings":"","what":"R - Packages / Software","title":"Regularized structural equation models with non-smooth penalties","text":"lavaan Rosseel, Y. (2012). lavaan: R Package Structural Equation Modeling. Journal Statistical Software, 48(2), 1–36. https://doi.org/10.18637/jss.v048.i02 regsem: Jacobucci, R. (2017). regsem: Regularized Structural Equation Modeling. ArXiv:1703.08489 [Stat]. http://arxiv.org/abs/1703.08489 lslx: Huang, P.-H. (2020). lslx: Semi-confirmatory structural equation modeling via penalized likelihood. Journal Statistical Software, 93(7). https://doi.org/10.18637/jss.v093.i07 fasta: Another implementation fista algorithm (Beck & Teboulle, 2009). ensmallen: Curtin, R. R., Edel, M., Prabhu, R. G., Basak, S., Lou, Z., & Sanderson, C. (2021). ensmallen library ﬂexible numerical optimization. Journal Machine Learning Research, 22, 1–6. regCtsem: Orzek, J. H., & Voelkle, M. C. (press). Regularized continuous time structural equation models: network perspective. Psychological Methods.","code":""},{"path":"/index.html","id":"regularized-structural-equation-modeling","dir":"","previous_headings":"","what":"Regularized Structural Equation Modeling","title":"Regularized structural equation models with non-smooth penalties","text":"Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Huang, P.-H. (2018). penalized likelihood method multi-group structural equation modelling. British Journal Mathematical Statistical Psychology, 71(3), 499–522. https://doi.org/10.1111/bmsp.12130 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793","code":""},{"path":"/index.html","id":"penalty-functions","dir":"","previous_headings":"","what":"Penalty Functions","title":"Regularized structural equation models with non-smooth penalties","text":"Candès, E. J., Wakin, M. B., & Boyd, S. P. (2008). Enhancing Sparsity Reweighted l1 Minimization. Journal Fourier Analysis Applications, 14(5–6), 877–905. https://doi.org/10.1007/s00041-008-9045-x Fan, J., & Li, R. (2001). Variable selection via nonconcave penalized likelihood oracle properties. Journal American Statistical Association, 96(456), 1348–1360. https://doi.org/10.1198/016214501753382273 Hoerl, . E., & Kennard, R. W. (1970). Ridge Regression: Biased Estimation Nonorthogonal Problems. Technometrics, 12(1), 55–67. https://doi.org/10.1080/00401706.1970.10488634 Tibshirani, R. (1996). Regression shrinkage selection via lasso. Journal Royal Statistical Society. Series B (Methodological), 58(1), 267–288. Zhang, C.-H. (2010). Nearly unbiased variable selection minimax concave penalty. Annals Statistics, 38(2), 894–942. https://doi.org/10.1214/09-AOS729 Zhang, T. (2010). Analysis Multi-stage Convex Relaxation Sparse Regularization. Journal Machine Learning Research, 11, 1081–1107. Zou, H. (2006). adaptive lasso oracle properties. Journal American Statistical Association, 101(476), 1418–1429. https://doi.org/10.1198/016214506000000735 Zou, H., & Hastie, T. (2005). Regularization variable selection via elastic net. Journal Royal Statistical Society: Series B, 67(2), 301–320. https://doi.org/10.1111/j.1467-9868.2005.00503.x","code":""},{"path":[]},{"path":"/index.html","id":"glmnet","dir":"","previous_headings":"Optimizer","what":"GLMNET","title":"Regularized structural equation models with non-smooth penalties","text":"Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization paths generalized linear models via coordinate descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421","code":""},{"path":"/index.html","id":"variants-of-ista","dir":"","previous_headings":"Optimizer","what":"Variants of ISTA","title":"Regularized structural equation models with non-smooth penalties","text":"Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). general iterative shrinkage thresholding algorithm non-convex regularized optimization problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/index.html","id":"miscellaneous","dir":"","previous_headings":"","what":"Miscellaneous","title":"Regularized structural equation models with non-smooth penalties","text":"Liang, X., Yang, Y., & Huang, J. (2018). Evaluation structural relationships autoregressive cross-lagged models longitudinal approximate invariance: Bayesian analysis. Structural Equation Modeling: Multidisciplinary Journal, 25(4), 558–572. https://doi.org/10.1080/10705511.2017.1410706 Bauer, D. J., Belzak, W. C. M., & Cole, V. T. (2020). Simplifying Assessment Measurement Invariance Multiple Background Variables: Using Regularized Moderated Nonlinear Factor Analysis Detect Differential Item Functioning. Structural Equation Modeling: Multidisciplinary Journal, 27(1), 43–55. https://doi.org/10.1080/10705511.2019.1642754","code":""},{"path":"/index.html","id":"important-notes","dir":"","previous_headings":"","what":"Important Notes","title":"Regularized structural equation models with non-smooth penalties","text":"SOFTWARE PROVIDED ‘’, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"/reference/AIC-Rcpp_SEMCpp-method.html","id":null,"dir":"Reference","previous_headings":"","what":"AIC — AIC,Rcpp_SEMCpp-method","title":"AIC — AIC,Rcpp_SEMCpp-method","text":"AIC","code":""},{"path":"/reference/AIC-Rcpp_SEMCpp-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"AIC — AIC,Rcpp_SEMCpp-method","text":"","code":"# S4 method for Rcpp_SEMCpp AIC(object)"},{"path":"/reference/AIC-Rcpp_SEMCpp-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"AIC — AIC,Rcpp_SEMCpp-method","text":"object object class Rcpp_SEMCpp","code":""},{"path":"/reference/AIC-Rcpp_SEMCpp-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"AIC — AIC,Rcpp_SEMCpp-method","text":"AIC values","code":""},{"path":"/reference/AIC-Rcpp_mgSEM-method.html","id":null,"dir":"Reference","previous_headings":"","what":"AIC — AIC,Rcpp_mgSEM-method","title":"AIC — AIC,Rcpp_mgSEM-method","text":"AIC","code":""},{"path":"/reference/AIC-Rcpp_mgSEM-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"AIC — AIC,Rcpp_mgSEM-method","text":"","code":"# S4 method for Rcpp_mgSEM AIC(object)"},{"path":"/reference/AIC-Rcpp_mgSEM-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"AIC — AIC,Rcpp_mgSEM-method","text":"object object class Rcpp_mgSEM","code":""},{"path":"/reference/AIC-Rcpp_mgSEM-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"AIC — AIC,Rcpp_mgSEM-method","text":"AIC values","code":""},{"path":"/reference/AIC-gpRegularized-method.html","id":null,"dir":"Reference","previous_headings":"","what":"AIC — AIC,gpRegularized-method","title":"AIC — AIC,gpRegularized-method","text":"returns AIC","code":""},{"path":"/reference/AIC-gpRegularized-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"AIC — AIC,gpRegularized-method","text":"","code":"# S4 method for gpRegularized AIC(object)"},{"path":"/reference/AIC-gpRegularized-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"AIC — AIC,gpRegularized-method","text":"object object class gpRegularized","code":""},{"path":"/reference/AIC-gpRegularized-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"AIC — AIC,gpRegularized-method","text":"data frame fit values, appended AIC","code":""},{"path":"/reference/AIC-regularizedSEM-method.html","id":null,"dir":"Reference","previous_headings":"","what":"AIC — AIC,regularizedSEM-method","title":"AIC — AIC,regularizedSEM-method","text":"returns AIC","code":""},{"path":"/reference/AIC-regularizedSEM-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"AIC — AIC,regularizedSEM-method","text":"","code":"# S4 method for regularizedSEM AIC(object)"},{"path":"/reference/AIC-regularizedSEM-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"AIC — AIC,regularizedSEM-method","text":"object object class regularizedSEM","code":""},{"path":"/reference/AIC-regularizedSEM-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"AIC — AIC,regularizedSEM-method","text":"AIC values","code":""},{"path":"/reference/AIC-regularizedSEMMixedPenalty-method.html","id":null,"dir":"Reference","previous_headings":"","what":"AIC — AIC,regularizedSEMMixedPenalty-method","title":"AIC — AIC,regularizedSEMMixedPenalty-method","text":"returns AIC","code":""},{"path":"/reference/AIC-regularizedSEMMixedPenalty-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"AIC — AIC,regularizedSEMMixedPenalty-method","text":"","code":"# S4 method for regularizedSEMMixedPenalty AIC(object)"},{"path":"/reference/AIC-regularizedSEMMixedPenalty-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"AIC — AIC,regularizedSEMMixedPenalty-method","text":"object object class regularizedSEMMixedPenalty","code":""},{"path":"/reference/AIC-regularizedSEMMixedPenalty-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"AIC — AIC,regularizedSEMMixedPenalty-method","text":"AIC values","code":""},{"path":"/reference/AIC-regularizedSEMWithCustomPenalty-method.html","id":null,"dir":"Reference","previous_headings":"","what":"AIC — AIC,regularizedSEMWithCustomPenalty-method","title":"AIC — AIC,regularizedSEMWithCustomPenalty-method","text":"returns AIC","code":""},{"path":"/reference/AIC-regularizedSEMWithCustomPenalty-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"AIC — AIC,regularizedSEMWithCustomPenalty-method","text":"","code":"# S4 method for regularizedSEMWithCustomPenalty AIC(object, penalizedParameterLabels, zeroThreshold)"},{"path":"/reference/AIC-regularizedSEMWithCustomPenalty-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"AIC — AIC,regularizedSEMWithCustomPenalty-method","text":"object object class regularizedSEMWithCustomPenalty penalizedParameterLabels vector labels penalized parameters zeroThreshold penalized parameters threshold counted zeroed","code":""},{"path":"/reference/AIC-regularizedSEMWithCustomPenalty-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"AIC — AIC,regularizedSEMWithCustomPenalty-method","text":"AIC values","code":""},{"path":"/reference/BIC-Rcpp_SEMCpp-method.html","id":null,"dir":"Reference","previous_headings":"","what":"BIC — BIC,Rcpp_SEMCpp-method","title":"BIC — BIC,Rcpp_SEMCpp-method","text":"BIC","code":""},{"path":"/reference/BIC-Rcpp_SEMCpp-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"BIC — BIC,Rcpp_SEMCpp-method","text":"","code":"# S4 method for Rcpp_SEMCpp BIC(object)"},{"path":"/reference/BIC-Rcpp_SEMCpp-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"BIC — BIC,Rcpp_SEMCpp-method","text":"object object class Rcpp_SEMCpp","code":""},{"path":"/reference/BIC-Rcpp_SEMCpp-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"BIC — BIC,Rcpp_SEMCpp-method","text":"BIC values","code":""},{"path":"/reference/BIC-Rcpp_mgSEM-method.html","id":null,"dir":"Reference","previous_headings":"","what":"BIC — BIC,Rcpp_mgSEM-method","title":"BIC — BIC,Rcpp_mgSEM-method","text":"BIC","code":""},{"path":"/reference/BIC-Rcpp_mgSEM-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"BIC — BIC,Rcpp_mgSEM-method","text":"","code":"# S4 method for Rcpp_mgSEM BIC(object)"},{"path":"/reference/BIC-Rcpp_mgSEM-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"BIC — BIC,Rcpp_mgSEM-method","text":"object object class Rcpp_mgSEM","code":""},{"path":"/reference/BIC-Rcpp_mgSEM-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"BIC — BIC,Rcpp_mgSEM-method","text":"BIC values","code":""},{"path":"/reference/BIC-gpRegularized-method.html","id":null,"dir":"Reference","previous_headings":"","what":"BIC — BIC,gpRegularized-method","title":"BIC — BIC,gpRegularized-method","text":"returns BIC","code":""},{"path":"/reference/BIC-gpRegularized-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"BIC — BIC,gpRegularized-method","text":"","code":"# S4 method for gpRegularized BIC(object)"},{"path":"/reference/BIC-gpRegularized-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"BIC — BIC,gpRegularized-method","text":"object object class gpRegularized","code":""},{"path":"/reference/BIC-gpRegularized-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"BIC — BIC,gpRegularized-method","text":"data frame fit values, appended BIC","code":""},{"path":"/reference/BIC-regularizedSEM-method.html","id":null,"dir":"Reference","previous_headings":"","what":"BIC — BIC,regularizedSEM-method","title":"BIC — BIC,regularizedSEM-method","text":"returns BIC","code":""},{"path":"/reference/BIC-regularizedSEM-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"BIC — BIC,regularizedSEM-method","text":"","code":"# S4 method for regularizedSEM BIC(object)"},{"path":"/reference/BIC-regularizedSEM-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"BIC — BIC,regularizedSEM-method","text":"object object class regularizedSEM","code":""},{"path":"/reference/BIC-regularizedSEM-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"BIC — BIC,regularizedSEM-method","text":"BIC values","code":""},{"path":"/reference/BIC-regularizedSEMMixedPenalty-method.html","id":null,"dir":"Reference","previous_headings":"","what":"BIC — BIC,regularizedSEMMixedPenalty-method","title":"BIC — BIC,regularizedSEMMixedPenalty-method","text":"returns BIC","code":""},{"path":"/reference/BIC-regularizedSEMMixedPenalty-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"BIC — BIC,regularizedSEMMixedPenalty-method","text":"","code":"# S4 method for regularizedSEMMixedPenalty BIC(object)"},{"path":"/reference/BIC-regularizedSEMMixedPenalty-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"BIC — BIC,regularizedSEMMixedPenalty-method","text":"object object class regularizedSEMMixedPenalty","code":""},{"path":"/reference/BIC-regularizedSEMMixedPenalty-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"BIC — BIC,regularizedSEMMixedPenalty-method","text":"BIC values","code":""},{"path":"/reference/BIC-regularizedSEMWithCustomPenalty-method.html","id":null,"dir":"Reference","previous_headings":"","what":"BIC — BIC,regularizedSEMWithCustomPenalty-method","title":"BIC — BIC,regularizedSEMWithCustomPenalty-method","text":"returns BIC","code":""},{"path":"/reference/BIC-regularizedSEMWithCustomPenalty-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"BIC — BIC,regularizedSEMWithCustomPenalty-method","text":"","code":"# S4 method for regularizedSEMWithCustomPenalty BIC(object, penalizedParameterLabels, zeroThreshold)"},{"path":"/reference/BIC-regularizedSEMWithCustomPenalty-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"BIC — BIC,regularizedSEMWithCustomPenalty-method","text":"object object class regularizedSEMWithCustomPenalty penalizedParameterLabels vector labels penalized parameters zeroThreshold penalized parameters threshold counted zeroed","code":""},{"path":"/reference/BIC-regularizedSEMWithCustomPenalty-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"BIC — BIC,regularizedSEMWithCustomPenalty-method","text":"BIC values","code":""},{"path":"/reference/GIC.html","id":null,"dir":"Reference","previous_headings":"","what":"GIC — GIC","title":"GIC — GIC","text":"FUNCTION DEVELOPMENT USED. computes generalized information criterion ","code":""},{"path":"/reference/GIC.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"GIC — GIC","text":"","code":"GIC(regularizedSEM, k = 2)"},{"path":"/reference/GIC.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"GIC — GIC","text":"regularizedSEM model class regularizedSEM k numeric value scale degrees freedom","code":""},{"path":"/reference/GIC.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"GIC — GIC","text":"vector GIC values.","code":""},{"path":"/reference/GIC.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"GIC — GIC","text":"-2-log-Likelihood + k*df k numeric value degrees freedom (df) multiplied. get equivalent AIC, use k = 2. equivalent BIC, set k = log(N) See Fan & Li (2001), p. 1355 Zhang et al. (2010), p. 314 details. Zhang, Y., Li, R., & Tsai, C.-L. (2010). Regularization Parameter Selections via Generalized Information Criterion. Journal American Statistical Association, 105(489), 312–323. https://doi.org/10.1198/jasa.2009.tm08013","code":""},{"path":"/reference/Rcpp_SEMCpp-class.html","id":null,"dir":"Reference","previous_headings":"","what":"internal representation of SEM in C++ — Rcpp_SEMCpp-class","title":"internal representation of SEM in C++ — Rcpp_SEMCpp-class","text":"internal representation SEM C++","code":""},{"path":"/reference/Rcpp_bfgsEnetMgSEM-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::bfgsEnetMgSEM — Rcpp_bfgsEnetMgSEM-class","title":"Wrapper for C++ module. See ?lessSEM::bfgsEnetMgSEM — Rcpp_bfgsEnetMgSEM-class","text":"Wrapper C++ module. See ?lessSEM::bfgsEnetMgSEM","code":""},{"path":"/reference/Rcpp_bfgsEnetSEM-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::bfgsEnetSEM — Rcpp_bfgsEnetSEM-class","title":"Wrapper for C++ module. See ?lessSEM::bfgsEnetSEM — Rcpp_bfgsEnetSEM-class","text":"Wrapper C++ module. See ?lessSEM::bfgsEnetSEM","code":""},{"path":"/reference/Rcpp_glmnetEnetGeneralPurpose-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::glmnetEnetGeneralPurpose — Rcpp_glmnetEnetGeneralPurpose-class","title":"Wrapper for C++ module. See ?lessSEM::glmnetEnetGeneralPurpose — Rcpp_glmnetEnetGeneralPurpose-class","text":"Wrapper C++ module. See ?lessSEM::glmnetEnetGeneralPurpose","code":""},{"path":"/reference/Rcpp_glmnetEnetGeneralPurposeCpp-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::glmnetEnetGeneralPurposeCpp — Rcpp_glmnetEnetGeneralPurposeCpp-class","title":"Wrapper for C++ module. See ?lessSEM::glmnetEnetGeneralPurposeCpp — Rcpp_glmnetEnetGeneralPurposeCpp-class","text":"Wrapper C++ module. See ?lessSEM::glmnetEnetGeneralPurposeCpp","code":""},{"path":"/reference/Rcpp_glmnetEnetMgSEM-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::glmnetEnetMgSEM — Rcpp_glmnetEnetMgSEM-class","title":"Wrapper for C++ module. See ?lessSEM::glmnetEnetMgSEM — Rcpp_glmnetEnetMgSEM-class","text":"Wrapper C++ module. See ?lessSEM::glmnetEnetMgSEM","code":""},{"path":"/reference/Rcpp_glmnetEnetSEM-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::glmnetEnetSEM — Rcpp_glmnetEnetSEM-class","title":"Wrapper for C++ module. See ?lessSEM::glmnetEnetSEM — Rcpp_glmnetEnetSEM-class","text":"Wrapper C++ module. See ?lessSEM::glmnetEnetSEM","code":""},{"path":"/reference/Rcpp_istaCappedL1GeneralPurpose-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::istaCappedL1GeneralPurpose — Rcpp_istaCappedL1GeneralPurpose-class","title":"Wrapper for C++ module. See ?lessSEM::istaCappedL1GeneralPurpose — Rcpp_istaCappedL1GeneralPurpose-class","text":"Wrapper C++ module. See ?lessSEM::istaCappedL1GeneralPurpose","code":""},{"path":"/reference/Rcpp_istaCappedL1GeneralPurposeCpp-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::istaCappedL1GeneralPurposeCpp — Rcpp_istaCappedL1GeneralPurposeCpp-class","title":"Wrapper for C++ module. See ?lessSEM::istaCappedL1GeneralPurposeCpp — Rcpp_istaCappedL1GeneralPurposeCpp-class","text":"Wrapper C++ module. See ?lessSEM::istaCappedL1GeneralPurposeCpp","code":""},{"path":"/reference/Rcpp_istaCappedL1SEM-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::istaCappedL1SEM — Rcpp_istaCappedL1SEM-class","title":"Wrapper for C++ module. See ?lessSEM::istaCappedL1SEM — Rcpp_istaCappedL1SEM-class","text":"Wrapper C++ module. See ?lessSEM::istaCappedL1SEM","code":""},{"path":"/reference/Rcpp_istaCappedL1mgSEM-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::istaCappedL1MgSEM — Rcpp_istaCappedL1mgSEM-class","title":"Wrapper for C++ module. See ?lessSEM::istaCappedL1MgSEM — Rcpp_istaCappedL1mgSEM-class","text":"Wrapper C++ module. See ?lessSEM::istaCappedL1MgSEM","code":""},{"path":"/reference/Rcpp_istaEnetGeneralPurpose-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::istaEnetGeneralPurpose — Rcpp_istaEnetGeneralPurpose-class","title":"Wrapper for C++ module. See ?lessSEM::istaEnetGeneralPurpose — Rcpp_istaEnetGeneralPurpose-class","text":"Wrapper C++ module. See ?lessSEM::istaEnetGeneralPurpose","code":""},{"path":"/reference/Rcpp_istaEnetGeneralPurposeCpp-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::istaEnetGeneralPurposeCpp — Rcpp_istaEnetGeneralPurposeCpp-class","title":"Wrapper for C++ module. See ?lessSEM::istaEnetGeneralPurposeCpp — Rcpp_istaEnetGeneralPurposeCpp-class","text":"Wrapper C++ module. See ?lessSEM::istaEnetGeneralPurposeCpp","code":""},{"path":"/reference/Rcpp_istaEnetMgSEM-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::istaEnetMgSEM — Rcpp_istaEnetMgSEM-class","title":"Wrapper for C++ module. See ?lessSEM::istaEnetMgSEM — Rcpp_istaEnetMgSEM-class","text":"Wrapper C++ module. See ?lessSEM::istaEnetMgSEM Wrapper C++ module. See ?lessSEM::istaEnetMgSEM","code":""},{"path":"/reference/Rcpp_istaEnetSEM-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::istaEnetSEM — Rcpp_istaEnetSEM-class","title":"Wrapper for C++ module. See ?lessSEM::istaEnetSEM — Rcpp_istaEnetSEM-class","text":"Wrapper C++ module. See ?lessSEM::istaEnetSEM Wrapper C++ module. See ?lessSEM::istaEnetSEM","code":""},{"path":"/reference/Rcpp_istaLSPMgSEM-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::istaLSPMgSEM — Rcpp_istaLSPMgSEM-class","title":"Wrapper for C++ module. See ?lessSEM::istaLSPMgSEM — Rcpp_istaLSPMgSEM-class","text":"Wrapper C++ module. See ?lessSEM::istaLSPMgSEM","code":""},{"path":"/reference/Rcpp_istaLSPSEM-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::istaLSPSEM — Rcpp_istaLSPSEM-class","title":"Wrapper for C++ module. See ?lessSEM::istaLSPSEM — Rcpp_istaLSPSEM-class","text":"Wrapper C++ module. See ?lessSEM::istaLSPSEM","code":""},{"path":"/reference/Rcpp_istaLspGeneralPurpose-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::istaLspGeneralPurpose — Rcpp_istaLspGeneralPurpose-class","title":"Wrapper for C++ module. See ?lessSEM::istaLspGeneralPurpose — Rcpp_istaLspGeneralPurpose-class","text":"Wrapper C++ module. See ?lessSEM::istaLspGeneralPurpose","code":""},{"path":"/reference/Rcpp_istaLspGeneralPurposeCpp-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::istaLspGeneralPurposeCpp — Rcpp_istaLspGeneralPurposeCpp-class","title":"Wrapper for C++ module. See ?lessSEM::istaLspGeneralPurposeCpp — Rcpp_istaLspGeneralPurposeCpp-class","text":"Wrapper C++ module. See ?lessSEM::istaLspGeneralPurposeCpp","code":""},{"path":"/reference/Rcpp_istaMcpGeneralPurpose-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::istaMcpGeneralPurpose — Rcpp_istaMcpGeneralPurpose-class","title":"Wrapper for C++ module. See ?lessSEM::istaMcpGeneralPurpose — Rcpp_istaMcpGeneralPurpose-class","text":"Wrapper C++ module. See ?lessSEM::istaMcpGeneralPurpose","code":""},{"path":"/reference/Rcpp_istaMcpGeneralPurposeCpp-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::istaMcpGeneralPurposeCpp — Rcpp_istaMcpGeneralPurposeCpp-class","title":"Wrapper for C++ module. See ?lessSEM::istaMcpGeneralPurposeCpp — Rcpp_istaMcpGeneralPurposeCpp-class","text":"Wrapper C++ module. See ?lessSEM::istaMcpGeneralPurposeCpp","code":""},{"path":"/reference/Rcpp_istaMcpMgSEM-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::istaMcpMgSEM — Rcpp_istaMcpMgSEM-class","title":"Wrapper for C++ module. See ?lessSEM::istaMcpMgSEM — Rcpp_istaMcpMgSEM-class","text":"Wrapper C++ module. See ?lessSEM::istaMcpMgSEM","code":""},{"path":"/reference/Rcpp_istaMcpSEM-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::istaMcpSEM — Rcpp_istaMcpSEM-class","title":"Wrapper for C++ module. See ?lessSEM::istaMcpSEM — Rcpp_istaMcpSEM-class","text":"Wrapper C++ module. See ?lessSEM::istaMcpSEM","code":""},{"path":"/reference/Rcpp_istaMixedPenaltySEM-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::istaMixedPenaltySEM — Rcpp_istaMixedPenaltySEM-class","title":"Wrapper for C++ module. See ?lessSEM::istaMixedPenaltySEM — Rcpp_istaMixedPenaltySEM-class","text":"Wrapper C++ module. See ?lessSEM::istaMixedPenaltySEM","code":""},{"path":"/reference/Rcpp_istaMixedPenaltymgSEM-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::istaMixedPenaltymgSEM — Rcpp_istaMixedPenaltymgSEM-class","title":"Wrapper for C++ module. See ?lessSEM::istaMixedPenaltymgSEM — Rcpp_istaMixedPenaltymgSEM-class","text":"Wrapper C++ module. See ?lessSEM::istaMixedPenaltymgSEM","code":""},{"path":"/reference/Rcpp_istaScadGeneralPurpose-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::istaScadGeneralPurpose — Rcpp_istaScadGeneralPurpose-class","title":"Wrapper for C++ module. See ?lessSEM::istaScadGeneralPurpose — Rcpp_istaScadGeneralPurpose-class","text":"Wrapper C++ module. See ?lessSEM::istaScadGeneralPurpose","code":""},{"path":"/reference/Rcpp_istaScadGeneralPurposeCpp-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::istaScadGeneralPurposeCpp — Rcpp_istaScadGeneralPurposeCpp-class","title":"Wrapper for C++ module. See ?lessSEM::istaScadGeneralPurposeCpp — Rcpp_istaScadGeneralPurposeCpp-class","text":"Wrapper C++ module. See ?lessSEM::istaScadGeneralPurposeCpp","code":""},{"path":"/reference/Rcpp_istaScadMgSEM-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::istaScadMgSEM — Rcpp_istaScadMgSEM-class","title":"Wrapper for C++ module. See ?lessSEM::istaScadMgSEM — Rcpp_istaScadMgSEM-class","text":"Wrapper C++ module. See ?lessSEM::istaScadMgSEM","code":""},{"path":"/reference/Rcpp_istaScadSEM-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::istaScadSEM — Rcpp_istaScadSEM-class","title":"Wrapper for C++ module. See ?lessSEM::istaScadSEM — Rcpp_istaScadSEM-class","text":"Wrapper C++ module. See ?lessSEM::istaScadSEM","code":""},{"path":"/reference/Rcpp_mgSEM-class.html","id":null,"dir":"Reference","previous_headings":"","what":"internal representation of SEM in C++ — Rcpp_mgSEM-class","title":"internal representation of SEM in C++ — Rcpp_mgSEM-class","text":"internal representation SEM C++","code":""},{"path":"/reference/SEMCpp.html","id":null,"dir":"Reference","previous_headings":"","what":"SEMCpp class — SEMCpp","title":"SEMCpp class — SEMCpp","text":"internal SEM representation","code":""},{"path":"/reference/SEMCpp.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"SEMCpp class — SEMCpp","text":"new Creates new SEMCpp. fill fills SEM elements Rcpp::List addTransformation adds transforamtions model implied Computes implied means covariance matrix fit Fits model. Returns -2 log likelihood getParameters Returns data frame model parameters. getParameterLabels Returns vector unique parameter labels used internally. getGradients Returns matrix scores. getScores Returns matrix scores. getHessian Returns hessian model. Expects labels parameters values parameters well boolean indicating raw. Finally, double (eps) controls precision approximation. computeTransformations compute transformations. setTransformationGradientStepSize change step size gradient computation transformations","code":""},{"path":"/reference/adaptiveLasso.html","id":null,"dir":"Reference","previous_headings":"","what":"adaptiveLasso — adaptiveLasso","title":"adaptiveLasso — adaptiveLasso","text":"Implements adaptive lasso regularization structural equation models. penalty function given : $$p( x_j) = p( x_j) = \\frac{1}{w_j}\\lambda| x_j|$$ Adaptive lasso regularization set parameters zero \\(\\lambda\\) large enough.","code":""},{"path":"/reference/adaptiveLasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"adaptiveLasso — adaptiveLasso","text":"","code":"adaptiveLasso(   lavaanModel,   regularized,   weights = NULL,   lambdas = NULL,   nLambdas = NULL,   reverse = TRUE,   curve = 1,   method = \"glmnet\",   modifyModel = lessSEM::modifyModel(),   control = lessSEM::controlGlmnet() )"},{"path":"/reference/adaptiveLasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"adaptiveLasso — adaptiveLasso","text":"lavaanModel model class lavaan regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object weights labeled vector weights parameters model. unsure parameters called, use getLavaanParameters(model) lavaan model object. set NULL, default weights used: inverse absolute values unregularized parameter estimates lambdas numeric vector: values tuning parameter lambda nLambdas alternative lambda: alpha = 1, lessSEM can automatically compute first lambda value sets regularized parameters zero. generate nLambda values 0 computed lambda. reverse set TRUE nLambdas used, lessSEM start largest lambda gradually decrease lambda. Otherwise, lessSEM start smallest lambda gradually increase . curve Allows unequally spaced lambda steps (e.g., .01,.02,.05,1,5,20). curve close 1 lambda values equally spaced, curve large lambda values concentrated close 0. See ?lessSEM::curveLambda information. method optimizer used? Currently implemented ista glmnet. ista, control argument can used switch related procedures (currently gist). modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. element generated controlIsta controlGlmnet functions. See ?controlIsta ?controlGlmnet details.","code":""},{"path":"/reference/adaptiveLasso.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"adaptiveLasso — adaptiveLasso","text":"Model class regularizedSEM","code":""},{"path":"/reference/adaptiveLasso.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"adaptiveLasso — adaptiveLasso","text":"Identical regsem, models specified using lavaan. Currently, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. Adaptive lasso regularization: Zou, H. (2006). adaptive lasso oracle properties. Journal American Statistical Association, 101(476), 1418–1429. https://doi.org/10.1198/016214506000000735 Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classiﬁcation. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/adaptiveLasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"adaptiveLasso — adaptiveLasso","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +      l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +      l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Optional: Plot the model # semPlot::semPaths(lavaanModel, #                   what = \"est\", #                   fade = FALSE)  lsem <- adaptiveLasso(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   # names of the regularized parameters:   regularized = paste0(\"l\", 6:15),   # in case of lasso and adaptive lasso, we can specify the number of lambda   # values to use. lessSEM will automatically find lambda_max and fit   # models for nLambda values between 0 and lambda_max. For the other   # penalty functions, lambdas must be specified explicitly   nLambdas = 50)  # use the plot-function to plot the regularized parameters: plot(lsem)  # the coefficients can be accessed with: coef(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters[1,]  # AIC and BIC: AIC(lsem) BIC(lsem)  # The best parameters can also be extracted with: coef(lsem, criterion = \"AIC\") coef(lsem, criterion = \"BIC\")   #### Advanced ### # Switching the optimizer # # Use the \"method\" argument to switch the optimizer. The control argument # must also be changed to the corresponding function: lsemIsta <- adaptiveLasso(   lavaanModel = lavaanModel,   regularized = paste0(\"l\", 6:15),   nLambdas = 50,   method = \"ista\",   control = controlIsta())  # Note: The results are basically identical: lsemIsta@parameters - lsem@parameters"},{"path":"/reference/addCappedL1.html","id":null,"dir":"Reference","previous_headings":"","what":"addCappedL1 — addCappedL1","title":"addCappedL1 — addCappedL1","text":"Implements cappedL1 regularization structural equation models. penalty function given : $$p( x_j) = \\lambda \\min(| x_j|, \\theta)$$ \\(\\theta > 0\\). cappedL1 penalty identical lasso parameters \\(\\theta\\) identical constant parameters \\(\\theta\\). adding constant fitting function change minimum, larger parameters can stay unregularized smaller ones set zero.","code":""},{"path":"/reference/addCappedL1.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"addCappedL1 — addCappedL1","text":"","code":"addCappedL1(mixedPenalty, regularized, lambdas, thetas)"},{"path":"/reference/addCappedL1.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"addCappedL1 — addCappedL1","text":"mixedPenalty model class mixedPenalty created mixedPenalty function (see ?mixedPenalty) regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda thetas parameters whose absolute value threshold penalized constant (theta)","code":""},{"path":"/reference/addCappedL1.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"addCappedL1 — addCappedL1","text":"Model class mixedPenalty. Use fit() - function fit model","code":""},{"path":"/reference/addCappedL1.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"addCappedL1 — addCappedL1","text":"Identical regsem, models specified using lavaan. Currently, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. CappedL1 regularization: Zhang, T. (2010). Analysis Multi-stage Convex Relaxation Sparse Regularization. Journal Machine Learning Research, 11, 1081–1107. Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/addElasticNet.html","id":null,"dir":"Reference","previous_headings":"","what":"addElasticNet — addElasticNet","title":"addElasticNet — addElasticNet","text":"Adds elastic net penalty specified parameters. penalty function given : $$p( x_j) = \\alpha\\lambda|x_j| + (1-\\alpha)\\lambda x_j^2$$ Note elastic net combines ridge lasso regularization. \\(\\alpha = 0\\), elastic net reduces ridge regularization. \\(\\alpha = 1\\) reduces lasso regularization. , elastic net compromise shrinkage lasso ridge penalty.","code":""},{"path":"/reference/addElasticNet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"addElasticNet — addElasticNet","text":"","code":"addElasticNet(mixedPenalty, regularized, alphas, lambdas, weights = 1)"},{"path":"/reference/addElasticNet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"addElasticNet — addElasticNet","text":"mixedPenalty model class mixedPenalty created mixedPenalty function (see ?mixedPenalty) regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object alphas numeric vector: values tuning parameter alpha. Set 1 lasso zero ridge. Anything elastic net penalty. lambdas numeric vector: values tuning parameter lambda weights can used give different weights different parameters","code":""},{"path":"/reference/addElasticNet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"addElasticNet — addElasticNet","text":"Model class mixedPenalty. Use fit() - function fit model","code":""},{"path":"/reference/addElasticNet.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"addElasticNet — addElasticNet","text":"Identical regsem, models specified using lavaan. Currently, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. Elastic net regularization: Zou, H., & Hastie, T. (2005). Regularization variable selection via elastic net. Journal Royal Statistical Society: Series B, 67(2), 301–320. https://doi.org/10.1111/j.1467-9868.2005.00503.x Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classiﬁcation. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/addLasso.html","id":null,"dir":"Reference","previous_headings":"","what":"addLasso — addLasso","title":"addLasso — addLasso","text":"Implements lasso regularization structural equation models. penalty function given : $$p( x_j) = \\lambda |x_j|$$ Lasso regularization set parameters zero \\(\\lambda\\) large enough","code":""},{"path":"/reference/addLasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"addLasso — addLasso","text":"","code":"addLasso(mixedPenalty, regularized, weights = 1, lambdas)"},{"path":"/reference/addLasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"addLasso — addLasso","text":"mixedPenalty model class mixedPenalty created mixedPenalty function (see ?mixedPenalty) regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object weights can used give different weights different parameters lambdas numeric vector: values tuning parameter lambda","code":""},{"path":"/reference/addLasso.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"addLasso — addLasso","text":"Model class mixedPenalty. Use fit() - function fit model","code":""},{"path":"/reference/addLasso.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"addLasso — addLasso","text":"Identical regsem, models specified using lavaan. Currently, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. Lasso regularization: Tibshirani, R. (1996). Regression shrinkage selection via lasso. Journal Royal Statistical Society. Series B (Methodological), 58(1), 267–288. Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classiﬁcation. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/addLsp.html","id":null,"dir":"Reference","previous_headings":"","what":"addLsp — addLsp","title":"addLsp — addLsp","text":"Implements lsp regularization structural equation models. penalty function given : $$p( x_j) = \\lambda \\log(1 + |x_j|/\\theta)$$ \\(\\theta > 0\\).","code":""},{"path":"/reference/addLsp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"addLsp — addLsp","text":"","code":"addLsp(mixedPenalty, regularized, lambdas, thetas)"},{"path":"/reference/addLsp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"addLsp — addLsp","text":"mixedPenalty model class mixedPenalty created mixedPenalty function (see ?mixedPenalty) regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda thetas parameters whose absolute value threshold penalized constant (theta)","code":""},{"path":"/reference/addLsp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"addLsp — addLsp","text":"Model class mixedPenalty. Use fit() - function fit model","code":""},{"path":"/reference/addLsp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"addLsp — addLsp","text":"Identical regsem, models specified using lavaan. Currently, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. lsp regularization: Candès, E. J., Wakin, M. B., & Boyd, S. P. (2008). Enhancing Sparsity Reweighted l1 Minimization. Journal Fourier Analysis Applications, 14(5–6), 877–905. https://doi.org/10.1007/s00041-008-9045-x Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classiﬁcation. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/addMcp.html","id":null,"dir":"Reference","previous_headings":"","what":"addMcp — addMcp","title":"addMcp — addMcp","text":"Implements mcp regularization structural equation models. penalty function given : $$p( x_j) = \\begin{cases} \\lambda |x_j| - x_j^2/(2\\theta) & \\text{} |x_j| \\leq \\theta\\lambda\\\\ \\theta\\lambda^2/2 & \\text{} |x_j| > \\lambda\\theta \\end{cases}$$ \\(\\theta > 0\\).","code":""},{"path":"/reference/addMcp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"addMcp — addMcp","text":"","code":"addMcp(mixedPenalty, regularized, lambdas, thetas)"},{"path":"/reference/addMcp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"addMcp — addMcp","text":"mixedPenalty model class mixedPenalty created mixedPenalty function (see ?mixedPenalty) regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda thetas parameters whose absolute value threshold penalized constant (theta)","code":""},{"path":"/reference/addMcp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"addMcp — addMcp","text":"Model class mixedPenalty. Use fit() - function fit model","code":""},{"path":"/reference/addMcp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"addMcp — addMcp","text":"Identical regsem, models specified using lavaan. Currently, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. mcp regularization: Zhang, C.-H. (2010). Nearly unbiased variable selection minimax concave penalty. Annals Statistics, 38(2), 894–942. https://doi.org/10.1214/09-AOS729 Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/addRidge.html","id":null,"dir":"Reference","previous_headings":"","what":"addRidge — addRidge","title":"addRidge — addRidge","text":"Implements ridge regularization structural equation models. penalty function given : $$p( x_j) = \\lambda x_j^2$$ Note ridge regularization set parameters zero result shrinkage towards zero.","code":""},{"path":"/reference/addRidge.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"addRidge — addRidge","text":"","code":"addRidge(mixedPenalty, regularized, weights = 1, lambdas)"},{"path":"/reference/addRidge.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"addRidge — addRidge","text":"mixedPenalty model class mixedPenalty created mixedPenalty function (see ?mixedPenalty) regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object weights can used give different weights different parameters lambdas numeric vector: values tuning parameter lambda","code":""},{"path":"/reference/addRidge.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"addRidge — addRidge","text":"Model class mixedPenalty. Use fit() - function fit model","code":""},{"path":"/reference/addRidge.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"addRidge — addRidge","text":"Identical regsem, models specified using lavaan. Currently, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. Ridge regularization: Hoerl, . E., & Kennard, R. W. (1970). Ridge Regression: Biased Estimation Nonorthogonal Problems. Technometrics, 12(1), 55–67. https://doi.org/10.1080/00401706.1970.10488634 Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classiﬁcation. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/addScad.html","id":null,"dir":"Reference","previous_headings":"","what":"addScad — addScad","title":"addScad — addScad","text":"Implements scad regularization structural equation models. penalty function given :  $$p( x_j) = \\begin{cases} \\lambda |x_j| & \\text{} |x_j| \\leq \\theta\\\\ \\frac{-x_j^2 + 2\\theta\\lambda |x_j| - \\lambda^2}{2(\\theta -1)} &  \\text{} \\lambda < |x_j| \\leq \\lambda\\theta \\\\ (\\theta + 1) \\lambda^2/2 & \\text{} |x_j| \\geq \\theta\\lambda\\\\ \\end{cases}$$ \\(\\theta > 2\\).","code":""},{"path":"/reference/addScad.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"addScad — addScad","text":"","code":"addScad(mixedPenalty, regularized, lambdas, thetas)"},{"path":"/reference/addScad.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"addScad — addScad","text":"mixedPenalty model class mixedPenalty created mixedPenalty function (see ?mixedPenalty) regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda thetas parameters whose absolute value threshold penalized constant (theta)","code":""},{"path":"/reference/addScad.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"addScad — addScad","text":"Model class mixedPenalty. Use fit() - function fit model","code":""},{"path":"/reference/addScad.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"addScad — addScad","text":"Identical regsem, models specified using lavaan. Currently, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. scad regularization: Fan, J., & Li, R. (2001). Variable selection via nonconcave penalized likelihood oracle properties. Journal American Statistical Association, 96(456), 1348–1360. https://doi.org/10.1198/016214501753382273 Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/bfgs.html","id":null,"dir":"Reference","previous_headings":"","what":"bfgs — bfgs","title":"bfgs — bfgs","text":"function allows optimizing models built lavaan using BFGS optimizer implemented lessSEM. elements can accessed \"@\" operator (see examples). main purpose make transformations lavaan models accessible.","code":""},{"path":"/reference/bfgs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"bfgs — bfgs","text":"","code":"bfgs(   lavaanModel,   modifyModel = lessSEM::modifyModel(),   control = lessSEM::controlBFGS() )"},{"path":"/reference/bfgs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"bfgs — bfgs","text":"lavaanModel model class lavaan modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. See ?controlBFGS details.","code":""},{"path":"/reference/bfgs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"bfgs — bfgs","text":"Model class regularizedSEM","code":""},{"path":"/reference/bfgs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"bfgs — bfgs","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +      l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +      l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)   lsem <- bfgs(   # pass the fitted lavaan model   lavaanModel = lavaanModel)  # the coefficients can be accessed with: coef(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters"},{"path":"/reference/bfgsEnet.html","id":null,"dir":"Reference","previous_headings":"","what":"smoothly approximated elastic net — bfgsEnet","title":"smoothly approximated elastic net — bfgsEnet","text":"Object smoothly approximated elastic net optimization bfgs optimizer","code":""},{"path":"/reference/bfgsEnet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"smoothly approximated elastic net — bfgsEnet","text":"list fit results","code":""},{"path":"/reference/bfgsEnet.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"smoothly approximated elastic net — bfgsEnet","text":"new creates new object. Requires (1) vector weights parameter (2) list control elements setHessian changes Hessian model. Expects matrix optimize optimize model. Expects vector starting values, SEM type SEM_Cpp, lambda alpha value.","code":""},{"path":"/reference/bfgsEnetMgSEM.html","id":null,"dir":"Reference","previous_headings":"","what":"smoothly approximated elastic net — bfgsEnetMgSEM","title":"smoothly approximated elastic net — bfgsEnetMgSEM","text":"Object smoothly approximated elastic net optimization bfgs optimizer","code":""},{"path":"/reference/bfgsEnetMgSEM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"smoothly approximated elastic net — bfgsEnetMgSEM","text":"list fit results","code":""},{"path":"/reference/bfgsEnetMgSEM.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"smoothly approximated elastic net — bfgsEnetMgSEM","text":"new creates new object. Requires (1) vector weights parameter (2) list control elements setHessian changes Hessian model. Expects matrix optimize optimize model. Expects vector starting values, SEM type SEM_Cpp, lambda alpha value.","code":""},{"path":"/reference/bfgsEnetSEM.html","id":null,"dir":"Reference","previous_headings":"","what":"smoothly approximated elastic net — bfgsEnetSEM","title":"smoothly approximated elastic net — bfgsEnetSEM","text":"Object smoothly approximated elastic net optimization bfgs optimizer","code":""},{"path":"/reference/bfgsEnetSEM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"smoothly approximated elastic net — bfgsEnetSEM","text":"list fit results","code":""},{"path":"/reference/bfgsEnetSEM.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"smoothly approximated elastic net — bfgsEnetSEM","text":"new creates new object. Requires (1) vector weights parameter (2) list control elements setHessian changes Hessian model. Expects matrix optimize optimize model. Expects vector starting values, SEM type SEM_Cpp, lambda alpha value.","code":""},{"path":"/reference/callFitFunction.html","id":null,"dir":"Reference","previous_headings":"","what":"callFitFunction — callFitFunction","title":"callFitFunction — callFitFunction","text":"wrapper call user defined fit function","code":""},{"path":"/reference/callFitFunction.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"callFitFunction — callFitFunction","text":"","code":"callFitFunction(fitFunctionSEXP, parameters, userSuppliedElements)"},{"path":"/reference/callFitFunction.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"callFitFunction — callFitFunction","text":"fitFunctionSEXP pointer fit function parameters vector parameter values userSuppliedElements list additional elements","code":""},{"path":"/reference/callFitFunction.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"callFitFunction — callFitFunction","text":"fit value (double)","code":""},{"path":"/reference/cappedL1.html","id":null,"dir":"Reference","previous_headings":"","what":"cappedL1 — cappedL1","title":"cappedL1 — cappedL1","text":"Implements cappedL1 regularization structural equation models. penalty function given : $$p( x_j) = \\lambda \\min(| x_j|, \\theta)$$ \\(\\theta > 0\\). cappedL1 penalty identical lasso parameters \\(\\theta\\) identical constant parameters \\(\\theta\\). adding constant fitting function change minimum, larger parameters can stay unregularized smaller ones set zero.","code":""},{"path":"/reference/cappedL1.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"cappedL1 — cappedL1","text":"","code":"cappedL1(   lavaanModel,   regularized,   lambdas,   thetas,   modifyModel = lessSEM::modifyModel(),   control = lessSEM::controlIsta() )"},{"path":"/reference/cappedL1.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"cappedL1 — cappedL1","text":"lavaanModel model class lavaan regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda thetas parameters whose absolute value threshold penalized constant (theta) modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. element generated controlIsta (see ?controlIsta)","code":""},{"path":"/reference/cappedL1.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"cappedL1 — cappedL1","text":"Model class regularizedSEM","code":""},{"path":"/reference/cappedL1.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"cappedL1 — cappedL1","text":"Identical regsem, models specified using lavaan. Currently, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. CappedL1 regularization: Zhang, T. (2010). Analysis Multi-stage Convex Relaxation Sparse Regularization. Journal Machine Learning Research, 11, 1081–1107. Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classiﬁcation. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/cappedL1.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"cappedL1 — cappedL1","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +      l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +      l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Optional: Plot the model # semPlot::semPaths(lavaanModel, #                   what = \"est\", #                   fade = FALSE)  lsem <- cappedL1(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   # names of the regularized parameters:   regularized = paste0(\"l\", 6:15),   lambdas = seq(0,1,length.out = 20),   thetas = seq(0.01,2,length.out = 5))  # the coefficients can be accessed with: coef(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters[1,]  # optional: plotting the paths requires installation of plotly # plot(lsem)"},{"path":"/reference/coef-Rcpp_SEMCpp-method.html","id":null,"dir":"Reference","previous_headings":"","what":"coef — coef,Rcpp_SEMCpp-method","title":"coef — coef,Rcpp_SEMCpp-method","text":"coef","code":""},{"path":"/reference/coef-Rcpp_SEMCpp-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"coef — coef,Rcpp_SEMCpp-method","text":"","code":"# S4 method for Rcpp_SEMCpp coef(object, ...)"},{"path":"/reference/coef-Rcpp_SEMCpp-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"coef — coef,Rcpp_SEMCpp-method","text":"object object class Rcpp_SEMCpp ... used","code":""},{"path":"/reference/coef-Rcpp_SEMCpp-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"coef — coef,Rcpp_SEMCpp-method","text":"coefficients model transformed form","code":""},{"path":"/reference/coef-Rcpp_mgSEM-method.html","id":null,"dir":"Reference","previous_headings":"","what":"coef — coef,Rcpp_mgSEM-method","title":"coef — coef,Rcpp_mgSEM-method","text":"coef","code":""},{"path":"/reference/coef-Rcpp_mgSEM-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"coef — coef,Rcpp_mgSEM-method","text":"","code":"# S4 method for Rcpp_mgSEM coef(object, ...)"},{"path":"/reference/coef-Rcpp_mgSEM-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"coef — coef,Rcpp_mgSEM-method","text":"object object class Rcpp_mgSEM ... used","code":""},{"path":"/reference/coef-Rcpp_mgSEM-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"coef — coef,Rcpp_mgSEM-method","text":"coefficients model transformed form","code":""},{"path":"/reference/coef-cvRegularizedSEM-method.html","id":null,"dir":"Reference","previous_headings":"","what":"coef — coef,cvRegularizedSEM-method","title":"coef — coef,cvRegularizedSEM-method","text":"Returns parameter estimates cvRegularizedSEM","code":""},{"path":"/reference/coef-cvRegularizedSEM-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"coef — coef,cvRegularizedSEM-method","text":"","code":"# S4 method for cvRegularizedSEM coef(object, ...)"},{"path":"/reference/coef-cvRegularizedSEM-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"coef — coef,cvRegularizedSEM-method","text":"object object class cvRegularizedSEM ... used","code":""},{"path":"/reference/coef-cvRegularizedSEM-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"coef — coef,cvRegularizedSEM-method","text":"parameter estimates cvRegularizedSEM","code":""},{"path":"/reference/coef-gpRegularized-method.html","id":null,"dir":"Reference","previous_headings":"","what":"coef — coef,gpRegularized-method","title":"coef — coef,gpRegularized-method","text":"Returns parameter estimates gpRegularized","code":""},{"path":"/reference/coef-gpRegularized-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"coef — coef,gpRegularized-method","text":"","code":"# S4 method for gpRegularized coef(object, ...)"},{"path":"/reference/coef-gpRegularized-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"coef — coef,gpRegularized-method","text":"object object class gpRegularized ... criterion can one : \"AIC\", \"BIC\". set NULL, parameters returned","code":""},{"path":"/reference/coef-gpRegularized-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"coef — coef,gpRegularized-method","text":"parameter estimates","code":""},{"path":"/reference/coef-regularizedSEM-method.html","id":null,"dir":"Reference","previous_headings":"","what":"coef — coef,regularizedSEM-method","title":"coef — coef,regularizedSEM-method","text":"Returns parameter estimates regularizedSEM","code":""},{"path":"/reference/coef-regularizedSEM-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"coef — coef,regularizedSEM-method","text":"","code":"# S4 method for regularizedSEM coef(object, ...)"},{"path":"/reference/coef-regularizedSEM-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"coef — coef,regularizedSEM-method","text":"object object class regularizedSEM ... criterion can one : \"AIC\", \"BIC\". set NULL, parameters returned","code":""},{"path":"/reference/coef-regularizedSEM-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"coef — coef,regularizedSEM-method","text":"parameters model data.frame","code":""},{"path":"/reference/coef-regularizedSEMMixedPenalty-method.html","id":null,"dir":"Reference","previous_headings":"","what":"coef — coef,regularizedSEMMixedPenalty-method","title":"coef — coef,regularizedSEMMixedPenalty-method","text":"Returns parameter estimates regularizedSEMMixedPenalty","code":""},{"path":"/reference/coef-regularizedSEMMixedPenalty-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"coef — coef,regularizedSEMMixedPenalty-method","text":"","code":"# S4 method for regularizedSEMMixedPenalty coef(object, ...)"},{"path":"/reference/coef-regularizedSEMMixedPenalty-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"coef — coef,regularizedSEMMixedPenalty-method","text":"object object class regularizedSEMMixedPenalty ... criterion can one : \"AIC\", \"BIC\". set NULL, parameters returned","code":""},{"path":"/reference/coef-regularizedSEMMixedPenalty-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"coef — coef,regularizedSEMMixedPenalty-method","text":"parameters model data.frame","code":""},{"path":"/reference/coef-regularizedSEMWithCustomPenalty-method.html","id":null,"dir":"Reference","previous_headings":"","what":"coef — coef,regularizedSEMWithCustomPenalty-method","title":"coef — coef,regularizedSEMWithCustomPenalty-method","text":"Returns parameter estimates regularizedSEMWithCustomPenalty","code":""},{"path":"/reference/coef-regularizedSEMWithCustomPenalty-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"coef — coef,regularizedSEMWithCustomPenalty-method","text":"","code":"# S4 method for regularizedSEMWithCustomPenalty coef(object, ...)"},{"path":"/reference/coef-regularizedSEMWithCustomPenalty-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"coef — coef,regularizedSEMWithCustomPenalty-method","text":"object object class regularizedSEMWithCustomPenalty ... used","code":""},{"path":"/reference/coef-regularizedSEMWithCustomPenalty-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"coef — coef,regularizedSEMWithCustomPenalty-method","text":"data.frame parameter estimates","code":""},{"path":"/reference/computeImpliedCovariance.html","id":null,"dir":"Reference","previous_headings":"","what":"computeImpliedCovariance — computeImpliedCovariance","title":"computeImpliedCovariance — computeImpliedCovariance","text":"computes implied covariance matrix SEM using RAM notation","code":""},{"path":"/reference/computeImpliedCovariance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"computeImpliedCovariance — computeImpliedCovariance","text":"","code":"computeImpliedCovariance(Fmatrix, impliedCovarianceFull)"},{"path":"/reference/computeImpliedCovariance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"computeImpliedCovariance — computeImpliedCovariance","text":"Fmatrix filter matrix impliedCovarianceFull implied covariance matrix including latent variables","code":""},{"path":"/reference/computeImpliedCovariance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"computeImpliedCovariance — computeImpliedCovariance","text":"matrix implied covariances","code":""},{"path":"/reference/computeImpliedCovarianceFull.html","id":null,"dir":"Reference","previous_headings":"","what":"computeImpliedCovarianceFull — computeImpliedCovarianceFull","title":"computeImpliedCovarianceFull — computeImpliedCovarianceFull","text":"computes implied covariance matrix including latent vaiables SEM using RAM notation","code":""},{"path":"/reference/computeImpliedCovarianceFull.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"computeImpliedCovarianceFull — computeImpliedCovarianceFull","text":"","code":"computeImpliedCovarianceFull(Amatrix, Smatrix, IminusAInverse)"},{"path":"/reference/computeImpliedCovarianceFull.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"computeImpliedCovarianceFull — computeImpliedCovarianceFull","text":"Amatrix matrix directed effects Smatrix matrix undirected effects IminusAInverse (-Amatrix)^(-1)","code":""},{"path":"/reference/computeImpliedCovarianceFull.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"computeImpliedCovarianceFull — computeImpliedCovarianceFull","text":"matrix implied covariances","code":""},{"path":"/reference/computeImpliedMeans.html","id":null,"dir":"Reference","previous_headings":"","what":"computeImpliedMeans — computeImpliedMeans","title":"computeImpliedMeans — computeImpliedMeans","text":"computes implied means vector SEM using RAM notation","code":""},{"path":"/reference/computeImpliedMeans.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"computeImpliedMeans — computeImpliedMeans","text":"","code":"computeImpliedMeans(Fmatrix, impliedMeansFull)"},{"path":"/reference/computeImpliedMeans.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"computeImpliedMeans — computeImpliedMeans","text":"Fmatrix filter matrix impliedMeansFull implied means vector including latent variables","code":""},{"path":"/reference/computeImpliedMeans.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"computeImpliedMeans — computeImpliedMeans","text":"matrix implied means","code":""},{"path":"/reference/computeImpliedMeansFull.html","id":null,"dir":"Reference","previous_headings":"","what":"computeImpliedMeansFull — computeImpliedMeansFull","title":"computeImpliedMeansFull — computeImpliedMeansFull","text":"computes implied means vector SEM including latent variables using RAM notation","code":""},{"path":"/reference/computeImpliedMeansFull.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"computeImpliedMeansFull — computeImpliedMeansFull","text":"","code":"computeImpliedMeansFull(Amatrix, Mvector, IminusAInverse)"},{"path":"/reference/computeImpliedMeansFull.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"computeImpliedMeansFull — computeImpliedMeansFull","text":"Amatrix matrix directed effects Mvector vector means IminusAInverse (-Amatrix)^(-1)","code":""},{"path":"/reference/computeImpliedMeansFull.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"computeImpliedMeansFull — computeImpliedMeansFull","text":"matrix implied means","code":""},{"path":"/reference/controlBFGS.html","id":null,"dir":"Reference","previous_headings":"","what":"controlBFGS — controlBFGS","title":"controlBFGS — controlBFGS","text":"Control BFGS optimizer.","code":""},{"path":"/reference/controlBFGS.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"controlBFGS — controlBFGS","text":"","code":"controlBFGS(   startingValues = \"est\",   initialHessian = ifelse(all(startingValues == \"est\"), \"lavaan\", \"compute\"),   saveHessian = FALSE,   stepSize = 0.9,   sigma = 1e-05,   gamma = 0,   maxIterOut = 1000,   maxIterIn = 1000,   maxIterLine = 500,   breakOuter = 1e-08,   breakInner = 1e-10,   convergenceCriterion = 0,   verbose = 0,   nCores = 1 )"},{"path":"/reference/controlBFGS.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"controlBFGS — controlBFGS","text":"startingValues option provide initial starting values. used first lambda. Three options supported. Setting \"est\" use estimates lavaan model object. Setting \"start\" use starting values lavaan model. Finally, labeled vector parameter values can passed function used starting values. initialHessian option provide initial Hessian optimizer. Must row column names corresponding parameter labels. use getLavaanParameters(lavaanModel) see labels. set \"scoreBased\", outer product scores used approximation (see https://en.wikipedia.org/wiki/Berndt%E2%80%93Hall%E2%80%93Hall%E2%80%93Hausman_algorithm). set \"compute\", initial hessian computed. set single value, diagonal matrix single value along diagonal used. default \"lavaan\" extracts Hessian lavaanModel. Hessian typically deviate internal SEM represenation lessSEM (due transformation variances), works quite well practice. saveHessian Hessian saved later use? Note: may take lot memory! stepSize Initial stepSize outer iteration (theta_k+1 = theta_k + stepSize * Stepdirection) sigma relevant lineSearch = 'GLMNET'. Controls sigma parameter Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421. gamma Controls gamma parameter Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421. Defaults 0. maxIterOut Maximal number outer iterations maxIterIn Maximal number inner iterations maxIterLine Maximal number iterations line search procedure breakOuter Stopping criterion outer iterations breakInner Stopping criterion inner iterations convergenceCriterion convergence criterion used outer iterations? possible 0 = GLMNET, 1 = fitChange, 2 = gradients. Note case gradients GLMNET, divide gradients (Hessian) log-Likelihood N otherwise considerably difficult larger sample sizes reach convergence criteria. verbose 0 prints additional information, > 0 prints GLMNET iterations nCores number core use. Multi-core support provided RcppParallel supported SEM, general purpose optimization.","code":""},{"path":"/reference/controlBFGS.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"controlBFGS — controlBFGS","text":"object class controlBFGS","code":""},{"path":"/reference/controlGlmnet.html","id":null,"dir":"Reference","previous_headings":"","what":"controlGlmnet — controlGlmnet","title":"controlGlmnet — controlGlmnet","text":"Control GLMNET optimizer.","code":""},{"path":"/reference/controlGlmnet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"controlGlmnet — controlGlmnet","text":"","code":"controlGlmnet(   startingValues = \"est\",   initialHessian = ifelse(all(startingValues == \"est\"), \"lavaan\", \"compute\"),   saveHessian = FALSE,   stepSize = 0.9,   sigma = 1e-05,   gamma = 0,   maxIterOut = 1000,   maxIterIn = 1000,   maxIterLine = 500,   breakOuter = 1e-08,   breakInner = 1e-10,   convergenceCriterion = 0,   verbose = 0,   nCores = 1 )"},{"path":"/reference/controlGlmnet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"controlGlmnet — controlGlmnet","text":"startingValues option provide initial starting values. used first lambda. Three options supported. Setting \"est\" use estimates lavaan model object. Setting \"start\" use starting values lavaan model. Finally, labeled vector parameter values can passed function used starting values. initialHessian option provide initial Hessian optimizer. Must row column names corresponding parameter labels. use getLavaanParameters(lavaanModel) see labels. set \"scoreBased\", outer product scores used approximation (see https://en.wikipedia.org/wiki/Berndt%E2%80%93Hall%E2%80%93Hall%E2%80%93Hausman_algorithm). set \"compute\", initial hessian computed. set single value, diagonal matrix single value along diagonal used. default \"lavaan\" extracts Hessian lavaanModel. Hessian typically deviate internal SEM represenation lessSEM (due transformation variances), works quite well practice. saveHessian Hessian saved later use? Note: may take lot memory! stepSize Initial stepSize outer iteration (theta_k+1 = theta_k + stepSize * Stepdirection) sigma relevant lineSearch = 'GLMNET'. Controls sigma parameter Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421. gamma Controls gamma parameter Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421. Defaults 0. maxIterOut Maximal number outer iterations maxIterIn Maximal number inner iterations maxIterLine Maximal number iterations line search procedure breakOuter Stopping criterion outer iterations breakInner Stopping criterion inner iterations convergenceCriterion convergence criterion used outer iterations? possible 0 = GLMNET, 1 = fitChange, 2 = gradients. Note case gradients GLMNET, divide gradients (Hessian) log-Likelihood N otherwise considerably difficult larger sample sizes reach convergence criteria. verbose 0 prints additional information, > 0 prints GLMNET iterations nCores number core use. Multi-core support provided RcppParallel supported SEM, general purpose optimization.","code":""},{"path":"/reference/controlGlmnet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"controlGlmnet — controlGlmnet","text":"object class controlGlmnet","code":""},{"path":"/reference/controlIsta.html","id":null,"dir":"Reference","previous_headings":"","what":"controlIsta — controlIsta","title":"controlIsta — controlIsta","text":"controlIsta","code":""},{"path":"/reference/controlIsta.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"controlIsta — controlIsta","text":"","code":"controlIsta(   startingValues = \"est\",   L0 = 0.1,   eta = 2,   accelerate = TRUE,   maxIterOut = 10000,   maxIterIn = 1000,   breakOuter = 1e-08,   convCritInner = 1,   sigma = 0.1,   stepSizeInheritance = ifelse(accelerate, 1, 3),   verbose = 0,   nCores = 1 )"},{"path":"/reference/controlIsta.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"controlIsta — controlIsta","text":"startingValues option provide initial starting values. used first lambda. Three options supported. Setting \"est\" use estimates lavaan model object. Setting \"start\" use starting values lavaan model. Finally, labeled vector parameter values can passed function used starting values. L0 L0 controls step size used first iteration eta eta controls much step size changes inner iterations (eta^)*L, inner iteration accelerate boolean: acceleration outlined Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231., p. 152 used? maxIterOut maximal number outer iterations maxIterIn maximal number inner iterations breakOuter change fit required break outer iteration. Note: value multiplied internally sample size N -2log-Likelihood depends directly sample size convCritInner related inner breaking condition. 0 = ista, presented Beck & Teboulle (2009); see Remark 3.1 p. 191 (ISTA backtracking) 1 = gist, presented Gong et al. (2013) (Equation 3) sigma sigma (0,1) used gist convergence criterion. larger sigma enforce larger improvement fit stepSizeInheritance step sizes carried forward iteration iteration? 0 = resets step size L0 iteration 1 = takes previous step size initial value next iteration 3 = Barzilai-Borwein procedure 4 = Barzilai-Borwein procedure, sometimes resets step size; can help optimizer caught bad spot. verbose set value > 0, fit every \"verbose\" iterations printed. nCores number core use. Multi-core support provided RcppParallel supported SEM, general purpose optimization.","code":""},{"path":"/reference/controlIsta.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"controlIsta — controlIsta","text":"object class controlIsta","code":""},{"path":"/reference/createSubsets.html","id":null,"dir":"Reference","previous_headings":"","what":"createSubsets — createSubsets","title":"createSubsets — createSubsets","text":"create subsets cross-validation","code":""},{"path":"/reference/createSubsets.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"createSubsets — createSubsets","text":"","code":"createSubsets(N, k)"},{"path":"/reference/createSubsets.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"createSubsets — createSubsets","text":"N number samples data set k number subsets create","code":""},{"path":"/reference/createSubsets.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"createSubsets — createSubsets","text":"matrix subsets","code":""},{"path":"/reference/createSubsets.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"createSubsets — createSubsets","text":"","code":"createSubsets(N=100, k = 5)"},{"path":"/reference/curveLambda.html","id":null,"dir":"Reference","previous_headings":"","what":"curveLambda — curveLambda","title":"curveLambda — curveLambda","text":"generates lambda values 0 lambdaMax using function described : https://math.stackexchange.com/questions/384613/exponential-function--values--0--1--x-values--0--1. function identical one implemented regCtsem package.","code":""},{"path":"/reference/curveLambda.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"curveLambda — curveLambda","text":"","code":"curveLambda(maxLambda, lambdasAutoCurve, lambdasAutoLength)"},{"path":"/reference/curveLambda.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"curveLambda — curveLambda","text":"maxLambda maximal lambda value lambdasAutoCurve controls curve. value close 1 result linear increase, larger values lambdas concentrated around 0 lambdasAutoLength number lambda values generate","code":""},{"path":"/reference/curveLambda.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"curveLambda — curveLambda","text":"","code":"plot(curveLambda(maxLambda = 10, lambdasAutoCurve = 1, lambdasAutoLength = 100)) plot(curveLambda(maxLambda = 10, lambdasAutoCurve = 5, lambdasAutoLength = 100)) plot(curveLambda(maxLambda = 10, lambdasAutoCurve = 100, lambdasAutoLength = 100))"},{"path":"/reference/cvAdaptiveLasso.html","id":null,"dir":"Reference","previous_headings":"","what":"cvAdaptiveLasso — cvAdaptiveLasso","title":"cvAdaptiveLasso — cvAdaptiveLasso","text":"Implements cross-validated adaptive lasso regularization structural equation models. penalty function given : $$p( x_j) = p( x_j) = \\frac{1}{w_j}\\lambda| x_j|$$ Adaptive lasso regularization set parameters zero \\(\\lambda\\) large enough.","code":""},{"path":"/reference/cvAdaptiveLasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"cvAdaptiveLasso — cvAdaptiveLasso","text":"","code":"cvAdaptiveLasso(   lavaanModel,   regularized,   weights = NULL,   lambdas,   k = 5,   standardize = FALSE,   returnSubsetParameters = FALSE,   method = \"glmnet\",   modifyModel = lessSEM::modifyModel(),   control = lessSEM::controlGlmnet() )"},{"path":"/reference/cvAdaptiveLasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"cvAdaptiveLasso — cvAdaptiveLasso","text":"lavaanModel model class lavaan regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object weights labeled vector weights parameters model. unsure parameters called, use getLavaanParameters(model) lavaan model object. set NULL, default weights used: inverse absolute values unregularized parameter estimates lambdas numeric vector: values tuning parameter lambda k number cross-validation folds. Alternatively, can pass matrix booleans (TRUE, FALSE) indicates person subset belongs . See ?lessSEM::createSubsets example matrix look like. standardize Standardizing data prior analysis can undermine cross- validation. Set standardize=TRUE automatically standardize data. returnSubsetParameters set TRUE return parameters training set method optimizer used? Currently implemented ista glmnet. ista, control argument can used switch related procedures (currently gist). modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. element generated controlIsta controlGlmnet functions. See ?controlIsta ?controlGlmnet details.","code":""},{"path":"/reference/cvAdaptiveLasso.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"cvAdaptiveLasso — cvAdaptiveLasso","text":"model class cvRegularizedSEM","code":""},{"path":"/reference/cvAdaptiveLasso.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"cvAdaptiveLasso — cvAdaptiveLasso","text":"Identical regsem, models specified using lavaan. Currenlty, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. Adaptive lasso regularization: Zou, H. (2006). adaptive lasso oracle properties. Journal American Statistical Association, 101(476), 1418–1429. https://doi.org/10.1198/016214506000000735 Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classiﬁcation. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/cvAdaptiveLasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"cvAdaptiveLasso — cvAdaptiveLasso","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +      l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +      l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Optional: Plot the model # semPlot::semPaths(lavaanModel, #                   what = \"est\", #                   fade = FALSE)  lsem <- cvAdaptiveLasso(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   # names of the regularized parameters:   regularized = paste0(\"l\", 6:15),   lambdas = seq(0,1,.1))  # use the plot-function to plot the cross-validation fit plot(lsem)  # the coefficients can be accessed with: coef(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters  # The best parameters can also be extracted with: coef(lsem)"},{"path":"/reference/cvCappedL1.html","id":null,"dir":"Reference","previous_headings":"","what":"cvCappedL1 — cvCappedL1","title":"cvCappedL1 — cvCappedL1","text":"Implements cappedL1 regularization structural equation models. penalty function given : $$p( x_j) = \\lambda \\min(| x_j|, \\theta)$$ \\(\\theta > 0\\). cappedL1 penalty identical lasso parameters \\(\\theta\\) identical constant parameters \\(\\theta\\). adding constant fitting function change minimum, larger parameters can stay unregularized smaller ones set zero.","code":""},{"path":"/reference/cvCappedL1.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"cvCappedL1 — cvCappedL1","text":"","code":"cvCappedL1(   lavaanModel,   regularized,   lambdas,   thetas,   k = 5,   standardize = FALSE,   returnSubsetParameters = FALSE,   modifyModel = lessSEM::modifyModel(),   control = lessSEM::controlIsta() )"},{"path":"/reference/cvCappedL1.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"cvCappedL1 — cvCappedL1","text":"lavaanModel model class lavaan regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda thetas parameters whose absolute value threshold penalized constant (theta) k number cross-validation folds. Alternatively, can pass matrix booleans (TRUE, FALSE) indicates person subset belongs . See ?lessSEM::createSubsets example matrix look like. standardize Standardizing data prior analysis can undermine cross- validation. Set standardize=TRUE automatically standardize data. returnSubsetParameters set TRUE return parameters training set modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. element generated controlIsta function. See ?controlIsta details.","code":""},{"path":"/reference/cvCappedL1.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"cvCappedL1 — cvCappedL1","text":"model class cvRegularizedSEM","code":""},{"path":"/reference/cvCappedL1.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"cvCappedL1 — cvCappedL1","text":"Identical regsem, models specified using lavaan. Currenlty, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. CappedL1 regularization: Zhang, T. (2010). Analysis Multi-stage Convex Relaxation Sparse Regularization. Journal Machine Learning Research, 11, 1081–1107. Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classiﬁcation. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/cvCappedL1.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"cvCappedL1 — cvCappedL1","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +      l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +      l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Optional: Plot the model # semPlot::semPaths(lavaanModel, #                   what = \"est\", #                   fade = FALSE)  lsem <- cvCappedL1(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   # names of the regularized parameters:   regularized = paste0(\"l\", 6:15),   lambdas = seq(0,1,length.out = 20),   thetas = seq(0.01,2,length.out = 5))  # the coefficients can be accessed with: coef(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters  # optional: plotting the cross-validation fit requires installation of plotly # plot(lsem)"},{"path":"/reference/cvElasticNet.html","id":null,"dir":"Reference","previous_headings":"","what":"cvElasticNet — cvElasticNet","title":"cvElasticNet — cvElasticNet","text":"Implements elastic net regularization structural equation models. penalty function given : $$p( x_j) = \\alpha\\lambda| x_j| + (1-\\alpha)\\lambda x_j^2$$ Note elastic net combines ridge lasso regularization. \\(\\alpha = 0\\), elastic net reduces ridge regularization. \\(\\alpha = 1\\) reduces lasso regularization. , elastic net compromise shrinkage lasso ridge penalty.","code":""},{"path":"/reference/cvElasticNet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"cvElasticNet — cvElasticNet","text":"","code":"cvElasticNet(   lavaanModel,   regularized,   lambdas,   alphas,   k = 5,   standardize = FALSE,   returnSubsetParameters = FALSE,   method = \"glmnet\",   modifyModel = lessSEM::modifyModel(),   control = lessSEM::controlGlmnet() )"},{"path":"/reference/cvElasticNet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"cvElasticNet — cvElasticNet","text":"lavaanModel model class lavaan regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda alphas numeric vector values tuning parameter alpha. Must 0 1. 0 = ridge, 1 = lasso. k number cross-validation folds. Alternatively, can pass matrix booleans (TRUE, FALSE) indicates person subset belongs . See ?lessSEM::createSubsets example matrix look like. standardize Standardizing data prior analysis can undermine cross- validation. Set standardize=TRUE automatically standardize data. returnSubsetParameters set TRUE return parameters training set method optimizer used? Currently implemented ista glmnet. ista, control argument can used switch related procedures (currently gist). modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. element generated controlIsta controlGlmnet functions. See ?controlIsta ?controlGlmnet details.","code":""},{"path":"/reference/cvElasticNet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"cvElasticNet — cvElasticNet","text":"model class cvRegularizedSEM","code":""},{"path":"/reference/cvElasticNet.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"cvElasticNet — cvElasticNet","text":"Identical regsem, models specified using lavaan. Currenlty, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. Elastic net regularization: Zou, H., & Hastie, T. (2005). Regularization variable selection via elastic net. Journal Royal Statistical Society: Series B, 67(2), 301–320. https://doi.org/10.1111/j.1467-9868.2005.00503.x Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classiﬁcation. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/cvElasticNet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"cvElasticNet — cvElasticNet","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +      l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +      l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Optional: Plot the model # semPlot::semPaths(lavaanModel, #                   what = \"est\", #                   fade = FALSE)  lsem <- cvElasticNet(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   # names of the regularized parameters:   regularized = paste0(\"l\", 6:15),   lambdas = seq(0,1,length.out = 20),   alphas = seq(0,1,.1))  # the coefficients can be accessed with: coef(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters  # optional: plotting the cross-validation fit requires installation of plotly # plot(lsem)"},{"path":"/reference/cvLasso.html","id":null,"dir":"Reference","previous_headings":"","what":"cvLasso — cvLasso","title":"cvLasso — cvLasso","text":"Implements cross-validated lasso regularization structural equation models. penalty function given : $$p( x_j) = \\lambda |x_j|$$ Lasso regularization set parameters zero \\(\\lambda\\) large enough","code":""},{"path":"/reference/cvLasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"cvLasso — cvLasso","text":"","code":"cvLasso(   lavaanModel,   regularized,   lambdas,   k = 5,   standardize = FALSE,   returnSubsetParameters = FALSE,   method = \"glmnet\",   modifyModel = lessSEM::modifyModel(),   control = lessSEM::controlGlmnet() )"},{"path":"/reference/cvLasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"cvLasso — cvLasso","text":"lavaanModel model class lavaan regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda k number cross-validation folds. Alternatively, can pass matrix booleans (TRUE, FALSE) indicates person subset belongs . See ?lessSEM::createSubsets example matrix look like. standardize Standardizing data prior analysis can undermine cross- validation. Set standardize=TRUE automatically standardize data. returnSubsetParameters set TRUE return parameters training set method optimizer used? Currently implemented ista glmnet. ista, control argument can used switch related procedures (currently gist). modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. element generated controlIsta controlGlmnet functions. See ?controlIsta ?controlGlmnet details.","code":""},{"path":"/reference/cvLasso.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"cvLasso — cvLasso","text":"model class cvRegularizedSEM","code":""},{"path":"/reference/cvLasso.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"cvLasso — cvLasso","text":"Identical regsem, models specified using lavaan. Currently, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. Lasso regularization: Tibshirani, R. (1996). Regression shrinkage selection via lasso. Journal Royal Statistical Society. Series B (Methodological), 58(1), 267–288. Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classiﬁcation. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/cvLasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"cvLasso — cvLasso","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +       l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +       l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Optional: Plot the model # semPlot::semPaths(lavaanModel,  #                   what = \"est\", #                   fade = FALSE)  lsem <- cvLasso(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   # names of the regularized parameters:   regularized = paste0(\"l\", 6:15),   lambdas = seq(0,1,.1),   k = 5, # number of cross-validation folds   standardize = TRUE) # automatic standardization  # use the plot-function to plot the cross-validation fit: plot(lsem)  # the coefficients can be accessed with: coef(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters  # The best parameters can also be extracted with: coef(lsem)"},{"path":"/reference/cvLsp.html","id":null,"dir":"Reference","previous_headings":"","what":"cvLsp — cvLsp","title":"cvLsp — cvLsp","text":"Implements lsp regularization structural equation models. penalty function given : $$p( x_j) = \\lambda \\log(1 + |x_j|/\\theta)$$ \\(\\theta > 0\\).","code":""},{"path":"/reference/cvLsp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"cvLsp — cvLsp","text":"","code":"cvLsp(   lavaanModel,   regularized,   lambdas,   thetas,   k = 5,   standardize = FALSE,   returnSubsetParameters = FALSE,   modifyModel = lessSEM::modifyModel(),   control = lessSEM::controlIsta() )"},{"path":"/reference/cvLsp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"cvLsp — cvLsp","text":"lavaanModel model class lavaan regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda thetas parameters whose absolute value threshold penalized constant (theta) k number cross-validation folds. Alternatively, can pass matrix booleans (TRUE, FALSE) indicates person subset belongs . See ?lessSEM::createSubsets example matrix look like. standardize Standardizing data prior analysis can undermine cross- validation. Set standardize=TRUE automatically standardize data. returnSubsetParameters set TRUE return parameters training set modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. element generated controlIsta function. See ?controlIsta","code":""},{"path":"/reference/cvLsp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"cvLsp — cvLsp","text":"model class cvRegularizedSEM","code":""},{"path":"/reference/cvLsp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"cvLsp — cvLsp","text":"Identical regsem, models specified using lavaan. Currenlty, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. lsp regularization: Candès, E. J., Wakin, M. B., & Boyd, S. P. (2008). Enhancing Sparsity Reweighted l1 Minimization. Journal Fourier Analysis Applications, 14(5–6), 877–905. https://doi.org/10.1007/s00041-008-9045-x Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classiﬁcation. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/cvLsp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"cvLsp — cvLsp","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +      l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +      l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Optional: Plot the model # semPlot::semPaths(lavaanModel, #                   what = \"est\", #                   fade = FALSE)  lsem <- cvLsp(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   # names of the regularized parameters:   regularized = paste0(\"l\", 6:15),   lambdas = seq(0,1,length.out = 20),   thetas = seq(0.01,2,length.out = 5))  # the coefficients can be accessed with: coef(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters  # optional: plotting the cross-validation fit requires installation of plotly # plot(lsem)"},{"path":"/reference/cvMcp.html","id":null,"dir":"Reference","previous_headings":"","what":"cvMcp — cvMcp","title":"cvMcp — cvMcp","text":"Implements mcp regularization structural equation models. penalty function given : $$p( x_j) = \\begin{cases} \\lambda |x_j| - x_j^2/(2\\theta) & \\text{} |x_j| \\leq \\theta\\lambda\\\\ \\theta\\lambda^2/2 & \\text{} |x_j| > \\lambda\\theta \\end{cases}$$ \\(\\theta > 0\\).","code":""},{"path":"/reference/cvMcp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"cvMcp — cvMcp","text":"","code":"cvMcp(   lavaanModel,   regularized,   lambdas,   thetas,   k = 5,   standardize = FALSE,   returnSubsetParameters = FALSE,   modifyModel = lessSEM::modifyModel(),   control = lessSEM::controlIsta() )"},{"path":"/reference/cvMcp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"cvMcp — cvMcp","text":"lavaanModel model class lavaan regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda thetas parameters whose absolute value threshold penalized constant (theta) k number cross-validation folds. Alternatively, can pass matrix booleans (TRUE, FALSE) indicates person subset belongs . See ?lessSEM::createSubsets example matrix look like. standardize Standardizing data prior analysis can undermine cross- validation. Set standardize=TRUE automatically standardize data. returnSubsetParameters set TRUE return parameters training set modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. element generated controlIsta function. See ?controlIsta","code":""},{"path":"/reference/cvMcp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"cvMcp — cvMcp","text":"model class cvRegularizedSEM","code":""},{"path":"/reference/cvMcp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"cvMcp — cvMcp","text":"Identical regsem, models specified using lavaan. Currenlty, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. mcp regularization: Zhang, C.-H. (2010). Nearly unbiased variable selection minimax concave penalty. Annals Statistics, 38(2), 894–942. https://doi.org/10.1214/09-AOS729 Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classiﬁcation. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/cvMcp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"cvMcp — cvMcp","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +      l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +      l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Optional: Plot the model # semPlot::semPaths(lavaanModel, #                   what = \"est\", #                   fade = FALSE)  lsem <- cvMcp(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   # names of the regularized parameters:   regularized = paste0(\"l\", 6:15),   lambdas = seq(0,1,length.out = 20),   thetas = seq(0.01,2,length.out = 5))  # the coefficients can be accessed with: coef(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters  # optional: plotting the cross-validation fit requires installation of plotly # plot(lsem)"},{"path":"/reference/cvRegularizedSEM-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Class for cross-validated regularized SEM — cvRegularizedSEM-class","title":"Class for cross-validated regularized SEM — cvRegularizedSEM-class","text":"Class cross-validated regularized SEM","code":""},{"path":"/reference/cvRegularizedSEM-class.html","id":"slots","dir":"Reference","previous_headings":"","what":"Slots","title":"Class for cross-validated regularized SEM — cvRegularizedSEM-class","text":"parameters data.frame parameter estimates best combination tuning parameters transformations transformed parameters cvfits data.frame combinations tuning parameters sum cross-validation fits parameterLabels character vector names parameters regularized character vector names regularized parameters cvfitsDetails data.frame cross-validation fits subset subsets matrix indicating person subset subsetParameters optional: data.frame parameter estimates combinations tuning parameters subsets misc list additional return elements","code":""},{"path":"/reference/cvRidge.html","id":null,"dir":"Reference","previous_headings":"","what":"cvRidge — cvRidge","title":"cvRidge — cvRidge","text":"Implements ridge regularization structural equation models. penalty function given : $$p( x_j) = \\lambda x_j^2$$ Note ridge regularization set parameters zero result shrinkage towards zero.","code":""},{"path":"/reference/cvRidge.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"cvRidge — cvRidge","text":"","code":"cvRidge(   lavaanModel,   regularized,   lambdas,   k = 5,   standardize = FALSE,   returnSubsetParameters = FALSE,   method = \"glmnet\",   modifyModel = lessSEM::modifyModel(),   control = lessSEM::controlGlmnet() )"},{"path":"/reference/cvRidge.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"cvRidge — cvRidge","text":"lavaanModel model class lavaan regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda k number cross-validation folds. Alternatively, can pass matrix booleans (TRUE, FALSE) indicates person subset belongs . See ?lessSEM::createSubsets example matrix look like. standardize Standardizing data prior analysis can undermine cross- validation. Set standardize=TRUE automatically standardize data. returnSubsetParameters set TRUE return parameters training set method optimizer used? Currently implemented ista glmnet. ista, control argument can used switch related procedures (currently gist). modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. element generated controlIsta controlGlmnet functions. See ?controlIsta ?controlGlmnet details.","code":""},{"path":"/reference/cvRidge.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"cvRidge — cvRidge","text":"model class cvRegularizedSEM","code":""},{"path":"/reference/cvRidge.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"cvRidge — cvRidge","text":"Identical regsem, models specified using lavaan. Currenlty, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. Ridge regularization: Hoerl, . E., & Kennard, R. W. (1970). Ridge Regression: Biased Estimation Nonorthogonal Problems. Technometrics, 12(1), 55–67. https://doi.org/10.1080/00401706.1970.10488634 Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classiﬁcation. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/cvRidge.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"cvRidge — cvRidge","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +      l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +      l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Optional: Plot the model # semPlot::semPaths(lavaanModel, #                   what = \"est\", #                   fade = FALSE)  lsem <- cvRidge(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   # names of the regularized parameters:   regularized = paste0(\"l\", 6:15),   lambdas = seq(0,1,length.out = 20))  # use the plot-function to plot the cross-validation fit: plot(lsem)  # the coefficients can be accessed with: coef(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters"},{"path":"/reference/cvRidgeBfgs.html","id":null,"dir":"Reference","previous_headings":"","what":"cvRidgeBfgs — cvRidgeBfgs","title":"cvRidgeBfgs — cvRidgeBfgs","text":"Implements cross-validated ridge regularization structural equation models. penalty function given : $$p( x_j) = \\lambda x_j^2$$ Note ridge regularization set parameters zero result shrinkage towards zero.","code":""},{"path":"/reference/cvRidgeBfgs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"cvRidgeBfgs — cvRidgeBfgs","text":"","code":"cvRidgeBfgs(   lavaanModel,   regularized,   lambdas,   k = 5,   standardize = FALSE,   returnSubsetParameters = FALSE,   modifyModel = lessSEM::modifyModel(),   control = lessSEM::controlBFGS() )"},{"path":"/reference/cvRidgeBfgs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"cvRidgeBfgs — cvRidgeBfgs","text":"lavaanModel model class lavaan regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda k number cross-validation folds. Alternatively, can pass matrix booleans (TRUE, FALSE) indicates person subset belongs . See ?lessSEM::createSubsets example matrix look like. standardize Standardizing data prior analysis can undermine cross- validation. Set standardize=TRUE automatically standardize data. returnSubsetParameters set TRUE return parameters training set modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. element generated controlBFGS function. See ?controlBFGS details.","code":""},{"path":"/reference/cvRidgeBfgs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"cvRidgeBfgs — cvRidgeBfgs","text":"model class cvRegularizedSEM","code":""},{"path":"/reference/cvRidgeBfgs.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"cvRidgeBfgs — cvRidgeBfgs","text":"Identical regsem, models specified using lavaan. Currently, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. Ridge regularization: Hoerl, . E., & Kennard, R. W. (1970). Ridge Regression: Biased Estimation Nonorthogonal Problems. Technometrics, 12(1), 55–67. https://doi.org/10.1080/00401706.1970.10488634 Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793","code":""},{"path":"/reference/cvRidgeBfgs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"cvRidgeBfgs — cvRidgeBfgs","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +      l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +      l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Optional: Plot the model # semPlot::semPaths(lavaanModel, #                   what = \"est\", #                   fade = FALSE)  lsem <- cvRidgeBfgs(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   # names of the regularized parameters:   regularized = paste0(\"l\", 6:15),   lambdas = seq(0,1,length.out = 20))  # use the plot-function to plot the cross-validation fit: plot(lsem)  # the coefficients can be accessed with: coef(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters"},{"path":"/reference/cvScad.html","id":null,"dir":"Reference","previous_headings":"","what":"cvScad — cvScad","title":"cvScad — cvScad","text":"Implements scad regularization structural equation models. penalty function given :  $$p( x_j) = \\begin{cases} \\lambda |x_j| & \\text{} |x_j| \\leq \\theta\\\\ \\frac{-x_j^2 + 2\\theta\\lambda |x_j| - \\lambda^2}{2(\\theta -1)} &  \\text{} \\lambda < |x_j| \\leq \\lambda\\theta \\\\ (\\theta + 1) \\lambda^2/2 & \\text{} |x_j| \\geq \\theta\\lambda\\\\ \\end{cases}$$ \\(\\theta > 2\\).","code":""},{"path":"/reference/cvScad.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"cvScad — cvScad","text":"","code":"cvScad(   lavaanModel,   regularized,   lambdas,   thetas,   k = 5,   standardize = FALSE,   returnSubsetParameters = FALSE,   modifyModel = lessSEM::modifyModel(),   control = lessSEM::controlIsta() )"},{"path":"/reference/cvScad.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"cvScad — cvScad","text":"lavaanModel model class lavaan regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda thetas parameters whose absolute value threshold penalized constant (theta) k number cross-validation folds. Alternatively, can pass matrix booleans (TRUE, FALSE) indicates person subset belongs . See ?lessSEM::createSubsets example matrix look like. standardize Standardizing data prior analysis can undermine cross- validation. Set standardize=TRUE automatically standardize data. returnSubsetParameters set TRUE return parameters training set modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. element generated controlIsta function. See ?controlIsta","code":""},{"path":"/reference/cvScad.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"cvScad — cvScad","text":"model class cvRegularizedSEM","code":""},{"path":"/reference/cvScad.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"cvScad — cvScad","text":"Identical regsem, models specified using lavaan. Currenlty, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. scad regularization: Fan, J., & Li, R. (2001). Variable selection via nonconcave penalized likelihood oracle properties. Journal American Statistical Association, 96(456), 1348–1360. https://doi.org/10.1198/016214501753382273 Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classiﬁcation. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/cvScad.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"cvScad — cvScad","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +      l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +      l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Optional: Plot the model # semPlot::semPaths(lavaanModel, #                   what = \"est\", #                   fade = FALSE)  lsem <- cvScad(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   # names of the regularized parameters:   regularized = paste0(\"l\", 6:15),   lambdas = seq(0,1,length.out = 20),   thetas = seq(2.01,5,length.out = 5))  # the coefficients can be accessed with: coef(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters  # optional: plotting the cross-validation fit requires installation of plotly # plot(lsem)"},{"path":"/reference/cvScaler.html","id":null,"dir":"Reference","previous_headings":"","what":"cvScaler — cvScaler","title":"cvScaler — cvScaler","text":"uses means standard deviations training set standardize test set. See, e.g., https://scikit-learn.org/stable/modules/cross_validation.html .","code":""},{"path":"/reference/cvScaler.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"cvScaler — cvScaler","text":"","code":"cvScaler(testSet, means, standardDeviations)"},{"path":"/reference/cvScaler.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"cvScaler — cvScaler","text":"testSet test data set means means training set standardDeviations standard deviations training set","code":""},{"path":"/reference/cvScaler.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"cvScaler — cvScaler","text":"scaled test set","code":""},{"path":"/reference/cvSmoothAdaptiveLasso.html","id":null,"dir":"Reference","previous_headings":"","what":"cvSmoothAdaptiveLasso — cvSmoothAdaptiveLasso","title":"cvSmoothAdaptiveLasso — cvSmoothAdaptiveLasso","text":"Implements cross-validated smooth adaptive lasso regularization structural equation models. penalty function given : $$p( x_j) = p( x_j) = \\frac{1}{w_j}\\lambda\\sqrt{(x_j + \\epsilon)^2}$$","code":""},{"path":"/reference/cvSmoothAdaptiveLasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"cvSmoothAdaptiveLasso — cvSmoothAdaptiveLasso","text":"","code":"cvSmoothAdaptiveLasso(   lavaanModel,   regularized,   weights = NULL,   lambdas,   epsilon,   k = 5,   standardize = FALSE,   returnSubsetParameters = FALSE,   modifyModel = lessSEM::modifyModel(),   control = lessSEM::controlBFGS() )"},{"path":"/reference/cvSmoothAdaptiveLasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"cvSmoothAdaptiveLasso — cvSmoothAdaptiveLasso","text":"lavaanModel model class lavaan regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object weights labeled vector weights parameters model. unsure parameters called, use getLavaanParameters(model) lavaan model object. set NULL, default weights used: inverse absolute values unregularized parameter estimates lambdas numeric vector: values tuning parameter lambda epsilon epsilon > 0; controls smoothness approximation. Larger values = smoother k number cross-validation folds. Alternatively, can pass matrix booleans (TRUE, FALSE) indicates person subset belongs . See ?lessSEM::createSubsets example matrix look like. standardize Standardizing data prior analysis can undermine cross- validation. Set standardize=TRUE automatically standardize data. returnSubsetParameters set TRUE return parameters training set modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. element generated controlBFGS function. See ?controlBFGS details.","code":""},{"path":"/reference/cvSmoothAdaptiveLasso.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"cvSmoothAdaptiveLasso — cvSmoothAdaptiveLasso","text":"model class cvRegularizedSEM","code":""},{"path":"/reference/cvSmoothAdaptiveLasso.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"cvSmoothAdaptiveLasso — cvSmoothAdaptiveLasso","text":"Identical regsem, models specified using lavaan. Currently, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. Adaptive lasso regularization: Zou, H. (2006). adaptive lasso oracle properties. Journal American Statistical Association, 101(476), 1418–1429. https://doi.org/10.1198/016214506000000735 Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793","code":""},{"path":"/reference/cvSmoothAdaptiveLasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"cvSmoothAdaptiveLasso — cvSmoothAdaptiveLasso","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +      l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +      l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Optional: Plot the model # semPlot::semPaths(lavaanModel, #                   what = \"est\", #                   fade = FALSE)  lsem <- cvSmoothAdaptiveLasso(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   # names of the regularized parameters:   regularized = paste0(\"l\", 6:15),   lambdas = seq(0,1,.1),   epsilon = 1e-8)  # use the plot-function to plot the cross-validation fit plot(lsem)  # the coefficients can be accessed with: coef(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters  # The best parameters can also be extracted with: coef(lsem)"},{"path":"/reference/cvSmoothElasticNet.html","id":null,"dir":"Reference","previous_headings":"","what":"cvSmoothElasticNet — cvSmoothElasticNet","title":"cvSmoothElasticNet — cvSmoothElasticNet","text":"Implements cross-validated  smooth elastic net regularization structural equation models. penalty function given : $$p( x_j) = \\alpha\\lambda\\sqrt{(x_j + \\epsilon)^2} + (1-\\alpha)\\lambda x_j^2$$ Note smooth elastic net combines ridge smooth lasso regularization. \\(\\alpha = 0\\), elastic net reduces ridge regularization. \\(\\alpha = 1\\) reduces smooth lasso regularization. , elastic net compromise shrinkage lasso ridge penalty.","code":""},{"path":"/reference/cvSmoothElasticNet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"cvSmoothElasticNet — cvSmoothElasticNet","text":"","code":"cvSmoothElasticNet(   lavaanModel,   regularized,   lambdas,   alphas,   epsilon,   k = 5,   standardize = FALSE,   returnSubsetParameters = FALSE,   modifyModel = lessSEM::modifyModel(),   control = lessSEM::controlBFGS() )"},{"path":"/reference/cvSmoothElasticNet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"cvSmoothElasticNet — cvSmoothElasticNet","text":"lavaanModel model class lavaan regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda alphas numeric vector values tuning parameter alpha. Must 0 1. 0 = ridge, 1 = lasso. epsilon epsilon > 0; controls smoothness approximation. Larger values = smoother k number cross-validation folds. Alternatively, can pass matrix booleans (TRUE, FALSE) indicates person subset belongs . See ?lessSEM::createSubsets example matrix look like. standardize Standardizing data prior analysis can undermine cross- validation. Set standardize=TRUE automatically standardize data. returnSubsetParameters set TRUE return parameters training set modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. element generated controlBFGS function. See ?controlBFGS details.","code":""},{"path":"/reference/cvSmoothElasticNet.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"cvSmoothElasticNet — cvSmoothElasticNet","text":"Identical regsem, models specified using lavaan. Currently, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. Elastic net regularization: Zou, H., & Hastie, T. (2005). Regularization variable selection via elastic net. Journal Royal Statistical Society: Series B, 67(2), 301–320. https://doi.org/10.1111/j.1467-9868.2005.00503.x Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793","code":""},{"path":"/reference/cvSmoothElasticNet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"cvSmoothElasticNet — cvSmoothElasticNet","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +      l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +      l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Optional: Plot the model # semPlot::semPaths(lavaanModel, #                   what = \"est\", #                   fade = FALSE)  lsem <- cvSmoothElasticNet(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   # names of the regularized parameters:   regularized = paste0(\"l\", 6:15),   epsilon = 1e-8,   lambdas = seq(0,1,length.out = 20),   alphas = seq(0,1,.1))  # the coefficients can be accessed with: coef(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters  # optional: plotting the cross-validation fit requires installation of plotly # plot(lsem)"},{"path":"/reference/cvSmoothLasso.html","id":null,"dir":"Reference","previous_headings":"","what":"cvSmoothLasso — cvSmoothLasso","title":"cvSmoothLasso — cvSmoothLasso","text":"Implements cross-validated smooth lasso regularization structural equation models. penalty function given : $$p( x_j) = \\lambda \\sqrt{(x_j + \\epsilon)^2}$$","code":""},{"path":"/reference/cvSmoothLasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"cvSmoothLasso — cvSmoothLasso","text":"","code":"cvSmoothLasso(   lavaanModel,   regularized,   lambdas,   epsilon,   k = 5,   standardize = FALSE,   returnSubsetParameters = FALSE,   modifyModel = lessSEM::modifyModel(),   control = lessSEM::controlBFGS() )"},{"path":"/reference/cvSmoothLasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"cvSmoothLasso — cvSmoothLasso","text":"lavaanModel model class lavaan regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda epsilon epsilon > 0; controls smoothness approximation. Larger values = smoother k number cross-validation folds. Alternatively, can pass matrix booleans (TRUE, FALSE) indicates person subset belongs . See ?lessSEM::createSubsets example matrix look like. standardize Standardizing data prior analysis can undermine cross- validation. Set standardize=TRUE automatically standardize data. returnSubsetParameters set TRUE return parameters training set modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. element generated controlBFGS function. See ?controlBFGS details.","code":""},{"path":"/reference/cvSmoothLasso.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"cvSmoothLasso — cvSmoothLasso","text":"model class cvRegularizedSEM","code":""},{"path":"/reference/cvSmoothLasso.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"cvSmoothLasso — cvSmoothLasso","text":"Identical regsem, models specified using lavaan. Currently, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. Lasso regularization: Tibshirani, R. (1996). Regression shrinkage selection via lasso. Journal Royal Statistical Society. Series B (Methodological), 58(1), 267–288. Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793","code":""},{"path":"/reference/cvSmoothLasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"cvSmoothLasso — cvSmoothLasso","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +       l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +       l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Optional: Plot the model # semPlot::semPaths(lavaanModel,  #                   what = \"est\", #                   fade = FALSE)  lsem <- cvSmoothLasso(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   # names of the regularized parameters:   regularized = paste0(\"l\", 6:15),   lambdas = seq(0,1,.1),   k = 5, # number of cross-validation folds   epsilon = 1e-8,   standardize = TRUE) # automatic standardization  # use the plot-function to plot the cross-validation fit: plot(lsem)  # the coefficients can be accessed with: coef(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters  # The best parameters can also be extracted with: coef(lsem)"},{"path":"/reference/dot-SEMFromLavaan.html","id":null,"dir":"Reference","previous_headings":"","what":".SEMFromLavaan — .SEMFromLavaan","title":".SEMFromLavaan — .SEMFromLavaan","text":"internal function. Translates object class lavaan internal model representation.","code":""},{"path":"/reference/dot-SEMFromLavaan.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".SEMFromLavaan — .SEMFromLavaan","text":"","code":".SEMFromLavaan(   lavaanModel,   whichPars = \"est\",   fit = TRUE,   addMeans = TRUE,   activeSet = NULL,   dataSet = NULL,   transformations = NULL,   transformationList = list(),   transformationGradientStepSize = 1e-06 )"},{"path":"/reference/dot-SEMFromLavaan.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".SEMFromLavaan — .SEMFromLavaan","text":"lavaanModel model class lavaan whichPars parameters used initialize model. set \"est\", parameters set estimated parameters lavaan model. set \"start\", starting values lavaan used. latter can useful parameters optimized afterwards setting parameters \"est\" may result model getting stuck local minimum. fit model fitted compared lavaanModel? addMeans lavaanModel meanstructure = FALSE, addMeans = TRUE add mean structure. FALSE set means observed variables average activeSet Option use subset individuals data set. Logical vector length N indicating subjects remain sample. dataSet optional: Pass alternative data set lessSEM:::.SEMFromLavaan replace original data set lavaanModel. transformationGradientStepSize step size used compute gradients transformations","code":""},{"path":"/reference/dot-SEMFromLavaan.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".SEMFromLavaan — .SEMFromLavaan","text":"Object class Rcpp_SEMCpp","code":""},{"path":"/reference/dot-SEMdata.html","id":null,"dir":"Reference","previous_headings":"","what":".SEMdata — .SEMdata","title":".SEMdata — .SEMdata","text":"internal function. Creates internal data representation","code":""},{"path":"/reference/dot-SEMdata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".SEMdata — .SEMdata","text":"","code":".SEMdata(rawData)"},{"path":"/reference/dot-SEMdata.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".SEMdata — .SEMdata","text":"rawData matrix raw data set","code":""},{"path":"/reference/dot-SEMdata.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".SEMdata — .SEMdata","text":"list internal representation data","code":""},{"path":"/reference/dot-addMeanStructure.html","id":null,"dir":"Reference","previous_headings":"","what":".addMeanStructure — .addMeanStructure","title":".addMeanStructure — .addMeanStructure","text":"adds mean strucuture parameter table","code":""},{"path":"/reference/dot-addMeanStructure.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".addMeanStructure — .addMeanStructure","text":"","code":".addMeanStructure(parameterTable, manifestNames, MvectorElements)"},{"path":"/reference/dot-addMeanStructure.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".addMeanStructure — .addMeanStructure","text":"parameterTable table parameters manifestNames names manifest variables MvectorElements elements means vector","code":""},{"path":"/reference/dot-addMeanStructure.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".addMeanStructure — .addMeanStructure","text":"parameterTable","code":""},{"path":"/reference/dot-checkLavaanModel.html","id":null,"dir":"Reference","previous_headings":"","what":".checkLavaanModel — .checkLavaanModel","title":".checkLavaanModel — .checkLavaanModel","text":"checks model type lavaan","code":""},{"path":"/reference/dot-checkLavaanModel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".checkLavaanModel — .checkLavaanModel","text":"","code":".checkLavaanModel(lavaanModel)"},{"path":"/reference/dot-checkLavaanModel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".checkLavaanModel — .checkLavaanModel","text":"lavaanModel m0del type lavaan","code":""},{"path":"/reference/dot-checkLavaanModel.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".checkLavaanModel — .checkLavaanModel","text":"nothing","code":""},{"path":"/reference/dot-checkPenalties.html","id":null,"dir":"Reference","previous_headings":"","what":".checkPenalties — .checkPenalties","title":".checkPenalties — .checkPenalties","text":"Internal function check mixedPenalty object","code":""},{"path":"/reference/dot-checkPenalties.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".checkPenalties — .checkPenalties","text":"","code":".checkPenalties(mixedPenalty)"},{"path":"/reference/dot-checkPenalties.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".checkPenalties — .checkPenalties","text":"mixedPenalty object class mixedPenalty. object can created mixedPenalty function. Penalties can added addCappedL1, addLasso, addLsp, addMcp, addScad functions.","code":""},{"path":"/reference/dot-compileTransformations.html","id":null,"dir":"Reference","previous_headings":"","what":".compileTransformations — .compileTransformations","title":".compileTransformations — .compileTransformations","text":"compile user defined parameter transformations pass SEM","code":""},{"path":"/reference/dot-compileTransformations.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".compileTransformations — .compileTransformations","text":"","code":".compileTransformations(syntax, parameterLabels, compile = TRUE)"},{"path":"/reference/dot-compileTransformations.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".compileTransformations — .compileTransformations","text":"syntax string user defined transformations parameterLabels names parameters model compile set FALSE, function compiled -> visual inspection","code":""},{"path":"/reference/dot-compileTransformations.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".compileTransformations — .compileTransformations","text":"list parameter names two Rcpp functions: (1) transformation function (2) function create pointer transformation function. starting values defined, returned well.","code":""},{"path":"/reference/dot-computeInitialHessian.html","id":null,"dir":"Reference","previous_headings":"","what":".computeInitialHessian — .computeInitialHessian","title":".computeInitialHessian — .computeInitialHessian","text":"computes initial Hessian used optimization. use parameter estimates lavaan starting values, typcially makes sense just use Hessian lavaan model initial Hessian","code":""},{"path":"/reference/dot-computeInitialHessian.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".computeInitialHessian — .computeInitialHessian","text":"","code":".computeInitialHessian(   initialHessian,   rawParameters,   lavaanModel,   SEM,   addMeans )"},{"path":"/reference/dot-computeInitialHessian.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".computeInitialHessian — .computeInitialHessian","text":"initialHessian option provide initial Hessian optimizer. Must row column names corresponding parameter labels. use getLavaanParameters(lavaanModel) see labels. set \"scoreBased\", outer product scores used approximation (see https://en.wikipedia.org/wiki/Berndt%E2%80%93Hall%E2%80%93Hall%E2%80%93Hausman_algorithm). set \"compute\", initial hessian computed. set single value, diagonal matrix single value along diagonal used. default \"lavaan\" extracts Hessian lavaanModel. Hessian typically deviate internal SEM represenation lessSEM (due transformation variances), works quite well practice. rawParameters vector raw parameters lavaanModel lavaan model object SEM internal SEM representation addMeans mean structure added model?","code":""},{"path":"/reference/dot-computeInitialHessian.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".computeInitialHessian — .computeInitialHessian","text":"Hessian matrix","code":""},{"path":"/reference/dot-createMultiGroupTransformations.html","id":null,"dir":"Reference","previous_headings":"","what":".createMultiGroupTransformations — .createMultiGroupTransformations","title":".createMultiGroupTransformations — .createMultiGroupTransformations","text":"compiles transformation function adapts parameter vector","code":""},{"path":"/reference/dot-createMultiGroupTransformations.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".createMultiGroupTransformations — .createMultiGroupTransformations","text":"","code":".createMultiGroupTransformations(transformations, parameterValues)"},{"path":"/reference/dot-createMultiGroupTransformations.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".createMultiGroupTransformations — .createMultiGroupTransformations","text":"transformations string transformations parameterValues values parameters already model","code":""},{"path":"/reference/dot-createMultiGroupTransformations.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".createMultiGroupTransformations — .createMultiGroupTransformations","text":"list extended parameter vector transformation function pointer","code":""},{"path":"/reference/dot-createParameterTable.html","id":null,"dir":"Reference","previous_headings":"","what":".createParameterTable — .createParameterTable","title":".createParameterTable — .createParameterTable","text":"create parameter table using elements extracted lavaan","code":""},{"path":"/reference/dot-createParameterTable.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".createParameterTable — .createParameterTable","text":"","code":".createParameterTable(   parameterValues,   parameterLabels,   modelParameters,   parameterIDs )"},{"path":"/reference/dot-createParameterTable.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".createParameterTable — .createParameterTable","text":"parameterValues values parameters parameterLabels names parameters modelParameters model parameters lavaan parameterIDs unique parameter ids lavaan -> identify parameter unique number","code":""},{"path":"/reference/dot-createParameterTable.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".createParameterTable — .createParameterTable","text":"parameter table lessSEM","code":""},{"path":"/reference/dot-createRcppTransformationFunction.html","id":null,"dir":"Reference","previous_headings":"","what":".createRcppTransformationFunction — .createRcppTransformationFunction","title":".createRcppTransformationFunction — .createRcppTransformationFunction","text":"create Rcpp function uses user-defined parameter transformation","code":""},{"path":"/reference/dot-createRcppTransformationFunction.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".createRcppTransformationFunction — .createRcppTransformationFunction","text":"","code":".createRcppTransformationFunction(syntax, parameters)"},{"path":"/reference/dot-createRcppTransformationFunction.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".createRcppTransformationFunction — .createRcppTransformationFunction","text":"syntax syntax user defined transformations parameters labels parameters used transformations","code":""},{"path":"/reference/dot-createRcppTransformationFunction.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".createRcppTransformationFunction — .createRcppTransformationFunction","text":"string functions compilations Rcpp","code":""},{"path":"/reference/dot-createTransformations.html","id":null,"dir":"Reference","previous_headings":"","what":".createTransformations — .createTransformations","title":".createTransformations — .createTransformations","text":"compiles transformation function adapts parameterTable","code":""},{"path":"/reference/dot-createTransformations.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".createTransformations — .createTransformations","text":"","code":".createTransformations(transformations, parameterLabels, parameterTable)"},{"path":"/reference/dot-createTransformations.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".createTransformations — .createTransformations","text":"transformations string transformations parameterLabels labels parameteres already model parameterTable existing parameter table","code":""},{"path":"/reference/dot-createTransformations.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".createTransformations — .createTransformations","text":"list parameterTable transformation function pointer","code":""},{"path":"/reference/dot-cvRegularizeSEMInternal.html","id":null,"dir":"Reference","previous_headings":"","what":".cvRegularizeSEMInternal — .cvRegularizeSEMInternal","title":".cvRegularizeSEMInternal — .cvRegularizeSEMInternal","text":"Combination regularized structural equation model cross-validation","code":""},{"path":"/reference/dot-cvRegularizeSEMInternal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".cvRegularizeSEMInternal — .cvRegularizeSEMInternal","text":"","code":".cvRegularizeSEMInternal(   lavaanModel,   k,   standardize,   penalty,   weights,   returnSubsetParameters,   tuningParameters,   method,   modifyModel,   control )"},{"path":"/reference/dot-cvRegularizeSEMInternal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".cvRegularizeSEMInternal — .cvRegularizeSEMInternal","text":"lavaanModel model class lavaan k number cross-validation folds. Alternatively, matrix pre-defined subsets can passed function. See ?lessSEM::cvLasso example standardize training test sets standardized? penalty string: name penalty used model weights labeled vector weights parameters model. returnSubsetParameters set TRUE, parameter estimates individual cross-validation training sets returned tuningParameters data.frame tuning parameter values method optimizer used? Currently implemented ista glmnet. ista, control argument can used switch related procedures (currently gist). modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. element generated controlIsta() controlGlmnet() functions.","code":""},{"path":"/reference/dot-cvRegularizeSEMInternal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".cvRegularizeSEMInternal — .cvRegularizeSEMInternal","text":"model class cvRegularizedSEM","code":""},{"path":"/reference/dot-cvRegularizeSEMInternal.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":".cvRegularizeSEMInternal — .cvRegularizeSEMInternal","text":"Internal function: function computes regularized models penalty functions implemented glmnet gist. Use dedicated penalty functions (e.g., lessSEM::cvLasso) penalize model.","code":""},{"path":"/reference/dot-cvRegularizeSmoothSEMInternal.html","id":null,"dir":"Reference","previous_headings":"","what":".cvRegularizeSmoothSEMInternal — .cvRegularizeSmoothSEMInternal","title":".cvRegularizeSmoothSEMInternal — .cvRegularizeSmoothSEMInternal","text":"Combination smoothly regularized structural equation model cross-validation","code":""},{"path":"/reference/dot-cvRegularizeSmoothSEMInternal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".cvRegularizeSmoothSEMInternal — .cvRegularizeSmoothSEMInternal","text":"","code":".cvRegularizeSmoothSEMInternal(   lavaanModel,   k,   standardize,   penalty,   weights,   returnSubsetParameters,   tuningParameters,   epsilon,   modifyModel,   method = \"bfgs\",   control )"},{"path":"/reference/dot-cvRegularizeSmoothSEMInternal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".cvRegularizeSmoothSEMInternal — .cvRegularizeSmoothSEMInternal","text":"lavaanModel model class lavaan k number cross-validation folds. Alternatively, matrix pre-defined subsets can passed function. See ?lessSEM::cvSmoothLasso example standardize training test sets standardized? penalty string: name penalty used model weights labeled vector weights parameters model. returnSubsetParameters set TRUE, parameter estimates individual cross-validation training sets returned tuningParameters data.frame tuning parameter values epsilon epsilon > 0; controls smoothness approximation. Larger values = smoother modifyModel used modify lavaanModel. See ?modifyModel. method optimizer used. Currently \"bfgs\" supported. control used control optimizer. element generated controlBFGS function. See ?controlBFGS details.","code":""},{"path":"/reference/dot-cvRegularizeSmoothSEMInternal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".cvRegularizeSmoothSEMInternal — .cvRegularizeSmoothSEMInternal","text":"model class cvRegularizedSEM","code":""},{"path":"/reference/dot-cvRegularizeSmoothSEMInternal.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":".cvRegularizeSmoothSEMInternal — .cvRegularizeSmoothSEMInternal","text":"Internal function: function computes regularized models penalty functions implemented bfgs. Use dedicated penalty functions (e.g., lessSEM::cvSmoothLasso) penalize model.","code":""},{"path":"/reference/dot-cvregsem2LavaanParameters.html","id":null,"dir":"Reference","previous_headings":"","what":".cvregsem2LavaanParameters — .cvregsem2LavaanParameters","title":".cvregsem2LavaanParameters — .cvregsem2LavaanParameters","text":"helper function: regsem lavaan use slightly different parameter labels. function can used translate parameter labels cv_regsem object lavaan labels","code":""},{"path":"/reference/dot-cvregsem2LavaanParameters.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".cvregsem2LavaanParameters — .cvregsem2LavaanParameters","text":"","code":".cvregsem2LavaanParameters(cvregsemModel, lavaanModel)"},{"path":"/reference/dot-cvregsem2LavaanParameters.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".cvregsem2LavaanParameters — .cvregsem2LavaanParameters","text":"cvregsemModel model class cvregsem lavaanModel model class lavaan","code":""},{"path":"/reference/dot-cvregsem2LavaanParameters.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".cvregsem2LavaanParameters — .cvregsem2LavaanParameters","text":"regsem parameters lavaan labels","code":""},{"path":"/reference/dot-defineDerivatives.html","id":null,"dir":"Reference","previous_headings":"","what":".defineDerivatives — .defineDerivatives","title":".defineDerivatives — .defineDerivatives","text":"adds elements required compute derivatives fitting function respect parameters SEMList","code":""},{"path":"/reference/dot-defineDerivatives.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".defineDerivatives — .defineDerivatives","text":"","code":".defineDerivatives(SEMList, parameterTable, modelMatrices)"},{"path":"/reference/dot-defineDerivatives.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".defineDerivatives — .defineDerivatives","text":"SEMList list representing SEM parameterTable table parameters modelMatrices matrices RAM model","code":""},{"path":"/reference/dot-defineDerivatives.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".defineDerivatives — .defineDerivatives","text":"SEMList","code":""},{"path":"/reference/dot-extractParametersFromSyntax.html","id":null,"dir":"Reference","previous_headings":"","what":".extractParametersFromSyntax — .extractParametersFromSyntax","title":".extractParametersFromSyntax — .extractParametersFromSyntax","text":"extract names parameters syntax","code":""},{"path":"/reference/dot-extractParametersFromSyntax.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".extractParametersFromSyntax — .extractParametersFromSyntax","text":"","code":".extractParametersFromSyntax(syntax, parameterLabels)"},{"path":"/reference/dot-extractParametersFromSyntax.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".extractParametersFromSyntax — .extractParametersFromSyntax","text":"syntax syntax parameter transformations parameterLabels names parameters model","code":""},{"path":"/reference/dot-extractParametersFromSyntax.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".extractParametersFromSyntax — .extractParametersFromSyntax","text":"vector names parameters used syntax vector boolean indicating parameter transformation result","code":""},{"path":"/reference/dot-extractSEMFromLavaan.html","id":null,"dir":"Reference","previous_headings":"","what":".extractSEMFromLavaan — .extractSEMFromLavaan","title":".extractSEMFromLavaan — .extractSEMFromLavaan","text":"internal function. Translates object class lavaan internal model representation.","code":""},{"path":"/reference/dot-extractSEMFromLavaan.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".extractSEMFromLavaan — .extractSEMFromLavaan","text":"","code":".extractSEMFromLavaan(   lavaanModel,   whichPars = \"est\",   fit = TRUE,   addMeans = TRUE,   activeSet = NULL,   dataSet = NULL,   transformations = NULL )"},{"path":"/reference/dot-extractSEMFromLavaan.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".extractSEMFromLavaan — .extractSEMFromLavaan","text":"lavaanModel model class lavaan whichPars parameters used initialize model. set \"est\", parameters set estimated parameters lavaan model. set \"start\", starting values lavaan used. latter can useful parameters optimized afterwards setting parameters \"est\" may result model getting stuck local minimum. fit model fitted compared lavaanModel? addMeans lavaanModel meanstructure = FALSE, addMeans = TRUE add mean structure. FALSE set means observed variables average activeSet Option use subset individuals data set. Logical vector length N indicating subjects remain sample. dataSet optional: Pass alternative data set lessSEM:::.SEMFromLavaan replace original data set lavaanModel. transformations optional: transform parameter values.","code":""},{"path":"/reference/dot-extractSEMFromLavaan.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".extractSEMFromLavaan — .extractSEMFromLavaan","text":"list SEMList (model RAM representation) fit (boolean indicating model fit compared lavaan)","code":""},{"path":"/reference/dot-fit.html","id":null,"dir":"Reference","previous_headings":"","what":".fit — .fit","title":".fit — .fit","text":"fits object class Rcpp_SEMCpp.","code":""},{"path":"/reference/dot-fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".fit — .fit","text":"","code":".fit(SEM)"},{"path":"/reference/dot-fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".fit — .fit","text":"SEM model class Rcpp_SEMCpp.","code":""},{"path":"/reference/dot-fit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".fit — .fit","text":"fitted SEM","code":""},{"path":"/reference/dot-fitFunction.html","id":null,"dir":"Reference","previous_headings":"","what":".fitFunction — .fitFunction","title":".fitFunction — .fitFunction","text":"internal function returns -2 log Likelihood object class Rcpp_SEMCpp. function can used optimizers","code":""},{"path":"/reference/dot-fitFunction.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".fitFunction — .fitFunction","text":"","code":".fitFunction(par, SEM, raw)"},{"path":"/reference/dot-fitFunction.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".fitFunction — .fitFunction","text":"par labeled vector parameter values SEM model class Rcpp_SEMCpp. raw controls internal transformations lessSEM used.","code":""},{"path":"/reference/dot-fitFunction.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".fitFunction — .fitFunction","text":"-2log-Likelihood","code":""},{"path":"/reference/dot-fitGlmnet.html","id":null,"dir":"Reference","previous_headings":"","what":".fitGlmnet — .fitGlmnet","title":".fitGlmnet — .fitGlmnet","text":"Optimizes object mixed penalty. See ?mixedPenalty details.","code":""},{"path":"/reference/dot-fitGlmnet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".fitGlmnet — .fitGlmnet","text":"","code":".fitGlmnet(mixedPenalty)"},{"path":"/reference/dot-fitGlmnet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".fitGlmnet — .fitGlmnet","text":"mixedPenalty object class mixedPenalty. object can created mixedPenalty function. Penalties can added addCappedL1, addElastiNet, addLasso, addLsp, addMcp, addScad functions.","code":""},{"path":"/reference/dot-fitIsta.html","id":null,"dir":"Reference","previous_headings":"","what":".fitIsta — .fitIsta","title":".fitIsta — .fitIsta","text":"Optimizes object mixed penalty. See ?mixedPenalty details.","code":""},{"path":"/reference/dot-fitIsta.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".fitIsta — .fitIsta","text":"","code":".fitIsta(mixedPenalty)"},{"path":"/reference/dot-fitIsta.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".fitIsta — .fitIsta","text":"mixedPenalty object class mixedPenalty. object can created mixedPenalty function. Penalties can added addCappedL1, addElastiNet, addLasso, addLsp, addMcp, addScad functions.","code":""},{"path":"/reference/dot-getGradients.html","id":null,"dir":"Reference","previous_headings":"","what":".getGradients — .getGradients","title":".getGradients — .getGradients","text":"returns gradients model class Rcpp_SEMCpp. internal model representation. Models class can generated lessSEM:::.SEMFromLavaan-function.","code":""},{"path":"/reference/dot-getGradients.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".getGradients — .getGradients","text":"","code":".getGradients(SEM, raw)"},{"path":"/reference/dot-getGradients.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".getGradients — .getGradients","text":"SEM model class Rcpp_SEMCpp raw controls internal transformations lessSEM used. lessSEM use exponential function variances avoid negative variances. set TRUE, gradients given internal parameter representation. Set FALSE get usual gradients","code":""},{"path":"/reference/dot-getGradients.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".getGradients — .getGradients","text":"vector derivatives -2log-Likelihood respect parameter","code":""},{"path":"/reference/dot-getHessian.html","id":null,"dir":"Reference","previous_headings":"","what":".getHessian — .getHessian","title":".getHessian — .getHessian","text":"returns Hessian model class Rcpp_SEMCpp. internal model representation. Models class can generated lessSEM:::.SEMFromLavaan-function. function adapted lavaan::lav_model_hessian.","code":""},{"path":"/reference/dot-getHessian.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".getHessian — .getHessian","text":"","code":".getHessian(SEM, raw, eps = 1e-07)"},{"path":"/reference/dot-getHessian.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".getHessian — .getHessian","text":"SEM model class Rcpp_SEMCpp raw controls internal transformations lessSEM used. lessSEM use exponential function variances avoid negative variances. set TRUE, gradients given internal parameter representation. Set FALSE get usual gradients eps eps controls step size numerical approximation.","code":""},{"path":"/reference/dot-getHessian.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".getHessian — .getHessian","text":"matrix second derivatives -2log-Likelihood respect parameter","code":""},{"path":"/reference/dot-getMaxLambda_C.html","id":null,"dir":"Reference","previous_headings":"","what":".getMaxLambda_C — .getMaxLambda_C","title":".getMaxLambda_C — .getMaxLambda_C","text":"generates first lambda sets regularized parameters zero","code":""},{"path":"/reference/dot-getMaxLambda_C.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".getMaxLambda_C — .getMaxLambda_C","text":"","code":".getMaxLambda_C(   regularizedModel,   SEM,   rawParameters,   weights,   N,   approx = FALSE )"},{"path":"/reference/dot-getMaxLambda_C.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".getMaxLambda_C — .getMaxLambda_C","text":"regularizedModel Model combining likelihood lasso type penalty SEM model class Rcpp_SEMCpp rawParameters labeled vector starting values weights weights given parameter penalty function N sample size approx set TRUE, .Machine$double.xmax^(.01) used instead .Machine$double.xmax^(.05)","code":""},{"path":"/reference/dot-getMaxLambda_C.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".getMaxLambda_C — .getMaxLambda_C","text":"first lambda value sets regularized parameters zero (plus tolerance)","code":""},{"path":"/reference/dot-getParameters.html","id":null,"dir":"Reference","previous_headings":"","what":".getParameters — .getParameters","title":".getParameters — .getParameters","text":"returns parameters internal model representation.","code":""},{"path":"/reference/dot-getParameters.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".getParameters — .getParameters","text":"","code":".getParameters(SEM, raw = FALSE, transformations = FALSE)"},{"path":"/reference/dot-getParameters.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".getParameters — .getParameters","text":"SEM model class Rcpp_SEMCpp. Models class raw controls parameter returned raw format transformed transformations transformed parameters included?","code":""},{"path":"/reference/dot-getParameters.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".getParameters — .getParameters","text":"labeled vector parameter values","code":""},{"path":"/reference/dot-getRawData.html","id":null,"dir":"Reference","previous_headings":"","what":".getRawData — .getRawData","title":".getRawData — .getRawData","text":"Extracts raw data lavaan adapts user supplied data set structure lavaan data","code":""},{"path":"/reference/dot-getRawData.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".getRawData — .getRawData","text":"","code":".getRawData(lavaanModel, dataSet)"},{"path":"/reference/dot-getRawData.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".getRawData — .getRawData","text":"lavaanModel model fitted lavaan dataSet user supplied data set","code":""},{"path":"/reference/dot-getRawData.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".getRawData — .getRawData","text":"raw data","code":""},{"path":"/reference/dot-getScores.html","id":null,"dir":"Reference","previous_headings":"","what":".getScores — .getScores","title":".getScores — .getScores","text":"returns scores model class Rcpp_SEMCpp. internal model representation. Models class can generated lessSEM:::.SEMFromLavaan-function.","code":""},{"path":"/reference/dot-getScores.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".getScores — .getScores","text":"","code":".getScores(SEM, raw)"},{"path":"/reference/dot-getScores.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".getScores — .getScores","text":"SEM model class Rcpp_SEMCpp raw controls internal transformations lessSEM used. lessSEM use exponential function variances avoid negative variances. set TRUE, scores given internal parameter representation. Set FALSE get usual scores","code":""},{"path":"/reference/dot-getScores.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".getScores — .getScores","text":"matrix derivatives -2log-Likelihood person parameter","code":""},{"path":"/reference/dot-gpGetMaxLambda.html","id":null,"dir":"Reference","previous_headings":"","what":".gpGetMaxLambda — .gpGetMaxLambda","title":".gpGetMaxLambda — .gpGetMaxLambda","text":"generates first lambda sets regularized parameters zero","code":""},{"path":"/reference/dot-gpGetMaxLambda.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".gpGetMaxLambda — .gpGetMaxLambda","text":"","code":".gpGetMaxLambda(   regularizedModel,   par,   fitFunction,   gradientFunction,   userSuppliedArguments,   weights )"},{"path":"/reference/dot-gpGetMaxLambda.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".gpGetMaxLambda — .gpGetMaxLambda","text":"regularizedModel Model combining likelihood lasso type penalty par labeled vector starting values fitFunction R fit function gradientFunction R gradient functions userSuppliedArguments list arguments fitFunction gradientFunction weights weights given parameter penalty function","code":""},{"path":"/reference/dot-gpGetMaxLambda.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".gpGetMaxLambda — .gpGetMaxLambda","text":"first lambda value sets regularized parameters zero (plus tolerance)","code":""},{"path":"/reference/dot-gpOptimizationInternal.html","id":null,"dir":"Reference","previous_headings":"","what":".gpOptimizationInternal — .gpOptimizationInternal","title":".gpOptimizationInternal — .gpOptimizationInternal","text":"Internal function: function computes regularized models penaltiy functions implemented glmnet gist. Use dedicated penalty functions (e.g., lessSEM::gpLasso) penalize model.","code":""},{"path":"/reference/dot-gpOptimizationInternal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".gpOptimizationInternal — .gpOptimizationInternal","text":"","code":".gpOptimizationInternal(   par,   weights,   fn,   gr = NULL,   additionalArguments,   isCpp = FALSE,   penalty,   tuningParameters,   method,   control )"},{"path":"/reference/dot-gpOptimizationInternal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".gpOptimizationInternal — .gpOptimizationInternal","text":"par labeled vector starting values weights labeled vector weights parameters model. fn R function takes parameters labels input returns fit value (single value) gr R function takes parameters labels input returns gradients objective function. set NULL, numDeriv used approximate gradients additionalArguments additional argument passed fn gr isCpp boolean: fn gr C++ function pointers? penalty string: name penalty used model tuningParameters data.frame tuning parameter values method optimizer used? Currently implemented ista glmnet. ista, control argument can used switch related procedures (currently gist). control used control optimizer. element generated controlIsta() controlGlmnet() functions.","code":""},{"path":"/reference/dot-gpOptimizationInternal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".gpOptimizationInternal — .gpOptimizationInternal","text":"Object class gpRegularized","code":""},{"path":"/reference/dot-gradientFunction.html","id":null,"dir":"Reference","previous_headings":"","what":".gradientFunction — .gradientFunction","title":".gradientFunction — .gradientFunction","text":"internal function returns gradients object class Rcpp_SEMCpp. function can used optimizers","code":""},{"path":"/reference/dot-gradientFunction.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".gradientFunction — .gradientFunction","text":"","code":".gradientFunction(par, SEM, raw)"},{"path":"/reference/dot-gradientFunction.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".gradientFunction — .gradientFunction","text":"par labeled vector parameter values SEM model class Rcpp_SEMCpp. raw controls internal transformations lessSEM used.","code":""},{"path":"/reference/dot-gradientFunction.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".gradientFunction — .gradientFunction","text":"gradients model","code":""},{"path":"/reference/dot-initializeMultiGroupSEMForRegularization.html","id":null,"dir":"Reference","previous_headings":"","what":".initializeMultiGroupSEMForRegularization — .initializeMultiGroupSEMForRegularization","title":".initializeMultiGroupSEMForRegularization — .initializeMultiGroupSEMForRegularization","text":"initializes internal C++ SEM regularization functions","code":""},{"path":"/reference/dot-initializeMultiGroupSEMForRegularization.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".initializeMultiGroupSEMForRegularization — .initializeMultiGroupSEMForRegularization","text":"","code":".initializeMultiGroupSEMForRegularization(   lavaanModels,   startingValues,   modifyModel )"},{"path":"/reference/dot-initializeMultiGroupSEMForRegularization.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".initializeMultiGroupSEMForRegularization — .initializeMultiGroupSEMForRegularization","text":"lavaanModels vector models class lavaan startingValues either set est, start, labeled vector starting values modifyModel user supplied model modifications","code":""},{"path":"/reference/dot-initializeMultiGroupSEMForRegularization.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".initializeMultiGroupSEMForRegularization — .initializeMultiGroupSEMForRegularization","text":"model used regularization procedure","code":""},{"path":"/reference/dot-initializeSEMForRegularization.html","id":null,"dir":"Reference","previous_headings":"","what":".initializeSEMForRegularization — .initializeSEMForRegularization","title":".initializeSEMForRegularization — .initializeSEMForRegularization","text":"initializes internal C++ SEM regularization functions","code":""},{"path":"/reference/dot-initializeSEMForRegularization.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".initializeSEMForRegularization — .initializeSEMForRegularization","text":"","code":".initializeSEMForRegularization(lavaanModel, startingValues, modifyModel)"},{"path":"/reference/dot-initializeSEMForRegularization.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".initializeSEMForRegularization — .initializeSEMForRegularization","text":"lavaanModel model class lavaan startingValues either set est, start, labeled vector starting values modifyModel user supplied model modifications","code":""},{"path":"/reference/dot-initializeSEMForRegularization.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".initializeSEMForRegularization — .initializeSEMForRegularization","text":"model used regularization procedure","code":""},{"path":"/reference/dot-initializeWeights.html","id":null,"dir":"Reference","previous_headings":"","what":".initializeWeights — .initializeWeights","title":".initializeWeights — .initializeWeights","text":"initialize adaptive lasso weights","code":""},{"path":"/reference/dot-initializeWeights.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".initializeWeights — .initializeWeights","text":"","code":".initializeWeights(   weights,   penalty,   method,   createAdaptiveLassoWeights,   control,   lavaanModel,   modifyModel,   startingValues,   rawParameters )"},{"path":"/reference/dot-initializeWeights.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".initializeWeights — .initializeWeights","text":"weights weight argument passed function penalty penalty used createAdaptiveLassoWeights adaptive lasso weights created? control list control elements optimizer lavaanModel model type lavaan modifyModel list model modifications startingValues either set est, start, labeled vector starting values rawParameters raw parameters","code":""},{"path":"/reference/dot-lavaan2regsemLabels.html","id":null,"dir":"Reference","previous_headings":"","what":".lavaan2regsemLabels — .lavaan2regsemLabels","title":".lavaan2regsemLabels — .lavaan2regsemLabels","text":"helper function: regsem lavaan use slightly different parameter labels. function can used get sets labels.","code":""},{"path":"/reference/dot-lavaan2regsemLabels.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".lavaan2regsemLabels — .lavaan2regsemLabels","text":"","code":".lavaan2regsemLabels(lavaanModel)"},{"path":"/reference/dot-lavaan2regsemLabels.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".lavaan2regsemLabels — .lavaan2regsemLabels","text":"lavaanModel model class lavaan","code":""},{"path":"/reference/dot-lavaan2regsemLabels.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".lavaan2regsemLabels — .lavaan2regsemLabels","text":"list lavaan regsem labels","code":""},{"path":"/reference/dot-likelihoodRatioFit.html","id":null,"dir":"Reference","previous_headings":"","what":".likelihoodRatioFit — .likelihoodRatioFit","title":".likelihoodRatioFit — .likelihoodRatioFit","text":"internal function returns likelihood ratio fit statistic","code":""},{"path":"/reference/dot-likelihoodRatioFit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".likelihoodRatioFit — .likelihoodRatioFit","text":"","code":".likelihoodRatioFit(par, SEM, raw)"},{"path":"/reference/dot-likelihoodRatioFit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".likelihoodRatioFit — .likelihoodRatioFit","text":"par labeled vector parameter values SEM model class Rcpp_SEMCpp. raw controls internal transformations lessSEM used.","code":""},{"path":"/reference/dot-likelihoodRatioFit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".likelihoodRatioFit — .likelihoodRatioFit","text":"likelihood ratio fit statistic","code":""},{"path":"/reference/dot-makeSingleLine.html","id":null,"dir":"Reference","previous_headings":"","what":".makeSingleLine — .makeSingleLine","title":".makeSingleLine — .makeSingleLine","text":"checks parameter: start: statement spans multiple lines reduces one line.","code":""},{"path":"/reference/dot-makeSingleLine.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".makeSingleLine — .makeSingleLine","text":"","code":".makeSingleLine(syntax, what)"},{"path":"/reference/dot-makeSingleLine.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".makeSingleLine — .makeSingleLine","text":"syntax reduced syntax statement look (parameters start)","code":""},{"path":"/reference/dot-makeSingleLine.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".makeSingleLine — .makeSingleLine","text":"syntax multi-line statements condensed one line","code":""},{"path":"/reference/dot-multiGroupSEMFromLavaan.html","id":null,"dir":"Reference","previous_headings":"","what":".multiGroupSEMFromLavaan — .multiGroupSEMFromLavaan","title":".multiGroupSEMFromLavaan — .multiGroupSEMFromLavaan","text":"internal function. Translates vector objects class lavaan internal model representation.","code":""},{"path":"/reference/dot-multiGroupSEMFromLavaan.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".multiGroupSEMFromLavaan — .multiGroupSEMFromLavaan","text":"","code":".multiGroupSEMFromLavaan(   lavaanModels,   whichPars = \"est\",   fit = TRUE,   addMeans = TRUE,   transformations = NULL,   transformationList = list(),   transformationGradientStepSize = 1e-06 )"},{"path":"/reference/dot-multiGroupSEMFromLavaan.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".multiGroupSEMFromLavaan — .multiGroupSEMFromLavaan","text":"lavaanModels vector lavaan models whichPars parameters used initialize model. set \"est\", parameters set estimated parameters lavaan model. set \"start\", starting values lavaan used. latter can useful parameters optimized afterwards setting parameters \"est\" may result model getting stuck local minimum. fit model fitted addMeans lavaanModel meanstructure = FALSE, addMeans = TRUE add mean structure. FALSE set means observed variables average transformations string transformations transformationList list transformations transformationGradientStepSize step size used compute gradients transformations","code":""},{"path":"/reference/dot-multiGroupSEMFromLavaan.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".multiGroupSEMFromLavaan — .multiGroupSEMFromLavaan","text":"Object class Rcpp_mgSEMCpp","code":""},{"path":"/reference/dot-noDotDotDot.html","id":null,"dir":"Reference","previous_headings":"","what":".noDotDotDot — .noDotDotDot","title":".noDotDotDot — .noDotDotDot","text":"remplaces dot dot dot part fitting gradient fuction","code":""},{"path":"/reference/dot-noDotDotDot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".noDotDotDot — .noDotDotDot","text":"","code":".noDotDotDot(fn, fnName, ...)"},{"path":"/reference/dot-noDotDotDot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".noDotDotDot — .noDotDotDot","text":"fn fit gradient function. IMPORTANT: FIRST ARGUMENT FUNCTION MUST PARAMETER VECTOR fnName name function fn ... additional arguments","code":""},{"path":"/reference/dot-noDotDotDot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".noDotDotDot — .noDotDotDot","text":"list (1) new function wraps fn (2) list arguments passed fn","code":""},{"path":"/reference/dot-optNLMINB.html","id":null,"dir":"Reference","previous_headings":"","what":"optNLMINB — .optNLMINB","title":"optNLMINB — .optNLMINB","text":"Optimize SEM nlminb","code":""},{"path":"/reference/dot-optNLMINB.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"optNLMINB — .optNLMINB","text":"","code":".optNLMINB(SEM)"},{"path":"/reference/dot-optNLMINB.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"optNLMINB — .optNLMINB","text":"SEM SEMCpp model","code":""},{"path":"/reference/dot-optNLMINB.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"optNLMINB — .optNLMINB","text":"Optimized SEM","code":""},{"path":"/reference/dot-penaltyTypes.html","id":null,"dir":"Reference","previous_headings":"","what":".penaltyTypes — .penaltyTypes","title":".penaltyTypes — .penaltyTypes","text":"translates penalty numeric value character character numeric value. numeric value used C++ backend.","code":""},{"path":"/reference/dot-penaltyTypes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".penaltyTypes — .penaltyTypes","text":"","code":".penaltyTypes(penalty)"},{"path":"/reference/dot-penaltyTypes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".penaltyTypes — .penaltyTypes","text":"penalty either number name penalty","code":""},{"path":"/reference/dot-printNote.html","id":null,"dir":"Reference","previous_headings":"","what":".printNote — .printNote","title":".printNote — .printNote","text":"Prints note.","code":""},{"path":"/reference/dot-printNote.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".printNote — .printNote","text":"","code":".printNote(...)"},{"path":"/reference/dot-printNote.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".printNote — .printNote","text":"... one multiple strings","code":""},{"path":"/reference/dot-printNote.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".printNote — .printNote","text":"nothing","code":""},{"path":"/reference/dot-reduceSyntax.html","id":null,"dir":"Reference","previous_headings":"","what":".reduceSyntax — .reduceSyntax","title":".reduceSyntax — .reduceSyntax","text":"reduce user defined parameter transformation syntax basic elements","code":""},{"path":"/reference/dot-reduceSyntax.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".reduceSyntax — .reduceSyntax","text":"","code":".reduceSyntax(syntax)"},{"path":"/reference/dot-reduceSyntax.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".reduceSyntax — .reduceSyntax","text":"syntax string user defined transformations","code":""},{"path":"/reference/dot-reduceSyntax.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".reduceSyntax — .reduceSyntax","text":"cut simplified version syntax","code":""},{"path":"/reference/dot-regularizeSEMInternal.html","id":null,"dir":"Reference","previous_headings":"","what":".regularizeSEMInternal — .regularizeSEMInternal","title":".regularizeSEMInternal — .regularizeSEMInternal","text":"Internal function: function computes regularized models penaltiy functions implemented glmnet gist. Use dedicated penalty functions (e.g., lessSEM::lasso) penalize model.","code":""},{"path":"/reference/dot-regularizeSEMInternal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".regularizeSEMInternal — .regularizeSEMInternal","text":"","code":".regularizeSEMInternal(   lavaanModel,   penalty,   weights,   tuningParameters,   method,   modifyModel,   control )"},{"path":"/reference/dot-regularizeSEMInternal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".regularizeSEMInternal — .regularizeSEMInternal","text":"lavaanModel model class lavaan penalty string: name penalty used model weights labeled vector weights parameters model. tuningParameters data.frame tuning parameter values method optimizer used? Currently implemented ista glmnet. ista, control argument can used switch related procedures (currently gist). modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. element generated controlIsta() controlGlmnet() functions.","code":""},{"path":"/reference/dot-regularizeSEMWithCustomPenaltyRsolnp.html","id":null,"dir":"Reference","previous_headings":"","what":".regularizeSEMWithCustomPenaltyRsolnp — .regularizeSEMWithCustomPenaltyRsolnp","title":".regularizeSEMWithCustomPenaltyRsolnp — .regularizeSEMWithCustomPenaltyRsolnp","text":"Optimize SEM custom penalty function using Rsolnp optimizer (see ?Rsolnp::solnp). optimizer default regsem (see ?regsem::cv_regsem).","code":""},{"path":"/reference/dot-regularizeSEMWithCustomPenaltyRsolnp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".regularizeSEMWithCustomPenaltyRsolnp — .regularizeSEMWithCustomPenaltyRsolnp","text":"","code":".regularizeSEMWithCustomPenaltyRsolnp(   lavaanModel,   individualPenaltyFunction,   tuningParameters,   penaltyFunctionArguments,   startingValues = \"est\",   carryOverParameters = TRUE,   control = list(trace = 0) )"},{"path":"/reference/dot-regularizeSEMWithCustomPenaltyRsolnp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".regularizeSEMWithCustomPenaltyRsolnp — .regularizeSEMWithCustomPenaltyRsolnp","text":"lavaanModel model class lavaan individualPenaltyFunction penalty function takes current parameter values first argument, tuning parameters second, penaltyFunctionArguments third argument returns single value - value penalty function single person. true penalty function non-differentiable (e.g., lasso) smooth approximation function provided. tuningParameters data.frame tuning parameter values. Important: function iterate rows tuning parameters pass penalty function penaltyFunctionArguments arguments passed individualPenaltyFunction, individualPenaltyFunctionGradient, individualPenaltyFunctionHessian startingValues option provide initial starting values. used first lambda. Three options supported. Setting \"est\" use estimates lavaan model object. Setting \"start\" use starting values lavaan model. Finally, labeled vector parameter values can passed function used starting values. carryOverParameters parameters previous iteration used starting values next iteration? control option set parameters optimizer; see ?Rsolnp::solnp","code":""},{"path":"/reference/dot-regularizeSEMWithCustomPenaltyRsolnp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".regularizeSEMWithCustomPenaltyRsolnp — .regularizeSEMWithCustomPenaltyRsolnp","text":"Model class regularizedSEMWithCustomPenalty","code":""},{"path":"/reference/dot-regularizeSEMWithCustomPenaltyRsolnp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":".regularizeSEMWithCustomPenaltyRsolnp — .regularizeSEMWithCustomPenaltyRsolnp","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +      l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +      l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Optional: Plot the model # semPlot::semPaths(lavaanModel, #                   what = \"est\", #                   fade = FALSE)  ## Defining a custom penalty function is a bit more complicated than # using the default ones provided in regularizeSEM (see ?lessSEM::regularizeSEM). # We start with the definition of the penalty function. Make sure that the derivatives # of this function are well defined (i.e., the function is smooth)! # We will use the lasso penalty as an example.  # The penalty function MUST accept three arguments; how you use them is up to you.  # The first argument are the parameters. lessSEM will pass a vector with the current # parameter values and their names to your function. The parameter labels will be # exactly the same as those used by lavaan! So you can check them before with:  lessSEM::getLavaanParameters(lavaanModel)  # The vector passed to your function will look like the vector above.  # The second argument is called tuningParameters MUST be a data.frame with the # tuning-parameters. lessSEM will automatically iterate over the rows of this object. # In case of LASSO regularization there is only one tuning parameter: lambda. # Therefore, we specify the tuningParameters object as:  tuningParameters <- data.frame(lambda = seq(0,1,.1)) # we will test 11 lambdas here print(tuningParameters)  # The third argument is called penaltyFunctionArguments and we can pass anything # we want here. For the lasso penalty, we need two additional things: # 1) We need the smoothing-parameter epsilon, which makes sure that our # penalty is differentiable # 2) We need the labels of the regularized parameters, so that we can only # penalize those while all others remain unpenalized.  penaltyFunctionArguments <- list(   eps = 1e-10,   regularizedParameterLabels = paste0(\"l\", 6:15) )  # Now, it is time to specify our custom penalty function:  smoothLASSO <- function(   # here are our three arguments:   parameters,   tuningParameters,   penaltyFunctionArguments ){   # to make it easier to see what is going on:   lambda <- tuningParameters$lambda # tuningParameters will be ONE ROW OF the   # tuningParameters object we created before -> it will contain just   # one lambda in this case!   eps <- penaltyFunctionArguments$eps   regularizedParameterLabels <- penaltyFunctionArguments$regularizedParameterLabels    regularizedParameters <- parameters[regularizedParameterLabels]    # now, let's define our penalty function:   penaltyLasso <- lambda*sum(sqrt(regularizedParameters^2 + eps))    return(penaltyLasso) } # Important: This penalty function is assumed to be for a single individual only. # lessSEM will multiply it with sample size N to get the penalty value of the # full sample!  #### Now we are ready to optimize! #### regsemApprox <- lessSEM:::.regularizeSEMWithCustomPenaltyRsolnp(lavaanModel = lavaanModel,                                                individualPenaltyFunction = smoothLASSO,                                                tuningParameters = tuningParameters,                                                penaltyFunctionArguments = penaltyFunctionArguments)  # let's compare the results to an exact optimization:  regsemExact <- lasso(   lavaanModel = lavaanModel,   regularized = paste0(\"l\", 6:15),   lambdas = tuningParameters$lambda)  head(regsemExact@parameters[,regsemExact@parameterLabels] -        regsemApprox@parameters[,regsemExact@parameterLabels]) # Note that the parameter estimates are basically identical."},{"path":"/reference/dot-regularizeSmoothSEMInternal.html","id":null,"dir":"Reference","previous_headings":"","what":".regularizeSmoothSEMInternal — .regularizeSmoothSEMInternal","title":".regularizeSmoothSEMInternal — .regularizeSmoothSEMInternal","text":"Internal function: function computes regularized models smooth penalty functions implemented bfgs. Use dedicated penalty functions (e.g., lessSEM::smoothLasso) penalize model.","code":""},{"path":"/reference/dot-regularizeSmoothSEMInternal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".regularizeSmoothSEMInternal — .regularizeSmoothSEMInternal","text":"","code":".regularizeSmoothSEMInternal(   lavaanModel,   penalty,   weights,   tuningParameters,   epsilon,   tau,   method = \"bfgs\",   modifyModel,   control )"},{"path":"/reference/dot-regularizeSmoothSEMInternal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".regularizeSmoothSEMInternal — .regularizeSmoothSEMInternal","text":"lavaanModel model class lavaan penalty string: name penalty used model weights labeled vector weights parameters model. tuningParameters data.frame tuning parameter values epsilon epsilon > 0; controls smoothness approximation. Larger values = smoother tau parameters threshold tau seen zeroed method optimizer used. Currently \"bfgs\" supported. modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. element generated controlBFGS function. See ?controlBFGS details.","code":""},{"path":"/reference/dot-regularizeSmoothSEMInternal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".regularizeSmoothSEMInternal — .regularizeSmoothSEMInternal","text":"regularizedSEM","code":""},{"path":"/reference/dot-setAMatrix.html","id":null,"dir":"Reference","previous_headings":"","what":".setAMatrix — .setAMatrix","title":".setAMatrix — .setAMatrix","text":"internal function. Populates matrix directed effects RAM notation","code":""},{"path":"/reference/dot-setAMatrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".setAMatrix — .setAMatrix","text":"","code":".setAMatrix(   model,   lavaanParameterTable,   nLatent,   nManifest,   latentNames,   manifestNames )"},{"path":"/reference/dot-setAMatrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".setAMatrix — .setAMatrix","text":"model model class lavaan lavaanParameterTable parameter table lavaan nLatent number latent variables nManifest number manifest variables latentNames names latent variables manifestNames names manifest variables","code":""},{"path":"/reference/dot-setFmatrix.html","id":null,"dir":"Reference","previous_headings":"","what":".setFmatrix — .setFmatrix","title":".setFmatrix — .setFmatrix","text":"returns filter matrix RAM","code":""},{"path":"/reference/dot-setFmatrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".setFmatrix — .setFmatrix","text":"","code":".setFmatrix(nManifest, manifestNames, nLatent, latentNames)"},{"path":"/reference/dot-setFmatrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".setFmatrix — .setFmatrix","text":"nManifest number manifest variables manifestNames names manifest variables nLatent number latent variables latentNames names latent variables","code":""},{"path":"/reference/dot-setFmatrix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".setFmatrix — .setFmatrix","text":"matrix","code":""},{"path":"/reference/dot-setMVector.html","id":null,"dir":"Reference","previous_headings":"","what":".setMVector — .setMVector","title":".setMVector — .setMVector","text":"internal function. Populates vector means RAM notation","code":""},{"path":"/reference/dot-setMVector.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".setMVector — .setMVector","text":"","code":".setMVector(   model,   lavaanParameterTable,   nLatent,   nManifest,   latentNames,   manifestNames,   rawData )"},{"path":"/reference/dot-setMVector.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".setMVector — .setMVector","text":"model model class lavaan lavaanParameterTable parameter table lavaan nLatent number latent variables nManifest number manifest variables latentNames names latent variables manifestNames names manifest variables rawData matrix raw data","code":""},{"path":"/reference/dot-setParameters.html","id":null,"dir":"Reference","previous_headings":"","what":".setParameters — .setParameters","title":".setParameters — .setParameters","text":"change parameters internal model representation.","code":""},{"path":"/reference/dot-setParameters.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".setParameters — .setParameters","text":"","code":".setParameters(SEM, labels, values, raw)"},{"path":"/reference/dot-setParameters.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".setParameters — .setParameters","text":"SEM model class Rcpp_SEMCpp. Models class labels vector parameter labels values vector parameter values raw parameters given raw format transformed?","code":""},{"path":"/reference/dot-setParameters.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".setParameters — .setParameters","text":"SEM changed parameter values","code":""},{"path":"/reference/dot-setSMatrix.html","id":null,"dir":"Reference","previous_headings":"","what":".setSMatrix — .setSMatrix","title":".setSMatrix — .setSMatrix","text":"internal function. Populates matrix undirected paths RAM notation","code":""},{"path":"/reference/dot-setSMatrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".setSMatrix — .setSMatrix","text":"","code":".setSMatrix(   model,   lavaanParameterTable,   nLatent,   nManifest,   latentNames,   manifestNames )"},{"path":"/reference/dot-setSMatrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".setSMatrix — .setSMatrix","text":"model model class lavaan lavaanParameterTable parameter table lavaan nLatent number latent variables nManifest number manifest variables latentNames names latent variables manifestNames names manifest variables","code":""},{"path":"/reference/dot-standardErrors.html","id":null,"dir":"Reference","previous_headings":"","what":".standardErrors — .standardErrors","title":".standardErrors — .standardErrors","text":"compute standard errors fitted SEM. IMPORTANT: Assumes SEM fitted parameter estimates ordinary maximum likelihood estimates","code":""},{"path":"/reference/dot-standardErrors.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".standardErrors — .standardErrors","text":"","code":".standardErrors(SEM, raw)"},{"path":"/reference/dot-standardErrors.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".standardErrors — .standardErrors","text":"SEM model class Rcpp_SEMCpp. raw controls internal transformations lessSEM used. set TRUE, standard errors returned internally used parameter specification","code":""},{"path":"/reference/dot-standardErrors.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".standardErrors — .standardErrors","text":"vector standard errors","code":""},{"path":"/reference/elasticNet.html","id":null,"dir":"Reference","previous_headings":"","what":"elasticNet — elasticNet","title":"elasticNet — elasticNet","text":"Implements elastic net regularization structural equation models. penalty function given : $$p( x_j) = \\alpha\\lambda| x_j| + (1-\\alpha)\\lambda x_j^2$$ Note elastic net combines ridge lasso regularization. \\(\\alpha = 0\\), elastic net reduces ridge regularization. \\(\\alpha = 1\\) reduces lasso regularization. , elastic net compromise shrinkage lasso ridge penalty.","code":""},{"path":"/reference/elasticNet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"elasticNet — elasticNet","text":"","code":"elasticNet(   lavaanModel,   regularized,   lambdas,   alphas,   method = \"glmnet\",   modifyModel = lessSEM::modifyModel(),   control = lessSEM::controlGlmnet() )"},{"path":"/reference/elasticNet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"elasticNet — elasticNet","text":"lavaanModel model class lavaan regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda alphas numeric vector values tuning parameter alpha. Must 0 1. 0 = ridge, 1 = lasso. method optimizer used? Currently implemented ista glmnet. ista, control argument can used switch related procedures (currently gist). modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. element generated lessSEM::controlIsta() controlGlmnet() functions.","code":""},{"path":"/reference/elasticNet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"elasticNet — elasticNet","text":"Model class regularizedSEM","code":""},{"path":"/reference/elasticNet.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"elasticNet — elasticNet","text":"Identical regsem, models specified using lavaan. Currently, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. Elastic net regularization: Zou, H., & Hastie, T. (2005). Regularization variable selection via elastic net. Journal Royal Statistical Society: Series B, 67(2), 301–320. https://doi.org/10.1111/j.1467-9868.2005.00503.x Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classiﬁcation. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/elasticNet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"elasticNet — elasticNet","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +      l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +      l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Optional: Plot the model # semPlot::semPaths(lavaanModel, #                   what = \"est\", #                   fade = FALSE)  lsem <- elasticNet(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   # names of the regularized parameters:   regularized = paste0(\"l\", 6:15),   lambdas = seq(0,1,length.out = 20),   alphas = seq(0,1,.1))  # the coefficients can be accessed with: coef(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters[1,]  # optional: plotting the paths requires installation of plotly # plot(lsem)  #### Advanced ### # Switching the optimizer # # Use the \"method\" argument to switch the optimizer. The control argument # must also be changed to the corresponding function: lsemIsta <- elasticNet(   lavaanModel = lavaanModel,   regularized = paste0(\"l\", 6:15),   lambdas = seq(0,1,length.out = 20),   alphas = seq(0,1,.1),   method = \"ista\",   control = controlIsta())  # Note: The results are basically identical: lsemIsta@parameters - lsem@parameters"},{"path":"/reference/fit.html","id":null,"dir":"Reference","previous_headings":"","what":"fit — fit","title":"fit — fit","text":"Optimizes object mixed penalty. See ?mixedPenalty details.","code":""},{"path":"/reference/fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"fit — fit","text":"","code":"fit(mixedPenalty)"},{"path":"/reference/fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"fit — fit","text":"mixedPenalty object class mixedPenalty. object can created mixedPenalty function. Penalties can added addCappedL1, addElastiNet, addLasso, addLsp, addMcp, addScad functions.","code":""},{"path":"/reference/fit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"fit — fit","text":"throws error case undefined penalty combinations.","code":""},{"path":"/reference/genericGradientApproximiation.html","id":null,"dir":"Reference","previous_headings":"","what":"genericGradientApproximiation — genericGradientApproximiation","title":"genericGradientApproximiation — genericGradientApproximiation","text":"function can used approximate gradients generic penalty function numDeriv","code":""},{"path":"/reference/genericGradientApproximiation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"genericGradientApproximiation — genericGradientApproximiation","text":"","code":"genericGradientApproximiation(   parameters,   tuningParameters,   penaltyFunctionArguments )"},{"path":"/reference/genericGradientApproximiation.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"genericGradientApproximiation — genericGradientApproximiation","text":"parameters vector labeled parameter values tuningParameters data.frame tuning parameters penaltyFunctionArguments list additional arguments passed penalty function. NOTE: penalty function must also object list! penalty function. Must accept three parameters: vector parameter values, data.frame tuning parameters, list penaltyFunctionArguments. see lessSEM::smoothLASSO example","code":""},{"path":"/reference/genericGradientApproximiation.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"genericGradientApproximiation — genericGradientApproximiation","text":"vector gradient values","code":""},{"path":"/reference/genericHessianApproximiation.html","id":null,"dir":"Reference","previous_headings":"","what":"genericHessianApproximiation — genericHessianApproximiation","title":"genericHessianApproximiation — genericHessianApproximiation","text":"function can used approximate Hessian generic penalty function numDeriv","code":""},{"path":"/reference/genericHessianApproximiation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"genericHessianApproximiation — genericHessianApproximiation","text":"","code":"genericHessianApproximiation(   parameters,   tuningParameters,   penaltyFunctionArguments )"},{"path":"/reference/genericHessianApproximiation.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"genericHessianApproximiation — genericHessianApproximiation","text":"parameters vector labeled parameter values tuningParameters data.frame tuning parameters penaltyFunctionArguments list additional arguments passed penalty function. NOTE: penalty function must also object list! penalty function. Must accept three parameters: vector parameter values, data.frame tuning parameters, list penaltyFunctionArguments. see lessSEM::smoothLASSO example","code":""},{"path":"/reference/genericHessianApproximiation.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"genericHessianApproximiation — genericHessianApproximiation","text":"matrix Hessian values","code":""},{"path":"/reference/getLavaanParameters.html","id":null,"dir":"Reference","previous_headings":"","what":"getLavaanParameters — getLavaanParameters","title":"getLavaanParameters — getLavaanParameters","text":"helper function: returns labeled vector parameters lavaan","code":""},{"path":"/reference/getLavaanParameters.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"getLavaanParameters — getLavaanParameters","text":"","code":"getLavaanParameters(lavaanModel, removeDuplicates = TRUE)"},{"path":"/reference/getLavaanParameters.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"getLavaanParameters — getLavaanParameters","text":"lavaanModel model class lavaan removeDuplicates duplicated parameters removed?","code":""},{"path":"/reference/getLavaanParameters.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"getLavaanParameters — getLavaanParameters","text":"returns labeled vector parameters lavaan","code":""},{"path":"/reference/getTuningParameterConfiguration.html","id":null,"dir":"Reference","previous_headings":"","what":"getTuningParameterConfiguration — getTuningParameterConfiguration","title":"getTuningParameterConfiguration — getTuningParameterConfiguration","text":"Returns lambda, theta, alpha values tuning parameters regularized SEM mixed penalty.","code":""},{"path":"/reference/getTuningParameterConfiguration.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"getTuningParameterConfiguration — getTuningParameterConfiguration","text":"","code":"getTuningParameterConfiguration(   regularizedSEMMixedPenalty,   tuningParameterConfiguration )"},{"path":"/reference/getTuningParameterConfiguration.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"getTuningParameterConfiguration — getTuningParameterConfiguration","text":"regularizedSEMMixedPenalty object type regularizedSEMMixedPenalty (see ?mixedPenalty) tuningParameterConfiguration integer indicating tuningParameterConfiguration extracted (e.g., 1). See entry row tuningParameterConfiguration regularizedSEMMixedPenalty@fits regularizedSEMMixedPenalty@parameters.","code":""},{"path":"/reference/getTuningParameterConfiguration.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"getTuningParameterConfiguration — getTuningParameterConfiguration","text":"data frame penalty tuning parameter settings","code":""},{"path":"/reference/glmnetEnetGeneralPurpose.html","id":null,"dir":"Reference","previous_headings":"","what":"elastic net optimization with glmnet optimizer — glmnetEnetGeneralPurpose","title":"elastic net optimization with glmnet optimizer — glmnetEnetGeneralPurpose","text":"Object elastic net optimization glmnet optimizer","code":""},{"path":"/reference/glmnetEnetGeneralPurpose.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"elastic net optimization with glmnet optimizer — glmnetEnetGeneralPurpose","text":"list fit results","code":""},{"path":"/reference/glmnetEnetGeneralPurpose.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"elastic net optimization with glmnet optimizer — glmnetEnetGeneralPurpose","text":"new creates new object. Requires (1) vector weights parameter (2) list control elements setHessian changes Hessian model. Expects matrix optimize optimize model. Expects vector starting values, R function compute fit, R function compute gradients, list elements fit gradient function require, lambda alpha value.","code":""},{"path":"/reference/glmnetEnetGeneralPurposeCpp.html","id":null,"dir":"Reference","previous_headings":"","what":"elastic net optimization with glmnet optimizer — glmnetEnetGeneralPurposeCpp","title":"elastic net optimization with glmnet optimizer — glmnetEnetGeneralPurposeCpp","text":"Object elastic net optimization glmnet optimizer","code":""},{"path":"/reference/glmnetEnetGeneralPurposeCpp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"elastic net optimization with glmnet optimizer — glmnetEnetGeneralPurposeCpp","text":"list fit results","code":""},{"path":"/reference/glmnetEnetGeneralPurposeCpp.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"elastic net optimization with glmnet optimizer — glmnetEnetGeneralPurposeCpp","text":"new creates new object. Requires (1) vector weights parameter (2) list control elements setHessian changes Hessian model. Expects matrix optimize optimize model. Expects vector starting values, SEXP function pointer compute fit, SEXP function pointer compute gradients, list elements fit gradient function require, lambda alpha value.","code":""},{"path":"/reference/glmnetEnetMgSEM.html","id":null,"dir":"Reference","previous_headings":"","what":"elastic net optimization with glmnet optimizer — glmnetEnetMgSEM","title":"elastic net optimization with glmnet optimizer — glmnetEnetMgSEM","text":"Object elastic net optimization glmnet optimizer","code":""},{"path":"/reference/glmnetEnetMgSEM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"elastic net optimization with glmnet optimizer — glmnetEnetMgSEM","text":"list fit results","code":""},{"path":"/reference/glmnetEnetMgSEM.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"elastic net optimization with glmnet optimizer — glmnetEnetMgSEM","text":"new creates new object. Requires (1) vector weights parameter (2) list control elements setHessian changes Hessian model. Expects matrix optimize optimize model. Expects vector starting values, SEM type SEM_Cpp, lambda alpha value.","code":""},{"path":"/reference/glmnetEnetSEM.html","id":null,"dir":"Reference","previous_headings":"","what":"elastic net optimization with glmnet optimizer — glmnetEnetSEM","title":"elastic net optimization with glmnet optimizer — glmnetEnetSEM","text":"Object elastic net optimization glmnet optimizer","code":""},{"path":"/reference/glmnetEnetSEM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"elastic net optimization with glmnet optimizer — glmnetEnetSEM","text":"list fit results","code":""},{"path":"/reference/glmnetEnetSEM.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"elastic net optimization with glmnet optimizer — glmnetEnetSEM","text":"new creates new object. Requires (1) vector weights parameter (2) list control elements setHessian changes Hessian model. Expects matrix optimize optimize model. Expects vector starting values, SEM type SEM_Cpp, lambda alpha value.","code":""},{"path":"/reference/gpAdaptiveLasso.html","id":null,"dir":"Reference","previous_headings":"","what":"gpAdaptiveLasso — gpAdaptiveLasso","title":"gpAdaptiveLasso — gpAdaptiveLasso","text":"Implements adaptive lasso regularization general purpose optimization problems. penalty function given : $$p( x_j) = p( x_j) = \\frac{1}{w_j}\\lambda| x_j|$$ Adaptive lasso regularization set parameters zero \\(\\lambda\\) large enough.","code":""},{"path":"/reference/gpAdaptiveLasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"gpAdaptiveLasso — gpAdaptiveLasso","text":"","code":"gpAdaptiveLasso(   par,   regularized,   weights = NULL,   fn,   gr = NULL,   lambdas = NULL,   nLambdas = NULL,   reverse = TRUE,   curve = 1,   ...,   method = \"glmnet\",   control = lessSEM::controlGlmnet() )"},{"path":"/reference/gpAdaptiveLasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"gpAdaptiveLasso — gpAdaptiveLasso","text":"par labeled vector starting values regularized vector names parameters regularized. weights labeled vector adaptive lasso weights. NULL use 1/abs(par) fn R function takes parameters input returns fit value (single value) gr R function takes parameters input returns gradients objective function. set NULL, numDeriv used approximate gradients lambdas numeric vector: values tuning parameter lambda nLambdas alternative lambda: alpha = 1, lessSEM can automatically compute first lambda value sets regularized parameters zero. generate nLambda values 0 computed lambda. reverse set TRUE nLambdas used, lessSEM start largest lambda gradually decrease lambda. Otherwise, lessSEM start smallest lambda gradually increase . curve Allows unequally spaced lambda steps (e.g., .01,.02,.05,1,5,20). curve close 1 lambda values equally spaced, curve large lambda values concentrated close 0. See ?lessSEM::curveLambda information. ... additional arguments passed fn gr method optimizer used? Currently implemented ista glmnet. ista, control argument can used switch related procedures (currently gist). control used control optimizer. element generated controlIsta controlGlmnet functions. See ?controlIsta ?controlGlmnet details.","code":""},{"path":"/reference/gpAdaptiveLasso.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"gpAdaptiveLasso — gpAdaptiveLasso","text":"Object class gpRegularized","code":""},{"path":"/reference/gpAdaptiveLasso.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"gpAdaptiveLasso — gpAdaptiveLasso","text":"interface similar optim. Users supply vector starting values (important: vector must labels) fitting function. fitting functions must take labeled vector parameter values first argument. remaining arguments passed ... argument. similar optim. gradient function gr optional. set NULL, numDeriv package used approximate gradients. Supplying gradient function can result considerable speed improvements. Adaptive lasso regularization: Zou, H. (2006). adaptive lasso oracle properties. Journal American Statistical Association, 101(476), 1418–1429. https://doi.org/10.1198/016214506000000735 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classiﬁcation. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/gpAdaptiveLasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"gpAdaptiveLasso — gpAdaptiveLasso","text":"","code":"# This example shows how to use the optimizers # for other objective functions. We will use # a linear regression as an example. Note that # this is not a useful application of the optimizers # as there are specialized packages for linear regression # (e.g., glmnet)  library(lessSEM) set.seed(123)  # first, we simulate data for our # linear regression. N <- 100 # number of persons p <- 10 # number of predictors X <- matrix(rnorm(N*p),  nrow = N, ncol = p) # design matrix b <- c(rep(1,4),         rep(0,6)) # true regression weights y <- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)  # First, we must construct a fiting function # which returns a single value. We will use # the residual sum squared as fitting function.  # Let's start setting up the fitting function: fittingFunction <- function(par, y, X, N){   # par is the parameter vector   # y is the observed dependent variable   # X is the design matrix   # N is the sample size   pred <- X %*% matrix(par, ncol = 1) #be explicit here:    # we need par to be a column vector   sse <- sum((y - pred)^2)   # we scale with .5/N to get the same results as glmnet   return((.5/N)*sse) }  # let's define the starting values: b <- c(solve(t(X)%*%X)%*%t(X)%*%y) # we will use the lm estimates names(b) <- paste0(\"b\", 1:length(b)) # names of regularized parameters regularized <- paste0(\"b\",1:p)  # define the weight for each of the parameters weights <- 1/abs(b) # we will re-scale the weights for equivalence to glmnet. # see ?glmnet for more details weights <- length(b)*weights/sum(weights)  # optimize adaptiveLassoPen <- gpAdaptiveLasso(   par = b,    regularized = regularized,    weights = weights,   fn = fittingFunction,    lambdas = seq(0,1,.01),    X = X,   y = y,   N = N ) plot(adaptiveLassoPen) AIC(adaptiveLassoPen)  # for comparison: # library(glmnet) # coef(glmnet(x = X, #            y = y, #            penalty.factor = weights, #            lambda = adaptiveLassoPen@fits$lambda[20], #            intercept = FALSE, #            standardize = FALSE))[,1] # adaptiveLassoPen@parameters[20,]"},{"path":"/reference/gpAdaptiveLassoCpp.html","id":null,"dir":"Reference","previous_headings":"","what":"gpAdaptiveLassoCpp — gpAdaptiveLassoCpp","title":"gpAdaptiveLassoCpp — gpAdaptiveLassoCpp","text":"Implements adaptive lasso regularization general purpose optimization problems C++ functions. penalty function given : $$p( x_j) = p( x_j) = \\frac{1}{w_j}\\lambda| x_j|$$ Adaptive lasso regularization set parameters zero \\(\\lambda\\) large enough.","code":""},{"path":"/reference/gpAdaptiveLassoCpp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"gpAdaptiveLassoCpp — gpAdaptiveLassoCpp","text":"","code":"gpAdaptiveLassoCpp(   par,   regularized,   weights = NULL,   fn,   gr,   lambdas = NULL,   nLambdas = NULL,   curve = 1,   additionalArguments,   method = \"glmnet\",   control = lessSEM::controlGlmnet() )"},{"path":"/reference/gpAdaptiveLassoCpp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"gpAdaptiveLassoCpp — gpAdaptiveLassoCpp","text":"par labeled vector starting values regularized vector names parameters regularized. weights labeled vector adaptive lasso weights. NULL use 1/abs(par) fn R function takes parameters input returns fit value (single value) gr R function takes parameters input returns gradients objective function. set NULL, numDeriv used approximate gradients lambdas numeric vector: values tuning parameter lambda nLambdas alternative lambda: alpha = 1, lessSEM can automatically compute first lambda value sets regularized parameters zero. generate nLambda values 0 computed lambda. curve Allows unequally spaced lambda steps (e.g., .01,.02,.05,1,5,20). curve close 1 lambda values equally spaced, curve large lambda values concentrated close 0. See ?lessSEM::curveLambda information. additionalArguments list additional arguments passed fn gr method optimizer used? Currently implemented ista glmnet. ista, control argument can used switch related procedures (currently gist). control used control optimizer. element generated controlIsta controlGlmnet functions. See ?controlIsta ?controlGlmnet details.","code":""},{"path":"/reference/gpAdaptiveLassoCpp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"gpAdaptiveLassoCpp — gpAdaptiveLassoCpp","text":"Object class gpRegularized","code":""},{"path":"/reference/gpAdaptiveLassoCpp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"gpAdaptiveLassoCpp — gpAdaptiveLassoCpp","text":"interface inspired optim, bit restrictive. Users supply vector starting values (important: vector must labels), fitting function, gradient function. fitting functions must take const Rcpp::NumericVector& parameter values first argument Rcpp::List& second argument Adaptive lasso regularization: Zou, H. (2006). adaptive lasso oracle properties. Journal American Statistical Association, 101(476), 1418–1429. https://doi.org/10.1198/016214506000000735 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classiﬁcation. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/gpAdaptiveLassoCpp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"gpAdaptiveLassoCpp — gpAdaptiveLassoCpp","text":"","code":"# This example shows how to use the optimizers # for C++ objective functions. We will use # a linear regression as an example. Note that # this is not a useful application of the optimizers # as there are specialized packages for linear regression # (e.g., glmnet)  library(Rcpp) library(lessSEM)  linreg <- ' // [[Rcpp::depends(RcppArmadillo)]] #include <RcppArmadillo.h>  // [[Rcpp::export]] double fitfunction(const Rcpp::NumericVector& parameters, Rcpp::List& data){   // extract all required elements:   arma::colvec b = Rcpp::as<arma::colvec>(parameters);   arma::colvec y = Rcpp::as<arma::colvec>(data[\"y\"]); // the dependent variable   arma::mat X = Rcpp::as<arma::mat>(data[\"X\"]); // the design matrix      // compute the sum of squared errors:     arma::mat sse = arma::trans(y-X*b)*(y-X*b);          // other packages, such as glmnet, scale the sse with      // 1/(2*N), where N is the sample size. We will do that here as well          sse *= 1.0/(2.0 * y.n_elem);          // note: We must return a double, but the sse is a matrix     // To get a double, just return the single value that is in      // this matrix:       return(sse(0,0)); }  // [[Rcpp::export]] arma::rowvec gradientfunction(const Rcpp::NumericVector& parameters, Rcpp::List& data){   // extract all required elements:   arma::colvec b = Rcpp::as<arma::colvec>(parameters);   arma::colvec y = Rcpp::as<arma::colvec>(data[\"y\"]); // the dependent variable   arma::mat X = Rcpp::as<arma::mat>(data[\"X\"]); // the design matrix      // note: we want to return our gradients as row-vector; therefore,   // we have to transpose the resulting column-vector:     arma::rowvec gradients = arma::trans(-2.0*X.t() * y + 2.0*X.t()*X*b);          // other packages, such as glmnet, scale the sse with      // 1/(2*N), where N is the sample size. We will do that here as well          gradients *= (.5/y.n_rows);          return(gradients); }  // https://gallery.rcpp.org/articles/passing-cpp-function-pointers/ typedef double (*fitFunPtr)(const Rcpp::NumericVector&, //parameters                 Rcpp::List& //additional elements ); typedef Rcpp::XPtr<fitFunPtr> fitFunPtr_t;  typedef arma::rowvec (*gradientFunPtr)(const Rcpp::NumericVector&, //parameters                       Rcpp::List& //additional elements ); typedef Rcpp::XPtr<gradientFunPtr> gradientFunPtr_t;  // [[Rcpp::export]] fitFunPtr_t fitfunPtr() {         return(fitFunPtr_t(new fitFunPtr(&fitfunction))); }  // [[Rcpp::export]] gradientFunPtr_t gradfunPtr() {         return(gradientFunPtr_t(new gradientFunPtr(&gradientfunction))); } '  Rcpp::sourceCpp(code = linreg)  ffp <- fitfunPtr() gfp <- gradfunPtr()  N <- 100 # number of persons p <- 10 # number of predictors X <- matrix(rnorm(N*p),  nrow = N, ncol = p) # design matrix b <- c(rep(1,4),         rep(0,6)) # true regression weights y <- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)  data <- list(\"y\" = y,              \"X\" = cbind(1,X)) parameters <- rep(0, ncol(data$X)) names(parameters) <- paste0(\"b\", 0:(length(parameters)-1))  al1 <- gpAdaptiveLassoCpp(par = parameters,                   regularized = paste0(\"b\", 1:(length(b)-1)),                  fn = ffp,                   gr = gfp,                   lambdas = seq(0,1,.1),                   additionalArguments = data)  al1@parameters"},{"path":"/reference/gpCappedL1.html","id":null,"dir":"Reference","previous_headings":"","what":"gpCappedL1 — gpCappedL1","title":"gpCappedL1 — gpCappedL1","text":"Implements cappedL1 regularization general purpose optimization problems. penalty function given : $$p( x_j) = \\lambda \\min(| x_j|, \\theta)$$ \\(\\theta > 0\\). cappedL1 penalty identical lasso parameters \\(\\theta\\) identical constant parameters \\(\\theta\\). adding constant fitting function change minimum, larger parameters can stay unregularized smaller ones set zero.","code":""},{"path":"/reference/gpCappedL1.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"gpCappedL1 — gpCappedL1","text":"","code":"gpCappedL1(   par,   fn,   gr = NULL,   ...,   regularized,   lambdas,   thetas,   control = lessSEM::controlIsta() )"},{"path":"/reference/gpCappedL1.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"gpCappedL1 — gpCappedL1","text":"par labeled vector starting values fn R function takes parameters labels input returns fit value (single value) gr R function takes parameters labels input returns gradients objective function. set NULL, numDeriv used approximate gradients ... additional arguments passed fn gr regularized vector names parameters regularized. lambdas numeric vector: values tuning parameter lambda thetas parameters whose absolute value threshold penalized constant (theta) control used control optimizer. element generated controlIsta (see ?controlIsta)","code":""},{"path":"/reference/gpCappedL1.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"gpCappedL1 — gpCappedL1","text":"Object class gpRegularized","code":""},{"path":"/reference/gpCappedL1.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"gpCappedL1 — gpCappedL1","text":"interface similar optim. Users supply vector starting values (important: vector must labels) fitting function. fitting functions must take labeled vector parameter values first argument. remaining arguments passed ... argument. similar optim. gradient function gr optional. set NULL, numDeriv package used approximate gradients. Supplying gradient function can result considerable speed improvements. CappedL1 regularization: Zhang, T. (2010). Analysis Multi-stage Convex Relaxation Sparse Regularization. Journal Machine Learning Research, 11, 1081–1107. details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classiﬁcation. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/gpCappedL1.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"gpCappedL1 — gpCappedL1","text":"","code":"# This example shows how to use the optimizers # for other objective functions. We will use # a linear regression as an example. Note that # this is not a useful application of the optimizers # as there are specialized packages for linear regression # (e.g., glmnet)  # This example shows how to use the optimizers # for other objective functions. We will use # a linear regression as an example. Note that # this is not a useful application of the optimizers # as there are specialized packages for linear regression # (e.g., glmnet)  library(lessSEM) set.seed(123)  # first, we simulate data for our # linear regression. N <- 100 # number of persons p <- 10 # number of predictors X <- matrix(rnorm(N*p),  nrow = N, ncol = p) # design matrix b <- c(rep(1,4),        rep(0,6)) # true regression weights y <- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)  # First, we must construct a fiting function # which returns a single value. We will use # the residual sum squared as fitting function.  # Let's start setting up the fitting function: fittingFunction <- function(par, y, X, N){   # par is the parameter vector   # y is the observed dependent variable   # X is the design matrix   # N is the sample size   pred <- X %*% matrix(par, ncol = 1) #be explicit here:   # we need par to be a column vector   sse <- sum((y - pred)^2)   # we scale with .5/N to get the same results as glmnet   return((.5/N)*sse) }  # let's define the starting values: b <- c(solve(t(X)%*%X)%*%t(X)%*%y) # we will use the lm estimates names(b) <- paste0(\"b\", 1:length(b)) # names of regularized parameters regularized <- paste0(\"b\",1:p)  # optimize cL1 <- gpCappedL1(   par = b,   regularized = regularized,   fn = fittingFunction,   lambdas = seq(0,1,.1),   thetas = c(0.001, .5, 1),   X = X,   y = y,   N = N )  # optional: plot requires plotly package # plot(cL1)  # for comparison  fittingFunction <- function(par, y, X, N, lambda, theta){   pred <- X %*% matrix(par, ncol = 1)   sse <- sum((y - pred)^2)   smoothAbs <- sqrt(par^2 + 1e-8)   pen <- lambda * ifelse(smoothAbs < theta, smoothAbs, theta)   return((.5/N)*sse + sum(pen)) }  round(   optim(par = b,       fn = fittingFunction,       y = y,       X = X,       N = N,       lambda =  cL1@fits$lambda[15],       theta =  cL1@fits$theta[15],       method = \"BFGS\")$par,   4) cL1@parameters[15,]"},{"path":"/reference/gpCappedL1Cpp.html","id":null,"dir":"Reference","previous_headings":"","what":"Implements cappedL1 regularization for general purpose optimization problems with C++ functions.\nThe penalty function is given by:\n$$p( x_j) = \\lambda \\min(| x_j|, \\theta)$$\nwhere \\(\\theta > 0\\). The cappedL1 penalty is identical to the lasso for\nparameters which are below \\(\\theta\\) and identical to a constant for parameters\nabove \\(\\theta\\). As adding a constant to the fitting function will not change its\nminimum, larger parameters can stay unregularized while smaller ones are set to zero. — gpCappedL1Cpp","title":"Implements cappedL1 regularization for general purpose optimization problems with C++ functions.\nThe penalty function is given by:\n$$p( x_j) = \\lambda \\min(| x_j|, \\theta)$$\nwhere \\(\\theta > 0\\). The cappedL1 penalty is identical to the lasso for\nparameters which are below \\(\\theta\\) and identical to a constant for parameters\nabove \\(\\theta\\). As adding a constant to the fitting function will not change its\nminimum, larger parameters can stay unregularized while smaller ones are set to zero. — gpCappedL1Cpp","text":"interface inspired optim, bit restrictive. Users supply vector starting values (important: vector must labels), fitting function, gradient function. fitting functions must take const Rcpp::NumericVector& parameter values first argument Rcpp::List& second argument","code":""},{"path":"/reference/gpCappedL1Cpp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Implements cappedL1 regularization for general purpose optimization problems with C++ functions.\nThe penalty function is given by:\n$$p( x_j) = \\lambda \\min(| x_j|, \\theta)$$\nwhere \\(\\theta > 0\\). The cappedL1 penalty is identical to the lasso for\nparameters which are below \\(\\theta\\) and identical to a constant for parameters\nabove \\(\\theta\\). As adding a constant to the fitting function will not change its\nminimum, larger parameters can stay unregularized while smaller ones are set to zero. — gpCappedL1Cpp","text":"","code":"gpCappedL1Cpp(   par,   fn,   gr,   additionalArguments,   regularized,   lambdas,   thetas,   control = lessSEM::controlIsta() )"},{"path":"/reference/gpCappedL1Cpp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Implements cappedL1 regularization for general purpose optimization problems with C++ functions.\nThe penalty function is given by:\n$$p( x_j) = \\lambda \\min(| x_j|, \\theta)$$\nwhere \\(\\theta > 0\\). The cappedL1 penalty is identical to the lasso for\nparameters which are below \\(\\theta\\) and identical to a constant for parameters\nabove \\(\\theta\\). As adding a constant to the fitting function will not change its\nminimum, larger parameters can stay unregularized while smaller ones are set to zero. — gpCappedL1Cpp","text":"par labeled vector starting values fn R function takes parameters labels input returns fit value (single value) gr R function takes parameters labels input returns gradients objective function. set NULL, numDeriv used approximate gradients additionalArguments list additional arguments passed fn gr regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda thetas parameters whose absolute value threshold penalized constant (theta) control used control optimizer. element generated controlIsta (see ?controlIsta)","code":""},{"path":"/reference/gpCappedL1Cpp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Implements cappedL1 regularization for general purpose optimization problems with C++ functions.\nThe penalty function is given by:\n$$p( x_j) = \\lambda \\min(| x_j|, \\theta)$$\nwhere \\(\\theta > 0\\). The cappedL1 penalty is identical to the lasso for\nparameters which are below \\(\\theta\\) and identical to a constant for parameters\nabove \\(\\theta\\). As adding a constant to the fitting function will not change its\nminimum, larger parameters can stay unregularized while smaller ones are set to zero. — gpCappedL1Cpp","text":"Object class gpRegularized","code":""},{"path":"/reference/gpCappedL1Cpp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Implements cappedL1 regularization for general purpose optimization problems with C++ functions.\nThe penalty function is given by:\n$$p( x_j) = \\lambda \\min(| x_j|, \\theta)$$\nwhere \\(\\theta > 0\\). The cappedL1 penalty is identical to the lasso for\nparameters which are below \\(\\theta\\) and identical to a constant for parameters\nabove \\(\\theta\\). As adding a constant to the fitting function will not change its\nminimum, larger parameters can stay unregularized while smaller ones are set to zero. — gpCappedL1Cpp","text":"CappedL1 regularization: Zhang, T. (2010). Analysis Multi-stage Convex Relaxation Sparse Regularization. Journal Machine Learning Research, 11, 1081–1107. details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classiﬁcation. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/gpCappedL1Cpp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Implements cappedL1 regularization for general purpose optimization problems with C++ functions.\nThe penalty function is given by:\n$$p( x_j) = \\lambda \\min(| x_j|, \\theta)$$\nwhere \\(\\theta > 0\\). The cappedL1 penalty is identical to the lasso for\nparameters which are below \\(\\theta\\) and identical to a constant for parameters\nabove \\(\\theta\\). As adding a constant to the fitting function will not change its\nminimum, larger parameters can stay unregularized while smaller ones are set to zero. — gpCappedL1Cpp","text":"","code":"# This example shows how to use the optimizers # for C++ objective functions. We will use # a linear regression as an example. Note that # this is not a useful application of the optimizers # as there are specialized packages for linear regression # (e.g., glmnet)  library(Rcpp) library(lessSEM)  linreg <- ' // [[Rcpp::depends(RcppArmadillo)]] #include <RcppArmadillo.h>  // [[Rcpp::export]] double fitfunction(const Rcpp::NumericVector& parameters, Rcpp::List& data){   // extract all required elements:   arma::colvec b = Rcpp::as<arma::colvec>(parameters);   arma::colvec y = Rcpp::as<arma::colvec>(data[\"y\"]); // the dependent variable   arma::mat X = Rcpp::as<arma::mat>(data[\"X\"]); // the design matrix      // compute the sum of squared errors:     arma::mat sse = arma::trans(y-X*b)*(y-X*b);          // other packages, such as glmnet, scale the sse with      // 1/(2*N), where N is the sample size. We will do that here as well          sse *= 1.0/(2.0 * y.n_elem);          // note: We must return a double, but the sse is a matrix     // To get a double, just return the single value that is in      // this matrix:       return(sse(0,0)); }  // [[Rcpp::export]] arma::rowvec gradientfunction(const Rcpp::NumericVector& parameters, Rcpp::List& data){   // extract all required elements:   arma::colvec b = Rcpp::as<arma::colvec>(parameters);   arma::colvec y = Rcpp::as<arma::colvec>(data[\"y\"]); // the dependent variable   arma::mat X = Rcpp::as<arma::mat>(data[\"X\"]); // the design matrix      // note: we want to return our gradients as row-vector; therefore,   // we have to transpose the resulting column-vector:     arma::rowvec gradients = arma::trans(-2.0*X.t() * y + 2.0*X.t()*X*b);          // other packages, such as glmnet, scale the sse with      // 1/(2*N), where N is the sample size. We will do that here as well          gradients *= (.5/y.n_rows);          return(gradients); }  // https://gallery.rcpp.org/articles/passing-cpp-function-pointers/ typedef double (*fitFunPtr)(const Rcpp::NumericVector&, //parameters                 Rcpp::List& //additional elements ); typedef Rcpp::XPtr<fitFunPtr> fitFunPtr_t;  typedef arma::rowvec (*gradientFunPtr)(const Rcpp::NumericVector&, //parameters                       Rcpp::List& //additional elements ); typedef Rcpp::XPtr<gradientFunPtr> gradientFunPtr_t;  // [[Rcpp::export]] fitFunPtr_t fitfunPtr() {         return(fitFunPtr_t(new fitFunPtr(&fitfunction))); }  // [[Rcpp::export]] gradientFunPtr_t gradfunPtr() {         return(gradientFunPtr_t(new gradientFunPtr(&gradientfunction))); } '  Rcpp::sourceCpp(code = linreg)  ffp <- fitfunPtr() gfp <- gradfunPtr()  N <- 100 # number of persons p <- 10 # number of predictors X <- matrix(rnorm(N*p),  nrow = N, ncol = p) # design matrix b <- c(rep(1,4),         rep(0,6)) # true regression weights y <- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)  data <- list(\"y\" = y,              \"X\" = cbind(1,X)) parameters <- rep(0, ncol(data$X)) names(parameters) <- paste0(\"b\", 0:(length(parameters)-1))  cL1 <- gpCappedL1Cpp(par = parameters,                   regularized = paste0(\"b\", 1:(length(b)-1)),                  fn = ffp,                   gr = gfp,                   lambdas = seq(0,1,.1),                   thetas = seq(0.1,1,.1),                  additionalArguments = data)  cL1@parameters"},{"path":"/reference/gpElasticNet.html","id":null,"dir":"Reference","previous_headings":"","what":"gpElasticNet — gpElasticNet","title":"gpElasticNet — gpElasticNet","text":"Implements elastic net regularization general purpose optimization problems. penalty function given : $$p( x_j) = p( x_j) = \\frac{1}{w_j}\\lambda| x_j|$$ Note elastic net combines ridge lasso regularization. \\(\\alpha = 0\\), elastic net reduces ridge regularization. \\(\\alpha = 1\\) reduces lasso regularization. , elastic net compromise shrinkage lasso ridge penalty.","code":""},{"path":"/reference/gpElasticNet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"gpElasticNet — gpElasticNet","text":"","code":"gpElasticNet(   par,   regularized,   fn,   gr = NULL,   lambdas,   alphas,   ...,   method = \"glmnet\",   control = lessSEM::controlGlmnet() )"},{"path":"/reference/gpElasticNet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"gpElasticNet — gpElasticNet","text":"par labeled vector starting values regularized vector names parameters regularized. fn R function takes parameters labels input returns fit value (single value) gr R function takes parameters labels input returns gradients objective function. set NULL, numDeriv used approximate gradients lambdas numeric vector: values tuning parameter lambda alphas numeric vector values tuning parameter alpha. Must 0 1. 0 = ridge, 1 = lasso. ... additional arguments passed fn gr method optimizer used? Currently implemented ista glmnet. ista, control argument can used switch related procedures (currently gist). control used control optimizer. element generated lessSEM::controlIsta() controlGlmnet() functions.","code":""},{"path":"/reference/gpElasticNet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"gpElasticNet — gpElasticNet","text":"Object class gpRegularized","code":""},{"path":"/reference/gpElasticNet.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"gpElasticNet — gpElasticNet","text":"interface similar optim. Users supply vector starting values (important: vector must labels) fitting function. fitting functions must take labeled vector parameter values first argument. remaining arguments passed ... argument. similar optim. gradient function gr optional. set NULL, numDeriv package used approximate gradients. Supplying gradient function can result considerable speed improvements. Elastic net regularization: Zou, H., & Hastie, T. (2005). Regularization variable selection via elastic net. Journal Royal Statistical Society: Series B, 67(2), 301–320. https://doi.org/10.1111/j.1467-9868.2005.00503.x details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classiﬁcation. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/gpElasticNet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"gpElasticNet — gpElasticNet","text":"","code":"# This example shows how to use the optimizers # for other objective functions. We will use # a linear regression as an example. Note that # this is not a useful application of the optimizers # as there are specialized packages for linear regression # (e.g., glmnet)  library(lessSEM) set.seed(123)  # first, we simulate data for our # linear regression. N <- 100 # number of persons p <- 10 # number of predictors X <- matrix(rnorm(N*p),  nrow = N, ncol = p) # design matrix b <- c(rep(1,4),        rep(0,6)) # true regression weights y <- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)  # First, we must construct a fiting function # which returns a single value. We will use # the residual sum squared as fitting function.  # Let's start setting up the fitting function: fittingFunction <- function(par, y, X, N){   # par is the parameter vector   # y is the observed dependent variable   # X is the design matrix   # N is the sample size   pred <- X %*% matrix(par, ncol = 1) #be explicit here:   # we need par to be a column vector   sse <- sum((y - pred)^2)   # we scale with .5/N to get the same results as glmnet   return((.5/N)*sse) }  # let's define the starting values: b <- c(solve(t(X)%*%X)%*%t(X)%*%y) # we will use the lm estimates names(b) <- paste0(\"b\", 1:length(b)) # names of regularized parameters regularized <- paste0(\"b\",1:p)  # optimize elasticNetPen <- gpElasticNet(   par = b,   regularized = regularized,   fn = fittingFunction,   lambdas = seq(0,1,.1),   alphas = c(0, .5, 1),   X = X,   y = y,   N = N )  # optional: plot requires plotly package # plot(elasticNetPen)  # for comparison: fittingFunction <- function(par, y, X, N, lambda, alpha){   pred <- X %*% matrix(par, ncol = 1)   sse <- sum((y - pred)^2)   return((.5/N)*sse + (1-alpha)*lambda * sum(par^2) + alpha*lambda *sum(sqrt(par^2 + 1e-8))) }  round(   optim(par = b,       fn = fittingFunction,       y = y,       X = X,       N = N,       lambda =  elasticNetPen@fits$lambda[15],       alpha =  elasticNetPen@fits$alpha[15],       method = \"BFGS\")$par,   4) elasticNetPen@parameters[15,]"},{"path":"/reference/gpElasticNetCpp.html","id":null,"dir":"Reference","previous_headings":"","what":"gpElasticNetCpp — gpElasticNetCpp","title":"gpElasticNetCpp — gpElasticNetCpp","text":"Implements elastic net regularization general purpose optimization problems C++ functions. penalty function given : $$p( x_j) = p( x_j) = \\frac{1}{w_j}\\lambda| x_j|$$ Note elastic net combines ridge lasso regularization. \\(\\alpha = 0\\), elastic net reduces ridge regularization. \\(\\alpha = 1\\) reduces lasso regularization. , elastic net compromise shrinkage lasso ridge penalty.","code":""},{"path":"/reference/gpElasticNetCpp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"gpElasticNetCpp — gpElasticNetCpp","text":"","code":"gpElasticNetCpp(   par,   regularized,   fn,   gr,   lambdas,   alphas,   additionalArguments,   method = \"glmnet\",   control = lessSEM::controlGlmnet() )"},{"path":"/reference/gpElasticNetCpp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"gpElasticNetCpp — gpElasticNetCpp","text":"par labeled vector starting values regularized vector names parameters regularized. fn R function takes parameters labels input returns fit value (single value) gr R function takes parameters labels input returns gradients objective function. set NULL, numDeriv used approximate gradients lambdas numeric vector: values tuning parameter lambda alphas numeric vector values tuning parameter alpha. Must 0 1. 0 = ridge, 1 = lasso. additionalArguments list additional arguments passed fn gr method optimizer used? Currently implemented ista glmnet. ista, control argument can used switch related procedures (currently gist). control used control optimizer. element generated lessSEM::controlIsta() controlGlmnet() functions.","code":""},{"path":"/reference/gpElasticNetCpp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"gpElasticNetCpp — gpElasticNetCpp","text":"Object class gpRegularized","code":""},{"path":"/reference/gpElasticNetCpp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"gpElasticNetCpp — gpElasticNetCpp","text":"interface inspired optim, bit restrictive. Users supply vector starting values (important: vector must labels), fitting function, gradient function. fitting functions must take const Rcpp::NumericVector& parameter values first argument Rcpp::List& second argument Elastic net regularization: Zou, H., & Hastie, T. (2005). Regularization variable selection via elastic net. Journal Royal Statistical Society: Series B, 67(2), 301–320. https://doi.org/10.1111/j.1467-9868.2005.00503.x details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classiﬁcation. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/gpElasticNetCpp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"gpElasticNetCpp — gpElasticNetCpp","text":"","code":"# This example shows how to use the optimizers # for C++ objective functions. We will use # a linear regression as an example. Note that # this is not a useful application of the optimizers # as there are specialized packages for linear regression # (e.g., glmnet)  library(Rcpp) library(lessSEM)  linreg <- ' // [[Rcpp::depends(RcppArmadillo)]] #include <RcppArmadillo.h>  // [[Rcpp::export]] double fitfunction(const Rcpp::NumericVector& parameters, Rcpp::List& data){   // extract all required elements:   arma::colvec b = Rcpp::as<arma::colvec>(parameters);   arma::colvec y = Rcpp::as<arma::colvec>(data[\"y\"]); // the dependent variable   arma::mat X = Rcpp::as<arma::mat>(data[\"X\"]); // the design matrix      // compute the sum of squared errors:     arma::mat sse = arma::trans(y-X*b)*(y-X*b);          // other packages, such as glmnet, scale the sse with      // 1/(2*N), where N is the sample size. We will do that here as well          sse *= 1.0/(2.0 * y.n_elem);          // note: We must return a double, but the sse is a matrix     // To get a double, just return the single value that is in      // this matrix:       return(sse(0,0)); }  // [[Rcpp::export]] arma::rowvec gradientfunction(const Rcpp::NumericVector& parameters, Rcpp::List& data){   // extract all required elements:   arma::colvec b = Rcpp::as<arma::colvec>(parameters);   arma::colvec y = Rcpp::as<arma::colvec>(data[\"y\"]); // the dependent variable   arma::mat X = Rcpp::as<arma::mat>(data[\"X\"]); // the design matrix      // note: we want to return our gradients as row-vector; therefore,   // we have to transpose the resulting column-vector:     arma::rowvec gradients = arma::trans(-2.0*X.t() * y + 2.0*X.t()*X*b);          // other packages, such as glmnet, scale the sse with      // 1/(2*N), where N is the sample size. We will do that here as well          gradients *= (.5/y.n_rows);          return(gradients); }  // https://gallery.rcpp.org/articles/passing-cpp-function-pointers/ typedef double (*fitFunPtr)(const Rcpp::NumericVector&, //parameters                 Rcpp::List& //additional elements ); typedef Rcpp::XPtr<fitFunPtr> fitFunPtr_t;  typedef arma::rowvec (*gradientFunPtr)(const Rcpp::NumericVector&, //parameters                       Rcpp::List& //additional elements ); typedef Rcpp::XPtr<gradientFunPtr> gradientFunPtr_t;  // [[Rcpp::export]] fitFunPtr_t fitfunPtr() {         return(fitFunPtr_t(new fitFunPtr(&fitfunction))); }  // [[Rcpp::export]] gradientFunPtr_t gradfunPtr() {         return(gradientFunPtr_t(new gradientFunPtr(&gradientfunction))); } '  Rcpp::sourceCpp(code = linreg)  ffp <- fitfunPtr() gfp <- gradfunPtr()  N <- 100 # number of persons p <- 10 # number of predictors X <- matrix(rnorm(N*p),  nrow = N, ncol = p) # design matrix b <- c(rep(1,4),         rep(0,6)) # true regression weights y <- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)  data <- list(\"y\" = y,              \"X\" = cbind(1,X)) parameters <- rep(0, ncol(data$X)) names(parameters) <- paste0(\"b\", 0:(length(parameters)-1))  en <- gpElasticNetCpp(par = parameters,                   regularized = paste0(\"b\", 1:(length(b)-1)),                  fn = ffp,                   gr = gfp,                   lambdas = seq(0,1,.1),                   alphas = c(0,.5,1),                  additionalArguments = data)  en@parameters"},{"path":"/reference/gpLasso.html","id":null,"dir":"Reference","previous_headings":"","what":"gpLasso — gpLasso","title":"gpLasso — gpLasso","text":"Implements lasso regularization general purpose optimization problems. penalty function given : $$p( x_j) = \\lambda |x_j|$$ Lasso regularization set parameters zero \\(\\lambda\\) large enough","code":""},{"path":"/reference/gpLasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"gpLasso — gpLasso","text":"","code":"gpLasso(   par,   regularized,   fn,   gr = NULL,   lambdas = NULL,   nLambdas = NULL,   reverse = TRUE,   curve = 1,   ...,   method = \"glmnet\",   control = lessSEM::controlGlmnet() )"},{"path":"/reference/gpLasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"gpLasso — gpLasso","text":"par labeled vector starting values regularized vector names parameters regularized. fn R function takes parameters input returns fit value (single value) gr R function takes parameters input returns gradients objective function. set NULL, numDeriv used approximate gradients lambdas numeric vector: values tuning parameter lambda nLambdas alternative lambda: alpha = 1, lessSEM can automatically compute first lambda value sets regularized parameters zero. generate nLambda values 0 computed lambda. reverse set TRUE nLambdas used, lessSEM start largest lambda gradually decrease lambda. Otherwise, lessSEM start smallest lambda gradually increase . curve Allows unequally spaced lambda steps (e.g., .01,.02,.05,1,5,20). curve close 1 lambda values equally spaced, curve large lambda values concentrated close 0. See ?lessSEM::curveLambda information. ... additional arguments passed fn gr method optimizer used? Currently implemented ista glmnet. ista, control argument can used switch related procedures (currently gist). control used control optimizer. element generated controlIsta controlGlmnet functions. See ?controlIsta ?controlGlmnet details.","code":""},{"path":"/reference/gpLasso.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"gpLasso — gpLasso","text":"Object class gpRegularized","code":""},{"path":"/reference/gpLasso.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"gpLasso — gpLasso","text":"interface similar optim. Users supply vector starting values (important: vector must labels) fitting function. fitting functions must take labeled vector parameter values first argument. remaining arguments passed ... argument. similar optim. gradient function gr optional. set NULL, numDeriv package used approximate gradients. Supplying gradient function can result considerable speed improvements. Lasso regularization: Tibshirani, R. (1996). Regression shrinkage selection via lasso. Journal Royal Statistical Society. Series B (Methodological), 58(1), 267–288. details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classiﬁcation. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/gpLasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"gpLasso — gpLasso","text":"","code":"# This example shows how to use the optimizers # for other objective functions. We will use # a linear regression as an example. Note that # this is not a useful application of the optimizers # as there are specialized packages for linear regression # (e.g., glmnet)  library(lessSEM) set.seed(123)  # first, we simulate data for our # linear regression. N <- 100 # number of persons p <- 10 # number of predictors X <- matrix(rnorm(N*p),  nrow = N, ncol = p) # design matrix b <- c(rep(1,4),         rep(0,6)) # true regression weights y <- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)  # First, we must construct a fiting function # which returns a single value. We will use # the residual sum squared as fitting function.  # Let's start setting up the fitting function: fittingFunction <- function(par, y, X, N){   # par is the parameter vector   # y is the observed dependent variable   # X is the design matrix   # N is the sample size   pred <- X %*% matrix(par, ncol = 1) #be explicit here:    # we need par to be a column vector   sse <- sum((y - pred)^2)   # we scale with .5/N to get the same results as glmnet   return((.5/N)*sse) }  # let's define the starting values: b <- rep(0,p) names(b) <- paste0(\"b\", 1:length(b)) # names of regularized parameters regularized <- paste0(\"b\",1:p)  # optimize lassoPen <- gpLasso(   par = b,    regularized = regularized,    fn = fittingFunction,    nLambdas = 100,    X = X,   y = y,   N = N ) plot(lassoPen) AIC(lassoPen)  # for comparison: #library(glmnet) #coef(glmnet(x = X, #            y = y,  #            lambda = lassoPen@fits$lambda[20], #            intercept = FALSE, #            standardize = FALSE))[,1] #lassoPen@parameters[20,]"},{"path":"/reference/gpLassoCpp.html","id":null,"dir":"Reference","previous_headings":"","what":"gpLassoCpp — gpLassoCpp","title":"gpLassoCpp — gpLassoCpp","text":"Implements lasso regularization general purpose optimization problems C++ functions. penalty function given : $$p( x_j) = \\lambda |x_j|$$ Lasso regularization set parameters zero \\(\\lambda\\) large enough","code":""},{"path":"/reference/gpLassoCpp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"gpLassoCpp — gpLassoCpp","text":"","code":"gpLassoCpp(   par,   regularized,   fn,   gr,   lambdas = NULL,   nLambdas = NULL,   curve = 1,   additionalArguments,   method = \"glmnet\",   control = lessSEM::controlGlmnet() )"},{"path":"/reference/gpLassoCpp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"gpLassoCpp — gpLassoCpp","text":"par labeled vector starting values regularized vector names parameters regularized. fn pointer Rcpp function takes parameters input returns fit value (single value) gr pointer Rcpp function takes parameters input returns gradients objective function. lambdas numeric vector: values tuning parameter lambda nLambdas alternative lambda: alpha = 1, lessSEM can automatically compute first lambda value sets regularized parameters zero. generate nLambda values 0 computed lambda. curve Allows unequally spaced lambda steps (e.g., .01,.02,.05,1,5,20). curve close 1 lambda values equally spaced, curve large lambda values concentrated close 0. See ?lessSEM::curveLambda information. additionalArguments list additional arguments passed fn gr method optimizer used? Currently implemented ista glmnet. ista, control argument can used switch related procedures (currently gist). control used control optimizer. element generated controlIsta controlGlmnet functions. See ?controlIsta ?controlGlmnet details.","code":""},{"path":"/reference/gpLassoCpp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"gpLassoCpp — gpLassoCpp","text":"Object class gpRegularized","code":""},{"path":"/reference/gpLassoCpp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"gpLassoCpp — gpLassoCpp","text":"interface inspired optim, bit restrictive. Users supply vector starting values (important: vector must labels), fitting function, gradient function. fitting functions must take const Rcpp::NumericVector& parameter values first argument Rcpp::List& second argument Lasso regularization: Tibshirani, R. (1996). Regression shrinkage selection via lasso. Journal Royal Statistical Society. Series B (Methodological), 58(1), 267–288. details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classiﬁcation. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/gpLassoCpp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"gpLassoCpp — gpLassoCpp","text":"","code":"# This example shows how to use the optimizers # for C++ objective functions. We will use # a linear regression as an example. Note that # this is not a useful application of the optimizers # as there are specialized packages for linear regression # (e.g., glmnet)  library(Rcpp) library(lessSEM)  linreg <- ' // [[Rcpp::depends(RcppArmadillo)]] #include <RcppArmadillo.h>  // [[Rcpp::export]] double fitfunction(const Rcpp::NumericVector& parameters, Rcpp::List& data){   // extract all required elements:   arma::colvec b = Rcpp::as<arma::colvec>(parameters);   arma::colvec y = Rcpp::as<arma::colvec>(data[\"y\"]); // the dependent variable   arma::mat X = Rcpp::as<arma::mat>(data[\"X\"]); // the design matrix      // compute the sum of squared errors:     arma::mat sse = arma::trans(y-X*b)*(y-X*b);          // other packages, such as glmnet, scale the sse with      // 1/(2*N), where N is the sample size. We will do that here as well          sse *= 1.0/(2.0 * y.n_elem);          // note: We must return a double, but the sse is a matrix     // To get a double, just return the single value that is in      // this matrix:       return(sse(0,0)); }  // [[Rcpp::export]] arma::rowvec gradientfunction(const Rcpp::NumericVector& parameters, Rcpp::List& data){   // extract all required elements:   arma::colvec b = Rcpp::as<arma::colvec>(parameters);   arma::colvec y = Rcpp::as<arma::colvec>(data[\"y\"]); // the dependent variable   arma::mat X = Rcpp::as<arma::mat>(data[\"X\"]); // the design matrix      // note: we want to return our gradients as row-vector; therefore,   // we have to transpose the resulting column-vector:     arma::rowvec gradients = arma::trans(-2.0*X.t() * y + 2.0*X.t()*X*b);          // other packages, such as glmnet, scale the sse with      // 1/(2*N), where N is the sample size. We will do that here as well          gradients *= (.5/y.n_rows);          return(gradients); }  // https://gallery.rcpp.org/articles/passing-cpp-function-pointers/ typedef double (*fitFunPtr)(const Rcpp::NumericVector&, //parameters                 Rcpp::List& //additional elements ); typedef Rcpp::XPtr<fitFunPtr> fitFunPtr_t;  typedef arma::rowvec (*gradientFunPtr)(const Rcpp::NumericVector&, //parameters                       Rcpp::List& //additional elements ); typedef Rcpp::XPtr<gradientFunPtr> gradientFunPtr_t;  // [[Rcpp::export]] fitFunPtr_t fitfunPtr() {         return(fitFunPtr_t(new fitFunPtr(&fitfunction))); }  // [[Rcpp::export]] gradientFunPtr_t gradfunPtr() {         return(gradientFunPtr_t(new gradientFunPtr(&gradientfunction))); } '  Rcpp::sourceCpp(code = linreg)  ffp <- fitfunPtr() gfp <- gradfunPtr()  N <- 100 # number of persons p <- 10 # number of predictors X <- matrix(rnorm(N*p),  nrow = N, ncol = p) # design matrix b <- c(rep(1,4),         rep(0,6)) # true regression weights y <- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)  data <- list(\"y\" = y,              \"X\" = cbind(1,X)) parameters <- rep(0, ncol(data$X)) names(parameters) <- paste0(\"b\", 0:(length(parameters)-1))  l1 <- gpLassoCpp(par = parameters,                   regularized = paste0(\"b\", 1:(length(b)-1)),                  fn = ffp,                   gr = gfp,                   lambdas = seq(0,1,.1),                   additionalArguments = data)  l1@parameters  # for comparison: #library(glmnet) #coef(glmnet(x = X, #            y = y,  #            lambda = l1@fits$lambda[2], #            intercept = TRUE, #            standardize = FALSE))[,2] #l1@parameters[2,]"},{"path":"/reference/gpLsp.html","id":null,"dir":"Reference","previous_headings":"","what":"gpLsp — gpLsp","title":"gpLsp — gpLsp","text":"Implements lsp regularization general purpose optimization problems. penalty function given :","code":""},{"path":"/reference/gpLsp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"gpLsp — gpLsp","text":"","code":"gpLsp(   par,   fn,   gr = NULL,   ...,   regularized,   lambdas,   thetas,   control = lessSEM::controlIsta() )"},{"path":"/reference/gpLsp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"gpLsp — gpLsp","text":"par labeled vector starting values fn R function takes parameters labels input returns fit value (single value) gr R function takes parameters labels input returns gradients objective function. set NULL, numDeriv used approximate gradients ... additional arguments passed fn gr regularized vector names parameters regularized. lambdas numeric vector: values tuning parameter lambda thetas numeric vector: values tuning parameter theta control used control optimizer. element generated controlIsta (see ?controlIsta)","code":""},{"path":"/reference/gpLsp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"gpLsp — gpLsp","text":"Object class gpRegularized","code":""},{"path":"/reference/gpLsp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"gpLsp — gpLsp","text":"interface similar optim. Users supply vector starting values (important: vector must labels) fitting function. fitting functions must take labeled vector parameter values first argument. remaining arguments passed ... argument. similar optim. gradient function gr optional. set NULL, numDeriv package used approximate gradients. Supplying gradient function can result considerable speed improvements. lsp regularization: Candès, E. J., Wakin, M. B., & Boyd, S. P. (2008). Enhancing Sparsity Reweighted l1 Minimization. Journal Fourier Analysis Applications, 14(5–6), 877–905. https://doi.org/10.1007/s00041-008-9045-x details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classiﬁcation. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/gpLsp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"gpLsp — gpLsp","text":"","code":"library(lessSEM) set.seed(123)  # first, we simulate data for our # linear regression. N <- 100 # number of persons p <- 10 # number of predictors X <- matrix(rnorm(N*p),  nrow = N, ncol = p) # design matrix b <- c(rep(1,4),        rep(0,6)) # true regression weights y <- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)  # First, we must construct a fiting function # which returns a single value. We will use # the residual sum squared as fitting function.  # Let's start setting up the fitting function: fittingFunction <- function(par, y, X, N){   # par is the parameter vector   # y is the observed dependent variable   # X is the design matrix   # N is the sample size   pred <- X %*% matrix(par, ncol = 1) #be explicit here:   # we need par to be a column vector   sse <- sum((y - pred)^2)   # we scale with .5/N to get the same results as glmnet   return((.5/N)*sse) }  # let's define the starting values: b <- c(solve(t(X)%*%X)%*%t(X)%*%y) # we will use the lm estimates names(b) <- paste0(\"b\", 1:length(b)) # names of regularized parameters regularized <- paste0(\"b\",1:p)  # optimize lspPen <- gpLsp(   par = b,   regularized = regularized,   fn = fittingFunction,   lambdas = seq(0,1,.1),   thetas = c(0.001, .5, 1),   X = X,   y = y,   N = N )  # optional: plot requires plotly package # plot(lspPen)  # for comparison  fittingFunction <- function(par, y, X, N, lambda, theta){   pred <- X %*% matrix(par, ncol = 1)   sse <- sum((y - pred)^2)   smoothAbs <- sqrt(par^2 + 1e-8)   pen <- lambda * log(1.0 + smoothAbs / theta)   return((.5/N)*sse + sum(pen)) }  round(   optim(par = b,       fn = fittingFunction,       y = y,       X = X,       N = N,       lambda =  lspPen@fits$lambda[15],       theta =  lspPen@fits$theta[15],       method = \"BFGS\")$par,   4) lspPen@parameters[15,]"},{"path":"/reference/gpLspCpp.html","id":null,"dir":"Reference","previous_headings":"","what":"Implements lsp regularization for general purpose optimization problems with C++ functions.\nThe penalty function is given by:\n$$p( x_j) = \\lambda \\log(1 + |x_j|/\\theta)$$\nwhere \\(\\theta > 0\\). — gpLspCpp","title":"Implements lsp regularization for general purpose optimization problems with C++ functions.\nThe penalty function is given by:\n$$p( x_j) = \\lambda \\log(1 + |x_j|/\\theta)$$\nwhere \\(\\theta > 0\\). — gpLspCpp","text":"interface inspired optim, bit restrictive. Users supply vector starting values (important: vector must labels), fitting function, gradient function. fitting functions must take const Rcpp::NumericVector& parameter values first argument Rcpp::List& second argument","code":""},{"path":"/reference/gpLspCpp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Implements lsp regularization for general purpose optimization problems with C++ functions.\nThe penalty function is given by:\n$$p( x_j) = \\lambda \\log(1 + |x_j|/\\theta)$$\nwhere \\(\\theta > 0\\). — gpLspCpp","text":"","code":"gpLspCpp(   par,   fn,   gr,   additionalArguments,   regularized,   lambdas,   thetas,   control = lessSEM::controlIsta() )"},{"path":"/reference/gpLspCpp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Implements lsp regularization for general purpose optimization problems with C++ functions.\nThe penalty function is given by:\n$$p( x_j) = \\lambda \\log(1 + |x_j|/\\theta)$$\nwhere \\(\\theta > 0\\). — gpLspCpp","text":"par labeled vector starting values fn R function takes parameters labels input returns fit value (single value) gr R function takes parameters labels input returns gradients objective function. set NULL, numDeriv used approximate gradients additionalArguments list additional arguments passed fn gr regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda thetas numeric vector: values tuning parameter theta control used control optimizer. element generated controlIsta (see ?controlIsta)","code":""},{"path":"/reference/gpLspCpp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Implements lsp regularization for general purpose optimization problems with C++ functions.\nThe penalty function is given by:\n$$p( x_j) = \\lambda \\log(1 + |x_j|/\\theta)$$\nwhere \\(\\theta > 0\\). — gpLspCpp","text":"Object class gpRegularized","code":""},{"path":"/reference/gpLspCpp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Implements lsp regularization for general purpose optimization problems with C++ functions.\nThe penalty function is given by:\n$$p( x_j) = \\lambda \\log(1 + |x_j|/\\theta)$$\nwhere \\(\\theta > 0\\). — gpLspCpp","text":"lsp regularization: Candès, E. J., Wakin, M. B., & Boyd, S. P. (2008). Enhancing Sparsity Reweighted l1 Minimization. Journal Fourier Analysis Applications, 14(5–6), 877–905. https://doi.org/10.1007/s00041-008-9045-x details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classiﬁcation. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/gpLspCpp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Implements lsp regularization for general purpose optimization problems with C++ functions.\nThe penalty function is given by:\n$$p( x_j) = \\lambda \\log(1 + |x_j|/\\theta)$$\nwhere \\(\\theta > 0\\). — gpLspCpp","text":"","code":"# This example shows how to use the optimizers # for C++ objective functions. We will use # a linear regression as an example. Note that # this is not a useful application of the optimizers # as there are specialized packages for linear regression # (e.g., glmnet)  library(Rcpp) library(lessSEM)  linreg <- ' // [[Rcpp::depends(RcppArmadillo)]] #include <RcppArmadillo.h>  // [[Rcpp::export]] double fitfunction(const Rcpp::NumericVector& parameters, Rcpp::List& data){   // extract all required elements:   arma::colvec b = Rcpp::as<arma::colvec>(parameters);   arma::colvec y = Rcpp::as<arma::colvec>(data[\"y\"]); // the dependent variable   arma::mat X = Rcpp::as<arma::mat>(data[\"X\"]); // the design matrix      // compute the sum of squared errors:     arma::mat sse = arma::trans(y-X*b)*(y-X*b);          // other packages, such as glmnet, scale the sse with      // 1/(2*N), where N is the sample size. We will do that here as well          sse *= 1.0/(2.0 * y.n_elem);          // note: We must return a double, but the sse is a matrix     // To get a double, just return the single value that is in      // this matrix:       return(sse(0,0)); }  // [[Rcpp::export]] arma::rowvec gradientfunction(const Rcpp::NumericVector& parameters, Rcpp::List& data){   // extract all required elements:   arma::colvec b = Rcpp::as<arma::colvec>(parameters);   arma::colvec y = Rcpp::as<arma::colvec>(data[\"y\"]); // the dependent variable   arma::mat X = Rcpp::as<arma::mat>(data[\"X\"]); // the design matrix      // note: we want to return our gradients as row-vector; therefore,   // we have to transpose the resulting column-vector:     arma::rowvec gradients = arma::trans(-2.0*X.t() * y + 2.0*X.t()*X*b);          // other packages, such as glmnet, scale the sse with      // 1/(2*N), where N is the sample size. We will do that here as well          gradients *= (.5/y.n_rows);          return(gradients); }  // https://gallery.rcpp.org/articles/passing-cpp-function-pointers/ typedef double (*fitFunPtr)(const Rcpp::NumericVector&, //parameters                 Rcpp::List& //additional elements ); typedef Rcpp::XPtr<fitFunPtr> fitFunPtr_t;  typedef arma::rowvec (*gradientFunPtr)(const Rcpp::NumericVector&, //parameters                       Rcpp::List& //additional elements ); typedef Rcpp::XPtr<gradientFunPtr> gradientFunPtr_t;  // [[Rcpp::export]] fitFunPtr_t fitfunPtr() {         return(fitFunPtr_t(new fitFunPtr(&fitfunction))); }  // [[Rcpp::export]] gradientFunPtr_t gradfunPtr() {         return(gradientFunPtr_t(new gradientFunPtr(&gradientfunction))); } '  Rcpp::sourceCpp(code = linreg)  ffp <- fitfunPtr() gfp <- gradfunPtr()  N <- 100 # number of persons p <- 10 # number of predictors X <- matrix(rnorm(N*p),  nrow = N, ncol = p) # design matrix b <- c(rep(1,4),         rep(0,6)) # true regression weights y <- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)  data <- list(\"y\" = y,              \"X\" = cbind(1,X)) parameters <- rep(0, ncol(data$X)) names(parameters) <- paste0(\"b\", 0:(length(parameters)-1))  l <- gpLspCpp(par = parameters,                   regularized = paste0(\"b\", 1:(length(b)-1)),                  fn = ffp,                   gr = gfp,                   lambdas = seq(0,1,.1),                   thetas = seq(0.1,1,.1),                  additionalArguments = data)  l@parameters"},{"path":"/reference/gpMcp.html","id":null,"dir":"Reference","previous_headings":"","what":"gpMcp — gpMcp","title":"gpMcp — gpMcp","text":"Implements mcp regularization general purpose optimization problems. penalty function given : $$p( x_j) = \\begin{cases} \\lambda |x_j| - x_j^2/(2\\theta) & \\text{} |x_j| \\leq \\theta\\lambda\\\\ \\theta\\lambda^2/2 & \\text{} |x_j| > \\lambda\\theta \\end{cases}$$ \\(\\theta > 0\\).","code":""},{"path":"/reference/gpMcp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"gpMcp — gpMcp","text":"","code":"gpMcp(   par,   fn,   gr = NULL,   ...,   regularized,   lambdas,   thetas,   control = lessSEM::controlIsta() )"},{"path":"/reference/gpMcp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"gpMcp — gpMcp","text":"par labeled vector starting values fn R function takes parameters labels input returns fit value (single value) gr R function takes parameters labels input returns gradients objective function. set NULL, numDeriv used approximate gradients ... additional arguments passed fn gr regularized vector names parameters regularized. lambdas numeric vector: values tuning parameter lambda thetas numeric vector: values tuning parameter theta control used control optimizer. element generated controlIsta (see ?controlIsta)","code":""},{"path":"/reference/gpMcp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"gpMcp — gpMcp","text":"Object class gpRegularized","code":""},{"path":"/reference/gpMcp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"gpMcp — gpMcp","text":"interface similar optim. Users supply vector starting values (important: vector must labels) fitting function. fitting functions must take labeled vector parameter values first argument. remaining arguments passed ... argument. similar optim. gradient function gr optional. set NULL, numDeriv package used approximate gradients. Supplying gradient function can result considerable speed improvements. mcp regularization: Zhang, C.-H. (2010). Nearly unbiased variable selection minimax concave penalty. Annals Statistics, 38(2), 894–942. https://doi.org/10.1214/09-AOS729 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classiﬁcation. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/gpMcp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"gpMcp — gpMcp","text":"","code":"# This example shows how to use the optimizers # for other objective functions. We will use # a linear regression as an example. Note that # this is not a useful application of the optimizers # as there are specialized packages for linear regression # (e.g., glmnet)  library(lessSEM) set.seed(123)  # first, we simulate data for our # linear regression. N <- 100 # number of persons p <- 10 # number of predictors X <- matrix(rnorm(N*p),  nrow = N, ncol = p) # design matrix b <- c(rep(1,4),        rep(0,6)) # true regression weights y <- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)  # First, we must construct a fiting function # which returns a single value. We will use # the residual sum squared as fitting function.  # Let's start setting up the fitting function: fittingFunction <- function(par, y, X, N){   # par is the parameter vector   # y is the observed dependent variable   # X is the design matrix   # N is the sample size   pred <- X %*% matrix(par, ncol = 1) #be explicit here:   # we need par to be a column vector   sse <- sum((y - pred)^2)   # we scale with .5/N to get the same results as glmnet   return((.5/N)*sse) }  # let's define the starting values: # first, let's add an intercept X <- cbind(1, X)  b <- c(solve(t(X)%*%X)%*%t(X)%*%y) # we will use the lm estimates names(b) <- paste0(\"b\", 0:(length(b)-1)) # names of regularized parameters regularized <- paste0(\"b\",1:p)  # optimize mcpPen <- gpMcp(   par = b,   regularized = regularized,   fn = fittingFunction,   lambdas = seq(0,1,.1),   thetas = c(1.001, 1.5, 2),   X = X,   y = y,   N = N )  # optional: plot requires plotly package # plot(mcpPen)  # for comparison library(ncvreg) mcpFit <- ncvreg(X = X[,-1],                   y = y,                   penalty = \"MCP\",                  lambda =  mcpPen@fits$lambda[15],                  gamma =  mcpPen@fits$theta[15]) coef(mcpFit) mcpPen@parameters[15,]"},{"path":"/reference/gpMcpCpp.html","id":null,"dir":"Reference","previous_headings":"","what":"gpMcpCpp — gpMcpCpp","title":"gpMcpCpp — gpMcpCpp","text":"Implements mcp regularization general purpose optimization problems C++ functions. penalty function given : $$p( x_j) = \\begin{cases} \\lambda |x_j| - x_j^2/(2\\theta) & \\text{} |x_j| \\leq \\theta\\lambda\\\\ \\theta\\lambda^2/2 & \\text{} |x_j| > \\lambda\\theta \\end{cases}$$ \\(\\theta > 0\\).","code":""},{"path":"/reference/gpMcpCpp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"gpMcpCpp — gpMcpCpp","text":"","code":"gpMcpCpp(   par,   fn,   gr,   additionalArguments,   regularized,   lambdas,   thetas,   control = lessSEM::controlIsta() )"},{"path":"/reference/gpMcpCpp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"gpMcpCpp — gpMcpCpp","text":"par labeled vector starting values fn R function takes parameters labels input returns fit value (single value) gr R function takes parameters labels input returns gradients objective function. set NULL, numDeriv used approximate gradients additionalArguments list additional arguments passed fn gr regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda thetas numeric vector: values tuning parameter theta control used control optimizer. element generated controlIsta (see ?controlIsta)","code":""},{"path":"/reference/gpMcpCpp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"gpMcpCpp — gpMcpCpp","text":"Object class gpRegularized","code":""},{"path":"/reference/gpMcpCpp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"gpMcpCpp — gpMcpCpp","text":"interface inspired optim, bit restrictive. Users supply vector starting values (important: vector must labels), fitting function, gradient function. fitting functions must take const Rcpp::NumericVector& parameter values first argument Rcpp::List& second argument mcp regularization: Zhang, C.-H. (2010). Nearly unbiased variable selection minimax concave penalty. Annals Statistics, 38(2), 894–942. https://doi.org/10.1214/09-AOS729 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classiﬁcation. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/gpMcpCpp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"gpMcpCpp — gpMcpCpp","text":"","code":"# This example shows how to use the optimizers # for C++ objective functions. We will use # a linear regression as an example. Note that # this is not a useful application of the optimizers # as there are specialized packages for linear regression # (e.g., glmnet)  library(Rcpp) library(lessSEM)  linreg <- ' // [[Rcpp::depends(RcppArmadillo)]] #include <RcppArmadillo.h>  // [[Rcpp::export]] double fitfunction(const Rcpp::NumericVector& parameters, Rcpp::List& data){   // extract all required elements:   arma::colvec b = Rcpp::as<arma::colvec>(parameters);   arma::colvec y = Rcpp::as<arma::colvec>(data[\"y\"]); // the dependent variable   arma::mat X = Rcpp::as<arma::mat>(data[\"X\"]); // the design matrix      // compute the sum of squared errors:     arma::mat sse = arma::trans(y-X*b)*(y-X*b);          // other packages, such as glmnet, scale the sse with      // 1/(2*N), where N is the sample size. We will do that here as well          sse *= 1.0/(2.0 * y.n_elem);          // note: We must return a double, but the sse is a matrix     // To get a double, just return the single value that is in      // this matrix:       return(sse(0,0)); }  // [[Rcpp::export]] arma::rowvec gradientfunction(const Rcpp::NumericVector& parameters, Rcpp::List& data){   // extract all required elements:   arma::colvec b = Rcpp::as<arma::colvec>(parameters);   arma::colvec y = Rcpp::as<arma::colvec>(data[\"y\"]); // the dependent variable   arma::mat X = Rcpp::as<arma::mat>(data[\"X\"]); // the design matrix      // note: we want to return our gradients as row-vector; therefore,   // we have to transpose the resulting column-vector:     arma::rowvec gradients = arma::trans(-2.0*X.t() * y + 2.0*X.t()*X*b);          // other packages, such as glmnet, scale the sse with      // 1/(2*N), where N is the sample size. We will do that here as well          gradients *= (.5/y.n_rows);          return(gradients); }  // https://gallery.rcpp.org/articles/passing-cpp-function-pointers/ typedef double (*fitFunPtr)(const Rcpp::NumericVector&, //parameters                 Rcpp::List& //additional elements ); typedef Rcpp::XPtr<fitFunPtr> fitFunPtr_t;  typedef arma::rowvec (*gradientFunPtr)(const Rcpp::NumericVector&, //parameters                       Rcpp::List& //additional elements ); typedef Rcpp::XPtr<gradientFunPtr> gradientFunPtr_t;  // [[Rcpp::export]] fitFunPtr_t fitfunPtr() {         return(fitFunPtr_t(new fitFunPtr(&fitfunction))); }  // [[Rcpp::export]] gradientFunPtr_t gradfunPtr() {         return(gradientFunPtr_t(new gradientFunPtr(&gradientfunction))); } '  Rcpp::sourceCpp(code = linreg)  ffp <- fitfunPtr() gfp <- gradfunPtr()  N <- 100 # number of persons p <- 10 # number of predictors X <- matrix(rnorm(N*p),  nrow = N, ncol = p) # design matrix b <- c(rep(1,4),         rep(0,6)) # true regression weights y <- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)  data <- list(\"y\" = y,              \"X\" = cbind(1,X)) parameters <- rep(0, ncol(data$X)) names(parameters) <- paste0(\"b\", 0:(length(parameters)-1))  m <- gpMcpCpp(par = parameters,                   regularized = paste0(\"b\", 1:(length(b)-1)),                  fn = ffp,                   gr = gfp,                   lambdas = seq(0,1,.1),                   thetas = seq(.1,1,.1),                  additionalArguments = data)  m@parameters"},{"path":"/reference/gpRegularized-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Class for regularized model using general purpose optimization interface — gpRegularized-class","title":"Class for regularized model using general purpose optimization interface — gpRegularized-class","text":"Class regularized model using general purpose optimization interface","code":""},{"path":"/reference/gpRegularized-class.html","id":"slots","dir":"Reference","previous_headings":"","what":"Slots","title":"Class for regularized model using general purpose optimization interface — gpRegularized-class","text":"penalty penalty used (e.g., \"lasso\") parameters data.frame parameter estimates fits data.frame fit results parameterLabels character vector names parameters weights vector weights given parameters penalty regularized character vector names regularized parameters internalOptimization list elements used internally inputArguments list elements passed user general purpose optimizer","code":""},{"path":"/reference/gpRidge.html","id":null,"dir":"Reference","previous_headings":"","what":"gpRidge — gpRidge","title":"gpRidge — gpRidge","text":"Implements ridge regularization general purpose optimization problems. penalty function given : $$p( x_j) = \\lambda x_j^2$$ Note ridge regularization set parameters zero result shrinkage towards zero.","code":""},{"path":"/reference/gpRidge.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"gpRidge — gpRidge","text":"","code":"gpRidge(   par,   regularized,   fn,   gr = NULL,   lambdas,   ...,   method = \"glmnet\",   control = lessSEM::controlGlmnet() )"},{"path":"/reference/gpRidge.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"gpRidge — gpRidge","text":"par labeled vector starting values regularized vector names parameters regularized. fn R function takes parameters input returns fit value (single value) gr R function takes parameters input returns gradients objective function. set NULL, numDeriv used approximate gradients lambdas numeric vector: values tuning parameter lambda ... additional arguments passed fn gr method optimizer used? Currently implemented ista glmnet. ista, control argument can used switch related procedures (currently gist). control used control optimizer. element generated controlIsta controlGlmnet functions. See ?controlIsta ?controlGlmnet details.","code":""},{"path":"/reference/gpRidge.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"gpRidge — gpRidge","text":"Object class gpRegularized","code":""},{"path":"/reference/gpRidge.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"gpRidge — gpRidge","text":"interface similar optim. Users supply vector starting values (important: vector must labels) fitting function. fitting functions must take labeled vector parameter values first argument. remaining arguments passed ... argument. similar optim. gradient function gr optional. set NULL, numDeriv package used approximate gradients. Supplying gradient function can result considerable speed improvements. Ridge regularization: Hoerl, . E., & Kennard, R. W. (1970). Ridge Regression: Biased Estimation Nonorthogonal Problems. Technometrics, 12(1), 55–67. https://doi.org/10.1080/00401706.1970.10488634 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classiﬁcation. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/gpRidge.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"gpRidge — gpRidge","text":"","code":"# This example shows how to use the optimizers # for other objective functions. We will use # a linear regression as an example. Note that # this is not a useful application of the optimizers # as there are specialized packages for linear regression # (e.g., glmnet)  library(lessSEM) set.seed(123)  # first, we simulate data for our # linear regression. N <- 100 # number of persons p <- 10 # number of predictors X <- matrix(rnorm(N*p),  nrow = N, ncol = p) # design matrix b <- c(rep(1,4),        rep(0,6)) # true regression weights y <- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)  # First, we must construct a fiting function # which returns a single value. We will use # the residual sum squared as fitting function.  # Let's start setting up the fitting function: fittingFunction <- function(par, y, X, N){   # par is the parameter vector   # y is the observed dependent variable   # X is the design matrix   # N is the sample size   pred <- X %*% matrix(par, ncol = 1) #be explicit here:   # we need par to be a column vector   sse <- sum((y - pred)^2)   # we scale with .5/N to get the same results as glmnet   return((.5/N)*sse) }  # let's define the starting values: b <- c(solve(t(X)%*%X)%*%t(X)%*%y) # we will use the lm estimates names(b) <- paste0(\"b\", 1:length(b)) # names of regularized parameters regularized <- paste0(\"b\",1:p)  # optimize ridgePen <- gpRidge(   par = b,   regularized = regularized,   fn = fittingFunction,   lambdas = seq(0,1,.01),   X = X,   y = y,   N = N ) plot(ridgePen)  # for comparison: # fittingFunction <- function(par, y, X, N, lambda){ #   pred <- X %*% matrix(par, ncol = 1)  #   sse <- sum((y - pred)^2) #   return((.5/N)*sse + lambda * sum(par^2)) # } #  # optim(par = b,  #       fn = fittingFunction,  #       y = y, #       X = X, #       N = N, #       lambda =  ridgePen@fits$lambda[20],  #       method = \"BFGS\")$par # ridgePen@parameters[20,]"},{"path":"/reference/gpRidgeCpp.html","id":null,"dir":"Reference","previous_headings":"","what":"gpRidgeCpp — gpRidgeCpp","title":"gpRidgeCpp — gpRidgeCpp","text":"Implements ridge regularization general purpose optimization problems C++ functions. penalty function given : $$p( x_j) = \\lambda x_j^2$$ Note ridge regularization set parameters zero result shrinkage towards zero.","code":""},{"path":"/reference/gpRidgeCpp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"gpRidgeCpp — gpRidgeCpp","text":"","code":"gpRidgeCpp(   par,   regularized,   fn,   gr,   lambdas,   additionalArguments,   method = \"glmnet\",   control = lessSEM::controlGlmnet() )"},{"path":"/reference/gpRidgeCpp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"gpRidgeCpp — gpRidgeCpp","text":"par labeled vector starting values regularized vector names parameters regularized. fn R function takes parameters input returns fit value (single value) gr R function takes parameters input returns gradients objective function. set NULL, numDeriv used approximate gradients lambdas numeric vector: values tuning parameter lambda additionalArguments list additional arguments passed fn gr method optimizer used? Currently implemented ista glmnet. ista, control argument can used switch related procedures (currently gist). control used control optimizer. element generated controlIsta controlGlmnet functions. See ?controlIsta ?controlGlmnet details.","code":""},{"path":"/reference/gpRidgeCpp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"gpRidgeCpp — gpRidgeCpp","text":"Object class gpRegularized","code":""},{"path":"/reference/gpRidgeCpp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"gpRidgeCpp — gpRidgeCpp","text":"interface inspired optim, bit restrictive. Users supply vector starting values (important: vector must labels), fitting function, gradient function. fitting functions must take const Rcpp::NumericVector& parameter values first argument Rcpp::List& second argument Ridge regularization: Hoerl, . E., & Kennard, R. W. (1970). Ridge Regression: Biased Estimation Nonorthogonal Problems. Technometrics, 12(1), 55–67. https://doi.org/10.1080/00401706.1970.10488634 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classiﬁcation. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/gpRidgeCpp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"gpRidgeCpp — gpRidgeCpp","text":"","code":"# This example shows how to use the optimizers # for C++ objective functions. We will use # a linear regression as an example. Note that # this is not a useful application of the optimizers # as there are specialized packages for linear regression # (e.g., glmnet)  library(Rcpp) library(lessSEM)  linreg <- ' // [[Rcpp::depends(RcppArmadillo)]] #include <RcppArmadillo.h>  // [[Rcpp::export]] double fitfunction(const Rcpp::NumericVector& parameters, Rcpp::List& data){   // extract all required elements:   arma::colvec b = Rcpp::as<arma::colvec>(parameters);   arma::colvec y = Rcpp::as<arma::colvec>(data[\"y\"]); // the dependent variable   arma::mat X = Rcpp::as<arma::mat>(data[\"X\"]); // the design matrix      // compute the sum of squared errors:     arma::mat sse = arma::trans(y-X*b)*(y-X*b);          // other packages, such as glmnet, scale the sse with      // 1/(2*N), where N is the sample size. We will do that here as well          sse *= 1.0/(2.0 * y.n_elem);          // note: We must return a double, but the sse is a matrix     // To get a double, just return the single value that is in      // this matrix:       return(sse(0,0)); }  // [[Rcpp::export]] arma::rowvec gradientfunction(const Rcpp::NumericVector& parameters, Rcpp::List& data){   // extract all required elements:   arma::colvec b = Rcpp::as<arma::colvec>(parameters);   arma::colvec y = Rcpp::as<arma::colvec>(data[\"y\"]); // the dependent variable   arma::mat X = Rcpp::as<arma::mat>(data[\"X\"]); // the design matrix      // note: we want to return our gradients as row-vector; therefore,   // we have to transpose the resulting column-vector:     arma::rowvec gradients = arma::trans(-2.0*X.t() * y + 2.0*X.t()*X*b);          // other packages, such as glmnet, scale the sse with      // 1/(2*N), where N is the sample size. We will do that here as well          gradients *= (.5/y.n_rows);          return(gradients); }  // https://gallery.rcpp.org/articles/passing-cpp-function-pointers/ typedef double (*fitFunPtr)(const Rcpp::NumericVector&, //parameters                 Rcpp::List& //additional elements ); typedef Rcpp::XPtr<fitFunPtr> fitFunPtr_t;  typedef arma::rowvec (*gradientFunPtr)(const Rcpp::NumericVector&, //parameters                       Rcpp::List& //additional elements ); typedef Rcpp::XPtr<gradientFunPtr> gradientFunPtr_t;  // [[Rcpp::export]] fitFunPtr_t fitfunPtr() {         return(fitFunPtr_t(new fitFunPtr(&fitfunction))); }  // [[Rcpp::export]] gradientFunPtr_t gradfunPtr() {         return(gradientFunPtr_t(new gradientFunPtr(&gradientfunction))); } '  Rcpp::sourceCpp(code = linreg)  ffp <- fitfunPtr() gfp <- gradfunPtr()  N <- 100 # number of persons p <- 10 # number of predictors X <- matrix(rnorm(N*p),  nrow = N, ncol = p) # design matrix b <- c(rep(1,4),         rep(0,6)) # true regression weights y <- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)  data <- list(\"y\" = y,              \"X\" = cbind(1,X)) parameters <- rep(0, ncol(data$X)) names(parameters) <- paste0(\"b\", 0:(length(parameters)-1))  r <- gpRidgeCpp(par = parameters,                   regularized = paste0(\"b\", 1:(length(b)-1)),                  fn = ffp,                   gr = gfp,                   lambdas = seq(0,1,.1),                   additionalArguments = data)  r@parameters"},{"path":"/reference/gpScad.html","id":null,"dir":"Reference","previous_headings":"","what":"gpScad — gpScad","title":"gpScad — gpScad","text":"Implements scad regularization general purpose optimization problems. penalty function given :  $$p( x_j) = \\begin{cases} \\lambda |x_j| & \\text{} |x_j| \\leq \\theta\\\\ \\frac{-x_j^2 + 2\\theta\\lambda |x_j| - \\lambda^2}{2(\\theta -1)} &  \\text{} \\lambda < |x_j| \\leq \\lambda\\theta \\\\ (\\theta + 1) \\lambda^2/2 & \\text{} |x_j| \\geq \\theta\\lambda\\\\ \\end{cases}$$ \\(\\theta > 2\\).","code":""},{"path":"/reference/gpScad.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"gpScad — gpScad","text":"","code":"gpScad(   par,   fn,   gr = NULL,   ...,   regularized,   lambdas,   thetas,   control = lessSEM::controlIsta() )"},{"path":"/reference/gpScad.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"gpScad — gpScad","text":"par labeled vector starting values fn R function takes parameters labels input returns fit value (single value) gr R function takes parameters labels input returns gradients objective function. set NULL, numDeriv used approximate gradients ... additional arguments passed fn gr regularized vector names parameters regularized. lambdas numeric vector: values tuning parameter lambda thetas numeric vector: values tuning parameter theta control used control optimizer. element generated controlIsta (see ?controlIsta)","code":""},{"path":"/reference/gpScad.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"gpScad — gpScad","text":"Object class gpRegularized","code":""},{"path":"/reference/gpScad.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"gpScad — gpScad","text":"interface similar optim. Users supply vector starting values (important: vector must labels) fitting function. fitting functions must take labeled vector parameter values first argument. remaining arguments passed ... argument. similar optim. gradient function gr optional. set NULL, numDeriv package used approximate gradients. Supplying gradient function can result considerable speed improvements. scad regularization: Fan, J., & Li, R. (2001). Variable selection via nonconcave penalized likelihood oracle properties. Journal American Statistical Association, 96(456), 1348–1360. https://doi.org/10.1198/016214501753382273 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classiﬁcation. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/gpScad.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"gpScad — gpScad","text":"","code":"# This example shows how to use the optimizers # for other objective functions. We will use # a linear regression as an example. Note that # this is not a useful application of the optimizers # as there are specialized packages for linear regression # (e.g., glmnet)  library(lessSEM) set.seed(123)  # first, we simulate data for our # linear regression. N <- 100 # number of persons p <- 10 # number of predictors X <- matrix(rnorm(N*p),  nrow = N, ncol = p) # design matrix b <- c(rep(1,4),        rep(0,6)) # true regression weights y <- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)  # First, we must construct a fiting function # which returns a single value. We will use # the residual sum squared as fitting function.  # Let's start setting up the fitting function: fittingFunction <- function(par, y, X, N){   # par is the parameter vector   # y is the observed dependent variable   # X is the design matrix   # N is the sample size   pred <- X %*% matrix(par, ncol = 1) #be explicit here:   # we need par to be a column vector   sse <- sum((y - pred)^2)   # we scale with .5/N to get the same results as glmnet   return((.5/N)*sse) }  # let's define the starting values: # first, let's add an intercept X <- cbind(1, X)  b <- c(solve(t(X)%*%X)%*%t(X)%*%y) # we will use the lm estimates names(b) <- paste0(\"b\", 0:(length(b)-1)) # names of regularized parameters regularized <- paste0(\"b\",1:p)  # optimize scadPen <- gpScad(   par = b,   regularized = regularized,   fn = fittingFunction,   lambdas = seq(0,1,.1),   thetas = c(2.001, 2.5, 5),   X = X,   y = y,   N = N )  # optional: plot requires plotly package # plot(scadPen)  # for comparison library(ncvreg) scadFit <- ncvreg(X = X[,-1],                   y = y,                   penalty = \"SCAD\",                  lambda =  scadPen@fits$lambda[15],                  gamma =  scadPen@fits$theta[15]) coef(scadFit) scadPen@parameters[15,]"},{"path":"/reference/gpScadCpp.html","id":null,"dir":"Reference","previous_headings":"","what":"gpScadCpp — gpScadCpp","title":"gpScadCpp — gpScadCpp","text":"Implements scad regularization general purpose optimization problems C++ functions. penalty function given :  $$p( x_j) = \\begin{cases} \\lambda |x_j| & \\text{} |x_j| \\leq \\theta\\\\ \\frac{-x_j^2 + 2\\theta\\lambda |x_j| - \\lambda^2}{2(\\theta -1)} &  \\text{} \\lambda < |x_j| \\leq \\lambda\\theta \\\\ (\\theta + 1) \\lambda^2/2 & \\text{} |x_j| \\geq \\theta\\lambda\\\\ \\end{cases}$$ \\(\\theta > 2\\).","code":""},{"path":"/reference/gpScadCpp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"gpScadCpp — gpScadCpp","text":"","code":"gpScadCpp(   par,   fn,   gr,   additionalArguments,   regularized,   lambdas,   thetas,   control = lessSEM::controlIsta() )"},{"path":"/reference/gpScadCpp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"gpScadCpp — gpScadCpp","text":"par labeled vector starting values fn R function takes parameters labels input returns fit value (single value) gr R function takes parameters labels input returns gradients objective function. set NULL, numDeriv used approximate gradients additionalArguments list additional arguments passed fn gr regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda thetas numeric vector: values tuning parameter theta control used control optimizer. element generated controlIsta (see ?controlIsta)","code":""},{"path":"/reference/gpScadCpp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"gpScadCpp — gpScadCpp","text":"Object class gpRegularized","code":""},{"path":"/reference/gpScadCpp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"gpScadCpp — gpScadCpp","text":"interface inspired optim, bit restrictive. Users supply vector starting values (important: vector must labels), fitting function, gradient function. fitting functions must take const Rcpp::NumericVector& parameter values first argument Rcpp::List& second argument scad regularization: Fan, J., & Li, R. (2001). Variable selection via nonconcave penalized likelihood oracle properties. Journal American Statistical Association, 96(456), 1348–1360. https://doi.org/10.1198/016214501753382273 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classiﬁcation. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/gpScadCpp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"gpScadCpp — gpScadCpp","text":"","code":"# This example shows how to use the optimizers # for C++ objective functions. We will use # a linear regression as an example. Note that # this is not a useful application of the optimizers # as there are specialized packages for linear regression # (e.g., glmnet)  library(Rcpp) library(lessSEM)  linreg <- ' // [[Rcpp::depends(RcppArmadillo)]] #include <RcppArmadillo.h>  // [[Rcpp::export]] double fitfunction(const Rcpp::NumericVector& parameters, Rcpp::List& data){   // extract all required elements:   arma::colvec b = Rcpp::as<arma::colvec>(parameters);   arma::colvec y = Rcpp::as<arma::colvec>(data[\"y\"]); // the dependent variable   arma::mat X = Rcpp::as<arma::mat>(data[\"X\"]); // the design matrix      // compute the sum of squared errors:     arma::mat sse = arma::trans(y-X*b)*(y-X*b);          // other packages, such as glmnet, scale the sse with      // 1/(2*N), where N is the sample size. We will do that here as well          sse *= 1.0/(2.0 * y.n_elem);          // note: We must return a double, but the sse is a matrix     // To get a double, just return the single value that is in      // this matrix:       return(sse(0,0)); }  // [[Rcpp::export]] arma::rowvec gradientfunction(const Rcpp::NumericVector& parameters, Rcpp::List& data){   // extract all required elements:   arma::colvec b = Rcpp::as<arma::colvec>(parameters);   arma::colvec y = Rcpp::as<arma::colvec>(data[\"y\"]); // the dependent variable   arma::mat X = Rcpp::as<arma::mat>(data[\"X\"]); // the design matrix      // note: we want to return our gradients as row-vector; therefore,   // we have to transpose the resulting column-vector:     arma::rowvec gradients = arma::trans(-2.0*X.t() * y + 2.0*X.t()*X*b);          // other packages, such as glmnet, scale the sse with      // 1/(2*N), where N is the sample size. We will do that here as well          gradients *= (.5/y.n_rows);          return(gradients); }  // https://gallery.rcpp.org/articles/passing-cpp-function-pointers/ typedef double (*fitFunPtr)(const Rcpp::NumericVector&, //parameters                 Rcpp::List& //additional elements ); typedef Rcpp::XPtr<fitFunPtr> fitFunPtr_t;  typedef arma::rowvec (*gradientFunPtr)(const Rcpp::NumericVector&, //parameters                       Rcpp::List& //additional elements ); typedef Rcpp::XPtr<gradientFunPtr> gradientFunPtr_t;  // [[Rcpp::export]] fitFunPtr_t fitfunPtr() {         return(fitFunPtr_t(new fitFunPtr(&fitfunction))); }  // [[Rcpp::export]] gradientFunPtr_t gradfunPtr() {         return(gradientFunPtr_t(new gradientFunPtr(&gradientfunction))); } '  Rcpp::sourceCpp(code = linreg)  ffp <- fitfunPtr() gfp <- gradfunPtr()  N <- 100 # number of persons p <- 10 # number of predictors X <- matrix(rnorm(N*p),  nrow = N, ncol = p) # design matrix b <- c(rep(1,4),         rep(0,6)) # true regression weights y <- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)  data <- list(\"y\" = y,              \"X\" = cbind(1,X)) parameters <- rep(0, ncol(data$X)) names(parameters) <- paste0(\"b\", 0:(length(parameters)-1))  s <- gpScadCpp(par = parameters,                   regularized = paste0(\"b\", 1:(length(b)-1)),                  fn = ffp,                   gr = gfp,                   lambdas = seq(0,1,.1),                   thetas = seq(2.1,3,.1),                  additionalArguments = data)  s@parameters"},{"path":"/reference/istaCappedL1GeneralPurpose.html","id":null,"dir":"Reference","previous_headings":"","what":"cappedL1 optimization with ista — istaCappedL1GeneralPurpose","title":"cappedL1 optimization with ista — istaCappedL1GeneralPurpose","text":"Object cappedL1 optimization ista optimizer","code":""},{"path":"/reference/istaCappedL1GeneralPurpose.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"cappedL1 optimization with ista — istaCappedL1GeneralPurpose","text":"list fit results","code":""},{"path":"/reference/istaCappedL1GeneralPurpose.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"cappedL1 optimization with ista — istaCappedL1GeneralPurpose","text":"new creates new object. Requires (1) vector weights parameter (2) list control elements optimize optimize model. Expects vector starting values, R function compute fit, R function compute gradients, list elements fit gradient function require, theta, lambda alpha value (alpha must 1).","code":""},{"path":"/reference/istaCappedL1GeneralPurposeCpp.html","id":null,"dir":"Reference","previous_headings":"","what":"cappedL1 optimization with ista — istaCappedL1GeneralPurposeCpp","title":"cappedL1 optimization with ista — istaCappedL1GeneralPurposeCpp","text":"Object cappedL1 optimization ista optimizer","code":""},{"path":"/reference/istaCappedL1GeneralPurposeCpp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"cappedL1 optimization with ista — istaCappedL1GeneralPurposeCpp","text":"list fit results","code":""},{"path":"/reference/istaCappedL1GeneralPurposeCpp.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"cappedL1 optimization with ista — istaCappedL1GeneralPurposeCpp","text":"new creates new object. Requires (1) vector weights parameter (2) list control elements optimize optimize model. Expects vector starting values, SEXP function pointer compute fit, SEXP function pointer compute gradients, list elements fit gradient function require, theta, lambda alpha value (alpha must 1).","code":""},{"path":"/reference/istaCappedL1SEM.html","id":null,"dir":"Reference","previous_headings":"","what":"cappedL1 optimization with ista — istaCappedL1SEM","title":"cappedL1 optimization with ista — istaCappedL1SEM","text":"Object elastic net optimization ista optimizer","code":""},{"path":"/reference/istaCappedL1SEM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"cappedL1 optimization with ista — istaCappedL1SEM","text":"list fit results","code":""},{"path":"/reference/istaCappedL1SEM.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"cappedL1 optimization with ista — istaCappedL1SEM","text":"new creates new object. Requires (1) vector weights parameter (2) list control elements optimize optimize model. Expects vector starting values, SEM type SEM_Cpp, theta value, lambda alpha value (alpha must 1).","code":""},{"path":"/reference/istaCappedL1mgSEM.html","id":null,"dir":"Reference","previous_headings":"","what":"cappedL1 optimization with ista — istaCappedL1mgSEM","title":"cappedL1 optimization with ista — istaCappedL1mgSEM","text":"Object elastic net optimization ista optimizer","code":""},{"path":"/reference/istaCappedL1mgSEM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"cappedL1 optimization with ista — istaCappedL1mgSEM","text":"list fit results","code":""},{"path":"/reference/istaCappedL1mgSEM.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"cappedL1 optimization with ista — istaCappedL1mgSEM","text":"new creates new object. Requires (1) vector weights parameter (2) list control elements optimize optimize model. Expects vector starting values, SEM type SEM_Cpp, theta value, lambda alpha value (alpha must 1).","code":""},{"path":"/reference/istaEnetGeneralPurpose.html","id":null,"dir":"Reference","previous_headings":"","what":"elastic net optimization with ista — istaEnetGeneralPurpose","title":"elastic net optimization with ista — istaEnetGeneralPurpose","text":"Object elastic net optimization ista optimizer","code":""},{"path":"/reference/istaEnetGeneralPurpose.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"elastic net optimization with ista — istaEnetGeneralPurpose","text":"list fit results","code":""},{"path":"/reference/istaEnetGeneralPurpose.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"elastic net optimization with ista — istaEnetGeneralPurpose","text":"new creates new object. Requires (1) vector weights parameter (2) list control elements optimize optimize model. Expects vector starting values, R function compute fit, R function compute gradients, list elements fit gradient function require, lambda alpha value.","code":""},{"path":"/reference/istaEnetGeneralPurposeCpp.html","id":null,"dir":"Reference","previous_headings":"","what":"elastic net optimization with ista — istaEnetGeneralPurposeCpp","title":"elastic net optimization with ista — istaEnetGeneralPurposeCpp","text":"Object elastic net optimization ista optimizer","code":""},{"path":"/reference/istaEnetGeneralPurposeCpp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"elastic net optimization with ista — istaEnetGeneralPurposeCpp","text":"list fit results","code":""},{"path":"/reference/istaEnetGeneralPurposeCpp.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"elastic net optimization with ista — istaEnetGeneralPurposeCpp","text":"new creates new object. Requires (1) vector weights parameter (2) list control elements optimize optimize model. Expects vector starting values, SEXP function pointer compute fit, SEXP function pointer compute gradients, list elements fit gradient function require, lambda alpha value.","code":""},{"path":"/reference/istaEnetMgSEM.html","id":null,"dir":"Reference","previous_headings":"","what":"elastic net optimization with ista optimizer — istaEnetMgSEM","title":"elastic net optimization with ista optimizer — istaEnetMgSEM","text":"Object elastic net optimization glmnet optimizer","code":""},{"path":"/reference/istaEnetMgSEM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"elastic net optimization with ista optimizer — istaEnetMgSEM","text":"list fit results","code":""},{"path":"/reference/istaEnetMgSEM.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"elastic net optimization with ista optimizer — istaEnetMgSEM","text":"new creates new object. Requires (1) vector weights parameter (2) list control elements optimize optimize model. Expects vector starting values, SEM type SEM_Cpp, lambda alpha value.","code":""},{"path":"/reference/istaEnetSEM.html","id":null,"dir":"Reference","previous_headings":"","what":"elastic net optimization with ista optimizer — istaEnetSEM","title":"elastic net optimization with ista optimizer — istaEnetSEM","text":"Object elastic net optimization glmnet optimizer","code":""},{"path":"/reference/istaEnetSEM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"elastic net optimization with ista optimizer — istaEnetSEM","text":"list fit results","code":""},{"path":"/reference/istaEnetSEM.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"elastic net optimization with ista optimizer — istaEnetSEM","text":"new creates new object. Requires (1) vector weights parameter (2) list control elements optimize optimize model. Expects vector starting values, SEM type SEM_Cpp, lambda alpha value.","code":""},{"path":"/reference/istaLSPMgSEM.html","id":null,"dir":"Reference","previous_headings":"","what":"lsp optimization with ista — istaLSPMgSEM","title":"lsp optimization with ista — istaLSPMgSEM","text":"Object lsp optimization ista optimizer","code":""},{"path":"/reference/istaLSPMgSEM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"lsp optimization with ista — istaLSPMgSEM","text":"list fit results","code":""},{"path":"/reference/istaLSPMgSEM.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"lsp optimization with ista — istaLSPMgSEM","text":"new creates new object. Requires (1) vector weights parameter (2) list control elements optimize optimize model. Expects vector starting values, SEM type SEM_Cpp, theta lambda value.","code":""},{"path":"/reference/istaLSPSEM.html","id":null,"dir":"Reference","previous_headings":"","what":"lsp optimization with ista — istaLSPSEM","title":"lsp optimization with ista — istaLSPSEM","text":"Object lsp optimization ista optimizer","code":""},{"path":"/reference/istaLSPSEM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"lsp optimization with ista — istaLSPSEM","text":"list fit results","code":""},{"path":"/reference/istaLSPSEM.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"lsp optimization with ista — istaLSPSEM","text":"new creates new object. Requires (1) vector weights parameter (2) list control elements optimize optimize model. Expects vector starting values, SEM type SEM_Cpp, theta lambda value.","code":""},{"path":"/reference/istaLspGeneralPurpose.html","id":null,"dir":"Reference","previous_headings":"","what":"lsp optimization with ista — istaLspGeneralPurpose","title":"lsp optimization with ista — istaLspGeneralPurpose","text":"Object lsp optimization ista optimizer","code":""},{"path":"/reference/istaLspGeneralPurpose.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"lsp optimization with ista — istaLspGeneralPurpose","text":"list fit results","code":""},{"path":"/reference/istaLspGeneralPurpose.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"lsp optimization with ista — istaLspGeneralPurpose","text":"new creates new object. Requires (1) vector weights parameter (2) list control elements optimize optimize model. Expects vector starting values, R function compute fit, R function compute gradients, list elements fit gradient function require, theta lambda value (alpha must 1).","code":""},{"path":"/reference/istaLspGeneralPurposeCpp.html","id":null,"dir":"Reference","previous_headings":"","what":"lsp optimization with ista — istaLspGeneralPurposeCpp","title":"lsp optimization with ista — istaLspGeneralPurposeCpp","text":"Object lsp optimization ista optimizer","code":""},{"path":"/reference/istaLspGeneralPurposeCpp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"lsp optimization with ista — istaLspGeneralPurposeCpp","text":"list fit results","code":""},{"path":"/reference/istaLspGeneralPurposeCpp.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"lsp optimization with ista — istaLspGeneralPurposeCpp","text":"new creates new object. Requires (1) vector weights parameter (2) list control elements optimize optimize model. Expects vector starting values, SEXP function pointer compute fit, SEXP function pointer compute gradients, list elements fit gradient function require, theta lambda value (alpha must 1).","code":""},{"path":"/reference/istaMcpGeneralPurpose.html","id":null,"dir":"Reference","previous_headings":"","what":"mcp optimization with ista — istaMcpGeneralPurpose","title":"mcp optimization with ista — istaMcpGeneralPurpose","text":"Object mcp optimization ista optimizer","code":""},{"path":"/reference/istaMcpGeneralPurpose.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"mcp optimization with ista — istaMcpGeneralPurpose","text":"list fit results","code":""},{"path":"/reference/istaMcpGeneralPurpose.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"mcp optimization with ista — istaMcpGeneralPurpose","text":"new creates new object. Requires (1) vector weights parameter (2) list control elements optimize optimize model. Expects vector starting values, R function compute fit, R function compute gradients, list elements fit gradient function require, theta lambda value (alpha must 1).","code":""},{"path":"/reference/istaMcpGeneralPurposeCpp.html","id":null,"dir":"Reference","previous_headings":"","what":"mcp optimization with ista — istaMcpGeneralPurposeCpp","title":"mcp optimization with ista — istaMcpGeneralPurposeCpp","text":"Object mcp optimization ista optimizer","code":""},{"path":"/reference/istaMcpGeneralPurposeCpp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"mcp optimization with ista — istaMcpGeneralPurposeCpp","text":"list fit results","code":""},{"path":"/reference/istaMcpGeneralPurposeCpp.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"mcp optimization with ista — istaMcpGeneralPurposeCpp","text":"new creates new object. Requires (1) vector weights parameter (2) list control elements optimize optimize model. Expects vector starting values, SEXP function pointer compute fit, SEXP function pointer compute gradients, list elements fit gradient function require, theta lambda value (alpha must 1).","code":""},{"path":"/reference/istaMcpMgSEM.html","id":null,"dir":"Reference","previous_headings":"","what":"mcp optimization with ista — istaMcpMgSEM","title":"mcp optimization with ista — istaMcpMgSEM","text":"Object mcp optimization ista optimizer","code":""},{"path":"/reference/istaMcpMgSEM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"mcp optimization with ista — istaMcpMgSEM","text":"list fit results","code":""},{"path":"/reference/istaMcpMgSEM.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"mcp optimization with ista — istaMcpMgSEM","text":"new creates new object. Requires (1) vector weights parameter (2) list control elements optimize optimize model. Expects vector starting values, SEM type SEM_Cpp, theta lambda value.","code":""},{"path":"/reference/istaMcpSEM.html","id":null,"dir":"Reference","previous_headings":"","what":"mcp optimization with ista — istaMcpSEM","title":"mcp optimization with ista — istaMcpSEM","text":"Object mcp optimization ista optimizer","code":""},{"path":"/reference/istaMcpSEM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"mcp optimization with ista — istaMcpSEM","text":"list fit results","code":""},{"path":"/reference/istaMcpSEM.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"mcp optimization with ista — istaMcpSEM","text":"new creates new object. Requires (1) vector weights parameter (2) list control elements optimize optimize model. Expects vector starting values, SEM type SEM_Cpp, theta lambda value.","code":""},{"path":"/reference/istaMixedPenaltySEM.html","id":null,"dir":"Reference","previous_headings":"","what":"mixed penalty optimization with ista — istaMixedPenaltySEM","title":"mixed penalty optimization with ista — istaMixedPenaltySEM","text":"Object elastic net optimization ista optimizer","code":""},{"path":"/reference/istaMixedPenaltySEM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"mixed penalty optimization with ista — istaMixedPenaltySEM","text":"list fit results","code":""},{"path":"/reference/istaMixedPenaltySEM.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"mixed penalty optimization with ista — istaMixedPenaltySEM","text":"new creates new object. Requires (1) vector weights parameter, (2) vector indicating penalty used, (3) list control elements optimize optimize model. Expects vector starting values, SEM type SEM_Cpp, theta value, lambda alpha value (alpha must 1).","code":""},{"path":"/reference/istaMixedPenaltymgSEM.html","id":null,"dir":"Reference","previous_headings":"","what":"mixed penalty optimization with ista — istaMixedPenaltymgSEM","title":"mixed penalty optimization with ista — istaMixedPenaltymgSEM","text":"Object elastic net optimization ista optimizer","code":""},{"path":"/reference/istaMixedPenaltymgSEM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"mixed penalty optimization with ista — istaMixedPenaltymgSEM","text":"list fit results","code":""},{"path":"/reference/istaMixedPenaltymgSEM.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"mixed penalty optimization with ista — istaMixedPenaltymgSEM","text":"new creates new object. Requires (1) vector weights parameter, (2) vector indicating penalty used, (3) list control elements optimize optimize model. Expects vector starting values, SEM type SEM_Cpp, theta value, lambda alpha value (alpha must 1).","code":""},{"path":"/reference/istaScadGeneralPurpose.html","id":null,"dir":"Reference","previous_headings":"","what":"scad optimization with ista — istaScadGeneralPurpose","title":"scad optimization with ista — istaScadGeneralPurpose","text":"Object scad optimization ista optimizer","code":""},{"path":"/reference/istaScadGeneralPurpose.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"scad optimization with ista — istaScadGeneralPurpose","text":"list fit results","code":""},{"path":"/reference/istaScadGeneralPurpose.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"scad optimization with ista — istaScadGeneralPurpose","text":"new creates new object. Requires (1) vector weights parameter (2) list control elements optimize optimize model. Expects vector starting values, R function compute fit, R function compute gradients, list elements fit gradient function require, theta lambda value (alpha must 1).","code":""},{"path":"/reference/istaScadGeneralPurposeCpp.html","id":null,"dir":"Reference","previous_headings":"","what":"scad optimization with ista — istaScadGeneralPurposeCpp","title":"scad optimization with ista — istaScadGeneralPurposeCpp","text":"Object scad optimization ista optimizer","code":""},{"path":"/reference/istaScadGeneralPurposeCpp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"scad optimization with ista — istaScadGeneralPurposeCpp","text":"list fit results","code":""},{"path":"/reference/istaScadGeneralPurposeCpp.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"scad optimization with ista — istaScadGeneralPurposeCpp","text":"new creates new object. Requires (1) vector weights parameter (2) list control elements optimize optimize model. Expects vector starting values, SEXP function pointer compute fit, SEXP function pointer compute gradients, list elements fit gradient function require, theta lambda value (alpha must 1).","code":""},{"path":"/reference/istaScadMgSEM.html","id":null,"dir":"Reference","previous_headings":"","what":"scad optimization with ista — istaScadMgSEM","title":"scad optimization with ista — istaScadMgSEM","text":"Object scad optimization ista optimizer","code":""},{"path":"/reference/istaScadMgSEM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"scad optimization with ista — istaScadMgSEM","text":"list fit results","code":""},{"path":"/reference/istaScadMgSEM.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"scad optimization with ista — istaScadMgSEM","text":"new creates new object. Requires (1) vector weights parameter (2) list control elements optimize optimize model. Expects vector starting values, SEM type SEM_Cpp, theta lambda value.","code":""},{"path":"/reference/istaScadSEM.html","id":null,"dir":"Reference","previous_headings":"","what":"scad optimization with ista — istaScadSEM","title":"scad optimization with ista — istaScadSEM","text":"Object scad optimization ista optimizer","code":""},{"path":"/reference/istaScadSEM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"scad optimization with ista — istaScadSEM","text":"list fit results","code":""},{"path":"/reference/istaScadSEM.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"scad optimization with ista — istaScadSEM","text":"new creates new object. Requires (1) vector weights parameter (2) list control elements optimize optimize model. Expects vector starting values, SEM type SEM_Cpp, theta lambda value.","code":""},{"path":"/reference/lasso.html","id":null,"dir":"Reference","previous_headings":"","what":"lasso — lasso","title":"lasso — lasso","text":"Implements lasso regularization structural equation models. penalty function given : $$p( x_j) = \\lambda |x_j|$$ Lasso regularization set parameters zero \\(\\lambda\\) large enough","code":""},{"path":"/reference/lasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"lasso — lasso","text":"","code":"lasso(   lavaanModel,   regularized,   lambdas = NULL,   nLambdas = NULL,   reverse = TRUE,   curve = 1,   method = \"glmnet\",   modifyModel = lessSEM::modifyModel(),   control = lessSEM::controlGlmnet() )"},{"path":"/reference/lasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"lasso — lasso","text":"lavaanModel model class lavaan regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda nLambdas alternative lambda: alpha = 1, lessSEM can automatically compute first lambda value sets regularized parameters zero. generate nLambda values 0 computed lambda. reverse set TRUE nLambdas used, lessSEM start largest lambda gradually decrease lambda. Otherwise, lessSEM start smallest lambda gradually increase . curve Allows unequally spaced lambda steps (e.g., .01,.02,.05,1,5,20). curve close 1 lambda values equally spaced, curve large lambda values concentrated close 0. See ?lessSEM::curveLambda information. method optimizer used? Currently implemented ista glmnet. ista, control argument can used switch related procedures (currently gist). modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. element generated controlIsta controlGlmnet functions. See ?controlIsta ?controlGlmnet details.","code":""},{"path":"/reference/lasso.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"lasso — lasso","text":"Model class regularizedSEM","code":""},{"path":"/reference/lasso.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"lasso — lasso","text":"Identical regsem, models specified using lavaan. Currently, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. Lasso regularization: Tibshirani, R. (1996). Regression shrinkage selection via lasso. Journal Royal Statistical Society. Series B (Methodological), 58(1), 267–288. Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classiﬁcation. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/lasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"lasso — lasso","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +       l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +       l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Optional: Plot the model # semPlot::semPaths(lavaanModel,  #                   what = \"est\", #                   fade = FALSE)  lsem <- lasso(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   # names of the regularized parameters:   regularized = paste0(\"l\", 6:15),   # in case of lasso and adaptive lasso, we can specify the number of lambda   # values to use. lessSEM will automatically find lambda_max and fit   # models for nLambda values between 0 and lambda_max. For the other   # penalty functions, lambdas must be specified explicitly   nLambdas = 50)  # use the plot-function to plot the regularized parameters: plot(lsem)  # the coefficients can be accessed with: coef(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters[1,]  # AIC and BIC: AIC(lsem) BIC(lsem)  # The best parameters can also be extracted with: coef(lsem, criterion = \"AIC\") coef(lsem, criterion = \"BIC\")   #### Advanced ### # Switching the optimizer #  # Use the \"method\" argument to switch the optimizer. The control argument # must also be changed to the corresponding function: lsemIsta <- lasso(   lavaanModel = lavaanModel,   regularized = paste0(\"l\", 6:15),   nLambdas = 50,   method = \"ista\",   control = controlIsta())  # Note: The results are basically identical: lsemIsta@parameters - lsem@parameters"},{"path":"/reference/lavaan2lslxLabels.html","id":null,"dir":"Reference","previous_headings":"","what":"lavaan2lslxLabels — lavaan2lslxLabels","title":"lavaan2lslxLabels — lavaan2lslxLabels","text":"helper function: lslx lavaan use slightly different parameter labels. function can used get sets labels.","code":""},{"path":"/reference/lavaan2lslxLabels.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"lavaan2lslxLabels — lavaan2lslxLabels","text":"","code":"lavaan2lslxLabels(lavaanModel)"},{"path":"/reference/lavaan2lslxLabels.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"lavaan2lslxLabels — lavaan2lslxLabels","text":"lavaanModel model class lavaan","code":""},{"path":"/reference/lavaan2lslxLabels.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"lavaan2lslxLabels — lavaan2lslxLabels","text":"list lavaan labels lslx labels","code":""},{"path":"/reference/lessSEM.html","id":null,"dir":"Reference","previous_headings":"","what":"lessSEM — lessSEM","title":"lessSEM — lessSEM","text":"Please see vignettes readme GitHub date description package","code":""},{"path":"/reference/lessSEM.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"lessSEM — lessSEM","text":"lessSEM (lessSEM estimates sparse SEM) R package provides regularized structural equation modeling (regularized SEM) non-smooth penalty functions (e.g., lasso) building lavaan. lessSEM heavily inspired regsem package lslx package provide similar functionality. objectives lessSEM provide ... flexible framework regularizing SEM optimizers SEM packages can used interface similar optim. Note: Please also check implementations regularized SEM mature R packages regsem lslx. Finally, may want check julia package StructuralEquationModels.jl. Currently, lessSEM following optimizers: (variants ) iterative shrinkage thresholding (e.g., Beck & Teboulle, 2009; Gong et al., 2013; Parikh & Boyd, 2013); optimization cappedL1, lsp, scad, mcp based Gong et al. (2013) glmnet (Friedman et al., 2010; Yuan et al., 2012; Huang, 2020) optimizers implemented based regCtsem package. importantly, optimizers lessSEM available packages. three ways implement documented vignette(\"General-Purpose-Optimization\", package = \"lessSEM\"). short, : using R interface: general purpose implementations functions called prefix \"gp\" (gpLasso, gpScad, ...). information examples can found documentation functions (e.g., ?lessSEM::gpLasso, ?lessSEM::gpAdaptiveLasso, ?lessSEM::gpElasticNet). interface similar optim optimizers R. using Rcpp, can pass C++ function pointers general purpose optimizers gpLassoCpp, gpScadCpp, ... (e.g., ?lessSEM::gpLassoCpp) optimizers implemented C++ header-files lessSEM. Thus, can accessed packages using C++. interface similar ensmallen library. implemented simple example elastic net regularization linear regressions lessLM package. can also find details general design optimizer interface vignette(\"-optimizer-interface\", package = \"lessSEM\"). Similar regsem, lessSEM specified using model built lavaan. lessSEM can handle missing data means full information maximum likelihood estimation allows equality constraints parameters. may, however, also want check regsem lslx offer features still missing lessSEM. distinct feature lessSEM parameter transformations (see example) mixed penalties. Furthermore, lessSEM provides multi-core support. See vignette(\"lessSEM\", package = \"lessSEM\") details.","code":""},{"path":"/reference/lessSEM.html","id":"installation","dir":"Reference","previous_headings":"","what":"Installation","title":"lessSEM — lessSEM","text":"want install lessSEM GitHub, use following commands R:","code":"if(!require(devtools))install.packages(\"devtools\") devtools::install_github(\"jhorzek/lessSEM\")"},{"path":"/reference/lessSEM.html","id":"introduction","dir":"Reference","previous_headings":"","what":"Introduction","title":"lessSEM — lessSEM","text":"find short introduction regularized SEM lessSEM package vignette('lessSEM', package = 'lessSEM'). information also provided documentation individual functions (e.g., see ?lessSEM::scad). Finally, find templates selection models can used lessSEM (e.g., cross-lagged panel model) package lessTemplates.","code":""},{"path":"/reference/lessSEM.html","id":"example","dir":"Reference","previous_headings":"","what":"Example","title":"lessSEM — lessSEM","text":"experimental feature used caution option extract lavaan object fitted regularized SEM. can useful fitting final model. case, best model given :   can get lavaan model parameters corresponding regularized model lambda = lambdaBest follows:   result can plotted , instance, semPlot:","code":"library(lessSEM) library(lavaan)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \"       f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +             l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +             l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15       f ~~ 1*f       \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Optional: Plot the model # semPlot::semPaths(lavaanModel,  #                   what = \"est\", #                   fade = FALSE)  rsem <- lasso(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   # names of the regularized parameters:   regularized = c(\"l6\", \"l7\", \"l8\", \"l9\", \"l10\",                   \"l11\", \"l12\", \"l13\", \"l14\", \"l15\"),   # in case of lasso and adaptive lasso, we can specify the number of lambda   # values to use. lessSEM will automatically find lambda_max and fit   # models for nLambda values between 0 and lambda_max. For the other   # penalty functions, lambdas must be specified explicitly   nLambdas = 50)  # use the plot-function to plot the regularized parameters: plot(rsem)  # elements of rsem can be accessed with the @ operator: rsem@parameters[1,]  # AIC and BIC: AIC(rsem) BIC(rsem)  # The best parameters can also be extracted with: coef(rsem, criterion = \"AIC\") coef(rsem, criterion = \"BIC\")  # cross-validation cv <- cvLasso(lavaanModel = lavaanModel,               regularized = c(\"l6\", \"l7\", \"l8\", \"l9\", \"l10\",                               \"l11\", \"l12\", \"l13\", \"l14\", \"l15\"),               lambdas = seq(0,1,.1),               standardize = TRUE)  # get best model according to cross-validation: coef(cv)  #### Advanced ### # Switching the optimizer #  # Use the \"method\" argument to switch the optimizer. The control argument # must also be changed to the corresponding function: rsemIsta <- lasso(   lavaanModel = lavaanModel,   regularized = paste0(\"l\", 6:15),   nLambdas = 50,   method = \"ista\",   control = controlIsta(     # Here, we can also specify that we want to use multiple cores:     nCores = 2))  # Note: The results are basically identical: rsemIsta@parameters - rsem@parameters lambdaBest <- coef(rsem, criterion = \"BIC\")$lambda lavaanModel <- lessSEM2Lavaan(regularizedSEM = rsem,                                lambda = lambdaBest) library(semPlot) semPaths(lavaanModel,          what = \"est\",          fade = FALSE)"},{"path":"/reference/lessSEM.html","id":"transformations","dir":"Reference","previous_headings":"","what":"Transformations","title":"lessSEM — lessSEM","text":"lessSEM allows parameter transformations , instance, used test measurement invariance longitudinal models (e.g., Liang, 2018; Bauer et al., 2020). thorough introduction provided vignette('Parameter-transformations', package = 'lessSEM'). example, test measurement invariance PoliticalDemocracy data set.   Finally, can extract best parameters:   differences (delta_a2, delta_b2, delta_c2) zeroed, can assume measurement invariance.","code":"library(lessSEM) library(lavaan) # we will use the PoliticalDemocracy from lavaan (see ?lavaan::sem) model <- '    # latent variable definitions      ind60 =~ x1 + x2 + x3      # assuming different loadings for different time points:      dem60 =~ y1 + a1*y2 + b1*y3 + c1*y4      dem65 =~ y5 + a2*y6 + b2*y7 + c2*y8    # regressions     dem60 ~ ind60     dem65 ~ ind60 + dem60    # residual correlations     y1 ~~ y5     y2 ~~ y4 + y6     y3 ~~ y7     y4 ~~ y8     y6 ~~ y8 '  fit <- sem(model, data = PoliticalDemocracy)  # We will define a transformation which regularizes differences # between loadings over time:  transformations <- \" // which parameters do we want to use? parameters: a1, a2, b1, b2, c1, c2, delta_a2, delta_b2, delta_c2  // transformations: a2 = a1 + delta_a2; b2 = b1 + delta_b2; c2 = c1 + delta_c2; \"  # setting delta_a2, delta_b2, or delta_c2 to zero implies measurement invariance # for the respective parameters (a1, b1, c1) lassoFit <- lasso(lavaanModel = lavaanFit,                    # we want to regularize the differences between the parameters                   regularized = c(\"delta_a2\", \"delta_b2\", \"delta_c2\"),                   nLambdas = 100,                   # Our model modification must make use of the modifyModel - function:                   modifyModel = modifyModel(transformations = transformations) ) coef(lassoFit, criterion = \"BIC\")"},{"path":"/reference/lessSEM.html","id":"multi-group-models-and-definition-variables","dir":"Reference","previous_headings":"","what":"Multi-Group Models and Definition Variables","title":"lessSEM — lessSEM","text":"lessSEM supports multi-group SEM , degree, definition variables. Regularized multi-group SEM proposed Huang (2018) implemented lslx (Huang, 2020). approach Huang (2018) currently flexible lessSEM can lslx allows separate penalties parameter differences parameter value groups. lessSEM, possible box. However, differences parameters can regularized. detailed introduction can found vignette(topic = \"Definition-Variables--Multi-Group-SEM\", package = \"lessSEM\"). Therein also explained multi-group SEM can used implement definition variables (e.g., latent growth curve models).","code":""},{"path":[]},{"path":"/reference/lessSEM.html","id":"r-packages-software","dir":"Reference","previous_headings":"","what":"R - Packages / Software","title":"lessSEM — lessSEM","text":"lavaan Rosseel, Y. (2012). lavaan: R Package Structural Equation Modeling. Journal Statistical Software, 48(2), 1–36. https://doi.org/10.18637/jss.v048.i02 regsem: Jacobucci, R. (2017). regsem: Regularized Structural Equation Modeling. ArXiv:1703.08489 Stat. http://arxiv.org/abs/1703.08489 lslx: Huang, P.-H. (2020). lslx: Semi-confirmatory structural equation modeling via penalized likelihood. Journal Statistical Software, 93(7). https://doi.org/10.18637/jss.v093.i07 fasta: Another implementation fista algorithm (Beck & Teboulle, 2009). ensmallen: Curtin, R. R., Edel, M., Prabhu, R. G., Basak, S., Lou, Z., & Sanderson, C. (2021). ensmallen library ﬂexible numerical optimization. Journal Machine Learning Research, 22, 1–6. regCtsem: Orzek, J. H., & Voelkle, M. C. (press). Regularized continuous time structural equation models: network perspective. Psychological Methods.","code":""},{"path":"/reference/lessSEM.html","id":"regularized-structural-equation-modeling","dir":"Reference","previous_headings":"","what":"Regularized Structural Equation Modeling","title":"lessSEM — lessSEM","text":"Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Huang, P.-H. (2018). penalized likelihood method multi-group structural equation modelling. British Journal Mathematical Statistical Psychology, 71(3), 499–522. https://doi.org/10.1111/bmsp.12130 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793","code":""},{"path":"/reference/lessSEM.html","id":"penalty-functions","dir":"Reference","previous_headings":"","what":"Penalty Functions","title":"lessSEM — lessSEM","text":"Candès, E. J., Wakin, M. B., & Boyd, S. P. (2008). Enhancing Sparsity Reweighted l1 Minimization. Journal Fourier Analysis Applications, 14(5–6), 877–905. https://doi.org/10.1007/s00041-008-9045-x Fan, J., & Li, R. (2001). Variable selection via nonconcave penalized likelihood oracle properties. Journal American Statistical Association, 96(456), 1348–1360. https://doi.org/10.1198/016214501753382273 Hoerl, . E., & Kennard, R. W. (1970). Ridge Regression: Biased Estimation Nonorthogonal Problems. Technometrics, 12(1), 55–67. https://doi.org/10.1080/00401706.1970.10488634 Tibshirani, R. (1996). Regression shrinkage selection via lasso. Journal Royal Statistical Society. Series B (Methodological), 58(1), 267–288. Zhang, C.-H. (2010). Nearly unbiased variable selection minimax concave penalty. Annals Statistics, 38(2), 894–942. https://doi.org/10.1214/09-AOS729 Zhang, T. (2010). Analysis Multi-stage Convex Relaxation Sparse Regularization. Journal Machine Learning Research, 11, 1081–1107. Zou, H. (2006). adaptive lasso oracle properties. Journal American Statistical Association, 101(476), 1418-1429. https://doi.org/10.1198/016214506000000735 Zou, H., & Hastie, T. (2005). Regularization variable selection via elastic net. Journal Royal Statistical Society: Series B, 67(2), 301–320. https://doi.org/10.1111/j.1467-9868.2005.00503.x","code":""},{"path":[]},{"path":"/reference/lessSEM.html","id":"glmnet","dir":"Reference","previous_headings":"","what":"GLMNET","title":"lessSEM — lessSEM","text":"Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization paths generalized linear models via coordinate descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421","code":""},{"path":"/reference/lessSEM.html","id":"variants-of-ista","dir":"Reference","previous_headings":"","what":"Variants of ISTA","title":"lessSEM — lessSEM","text":"Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). general iterative shrinkage thresholding algorithm non-convex regularized optimization problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/lessSEM.html","id":"miscellaneous","dir":"Reference","previous_headings":"","what":"Miscellaneous","title":"lessSEM — lessSEM","text":"Liang, X., Yang, Y., & Huang, J. (2018). Evaluation structural relationships autoregressive cross-lagged models longitudinal approximate invariance: Bayesian analysis. Structural Equation Modeling: Multidisciplinary Journal, 25(4), 558–572. https://doi.org/10.1080/10705511.2017.1410706 Bauer, D. J., Belzak, W. C. M., & Cole, V. T. (2020). Simplifying Assessment Measurement Invariance Multiple Background Variables: Using Regularized Moderated Nonlinear Factor Analysis Detect Differential Item Functioning. Structural Equation Modeling: Multidisciplinary Journal, 27(1), 43–55. https://doi.org/10.1080/10705511.2019.1642754","code":""},{"path":"/reference/lessSEM.html","id":"important-notes","dir":"Reference","previous_headings":"","what":"Important Notes","title":"lessSEM — lessSEM","text":"SOFTWARE PROVIDED '', WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"/reference/lessSEM.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"lessSEM — lessSEM","text":"Jannik Orzek orzek@mpib-berlin.mpg.de","code":""},{"path":"/reference/lessSEM2Lavaan.html","id":null,"dir":"Reference","previous_headings":"","what":"lessSEM2Lavaan — lessSEM2Lavaan","title":"lessSEM2Lavaan — lessSEM2Lavaan","text":"Creates lavaan model object lessSEM (possible).","code":""},{"path":"/reference/lessSEM2Lavaan.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"lessSEM2Lavaan — lessSEM2Lavaan","text":"","code":"lessSEM2Lavaan(regularizedSEM, lambda, theta = NULL)"},{"path":"/reference/lessSEM2Lavaan.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"lessSEM2Lavaan — lessSEM2Lavaan","text":"regularizedSEM object created lessSEM lambda value tuning parameter lambda theta value tuning parameter theta","code":""},{"path":"/reference/lessSEM2Lavaan.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"lessSEM2Lavaan — lessSEM2Lavaan","text":"lavaan model","code":""},{"path":"/reference/lessSEMCoef-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Class for the coefficients estimated by lessSEM. — lessSEMCoef-class","title":"Class for the coefficients estimated by lessSEM. — lessSEMCoef-class","text":"Class coefficients estimated lessSEM.","code":""},{"path":"/reference/lessSEMCoef-class.html","id":"slots","dir":"Reference","previous_headings":"","what":"Slots","title":"Class for the coefficients estimated by lessSEM. — lessSEMCoef-class","text":"tuningParameters tuning parameters estimates parameter estimates","code":""},{"path":"/reference/logLik-Rcpp_SEMCpp-method.html","id":null,"dir":"Reference","previous_headings":"","what":"logLik — logLik,Rcpp_SEMCpp-method","title":"logLik — logLik,Rcpp_SEMCpp-method","text":"logLik","code":""},{"path":"/reference/logLik-Rcpp_SEMCpp-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"logLik — logLik,Rcpp_SEMCpp-method","text":"","code":"# S4 method for Rcpp_SEMCpp logLik(object)"},{"path":"/reference/logLik-Rcpp_SEMCpp-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"logLik — logLik,Rcpp_SEMCpp-method","text":"object object class Rcpp_SEMCpp","code":""},{"path":"/reference/logLik-Rcpp_SEMCpp-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"logLik — logLik,Rcpp_SEMCpp-method","text":"log-likelihood model","code":""},{"path":"/reference/logLik-Rcpp_mgSEM-method.html","id":null,"dir":"Reference","previous_headings":"","what":"logLik — logLik,Rcpp_mgSEM-method","title":"logLik — logLik,Rcpp_mgSEM-method","text":"logLik","code":""},{"path":"/reference/logLik-Rcpp_mgSEM-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"logLik — logLik,Rcpp_mgSEM-method","text":"","code":"# S4 method for Rcpp_mgSEM logLik(object)"},{"path":"/reference/logLik-Rcpp_mgSEM-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"logLik — logLik,Rcpp_mgSEM-method","text":"object object class Rcpp_mgSEM","code":""},{"path":"/reference/logLik-Rcpp_mgSEM-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"logLik — logLik,Rcpp_mgSEM-method","text":"log-likelihood model","code":""},{"path":"/reference/logLikelihood-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Class for log-likelihood of regularized SEM. Note: we define a custom logLik -\nFunction because the generic one is using df = number of parameters which might be confusing. — logLikelihood-class","title":"Class for log-likelihood of regularized SEM. Note: we define a custom logLik -\nFunction because the generic one is using df = number of parameters which might be confusing. — logLikelihood-class","text":"Class log-likelihood regularized SEM. Note: define custom logLik - Function generic one using df = number parameters might confusing.","code":""},{"path":"/reference/logLikelihood-class.html","id":"slots","dir":"Reference","previous_headings":"","what":"Slots","title":"Class for log-likelihood of regularized SEM. Note: we define a custom logLik -\nFunction because the generic one is using df = number of parameters which might be confusing. — logLikelihood-class","text":"logLik log-Likelihood nParameters number parameters model N number persons data set","code":""},{"path":"/reference/lsp.html","id":null,"dir":"Reference","previous_headings":"","what":"lsp — lsp","title":"lsp — lsp","text":"Implements lsp regularization structural equation models. penalty function given : $$p( x_j) = \\lambda \\log(1 + |x_j|/\\theta)$$ \\(\\theta > 0\\).","code":""},{"path":"/reference/lsp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"lsp — lsp","text":"","code":"lsp(   lavaanModel,   regularized,   lambdas,   thetas,   modifyModel = lessSEM::modifyModel(),   control = lessSEM::controlIsta() )"},{"path":"/reference/lsp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"lsp — lsp","text":"lavaanModel model class lavaan regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda thetas parameters whose absolute value threshold penalized constant (theta) modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. element generated controlIsta (see ?controlIsta)","code":""},{"path":"/reference/lsp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"lsp — lsp","text":"Model class regularizedSEM","code":""},{"path":"/reference/lsp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"lsp — lsp","text":"Identical regsem, models specified using lavaan. Currently, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. lsp regularization: Candès, E. J., Wakin, M. B., & Boyd, S. P. (2008). Enhancing Sparsity Reweighted l1 Minimization. Journal Fourier Analysis Applications, 14(5–6), 877–905. https://doi.org/10.1007/s00041-008-9045-x Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classiﬁcation. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/lsp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"lsp — lsp","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +      l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +      l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Optional: Plot the model # semPlot::semPaths(lavaanModel, #                   what = \"est\", #                   fade = FALSE)  lsem <- lsp(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   # names of the regularized parameters:   regularized = paste0(\"l\", 6:15),   lambdas = seq(0,1,length.out = 20),   thetas = seq(0.01,2,length.out = 5))  # the coefficients can be accessed with: coef(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters[1,]  # optional: plotting the paths requires installation of plotly # plot(lsem)"},{"path":"/reference/makePtrs.html","id":null,"dir":"Reference","previous_headings":"","what":"makePtrs — makePtrs","title":"makePtrs — makePtrs","text":"function helps create pointers necessary use Cpp interface","code":""},{"path":"/reference/makePtrs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"makePtrs — makePtrs","text":"","code":"makePtrs(fitFunName, gradFunName)"},{"path":"/reference/makePtrs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"makePtrs — makePtrs","text":"fitFunName name C++ fit function (IMPORTANT: must name used C++) gradFunName name C++ gradient function (IMPORTANT: must name used C++)","code":""},{"path":"/reference/makePtrs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"makePtrs — makePtrs","text":"string can copied C++ function create pointers.","code":""},{"path":"/reference/mcp.html","id":null,"dir":"Reference","previous_headings":"","what":"mcp — mcp","title":"mcp — mcp","text":"Implements mcp regularization structural equation models. penalty function given : $$p( x_j) = \\begin{cases} \\lambda |x_j| - x_j^2/(2\\theta) & \\text{} |x_j| \\leq \\theta\\lambda\\\\ \\theta\\lambda^2/2 & \\text{} |x_j| > \\lambda\\theta \\end{cases}$$ \\(\\theta > 0\\).","code":""},{"path":"/reference/mcp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"mcp — mcp","text":"","code":"mcp(   lavaanModel,   regularized,   lambdas,   thetas,   modifyModel = lessSEM::modifyModel(),   control = lessSEM::controlIsta() )"},{"path":"/reference/mcp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"mcp — mcp","text":"lavaanModel model class lavaan regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda thetas parameters whose absolute value threshold penalized constant (theta) modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. element generated controlIsta (see ?controlIsta)","code":""},{"path":"/reference/mcp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"mcp — mcp","text":"Model class regularizedSEM","code":""},{"path":"/reference/mcp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"mcp — mcp","text":"Identical regsem, models specified using lavaan. Currently, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. mcp regularization: Zhang, C.-H. (2010). Nearly unbiased variable selection minimax concave penalty. Annals Statistics, 38(2), 894–942. https://doi.org/10.1214/09-AOS729 Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classiﬁcation. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/mcp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"mcp — mcp","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +      l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +      l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Optional: Plot the model # semPlot::semPaths(lavaanModel, #                   what = \"est\", #                   fade = FALSE)  lsem <- mcp(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   # names of the regularized parameters:   regularized = paste0(\"l\", 6:15),   lambdas = seq(0,1,length.out = 20),   thetas = seq(0.01,2,length.out = 5))  # the coefficients can be accessed with: coef(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters[1,]  # optional: plotting the paths requires installation of plotly # plot(lsem)"},{"path":"/reference/mcpPenalty_C.html","id":null,"dir":"Reference","previous_headings":"","what":"mcpPenalty_C — mcpPenalty_C","title":"mcpPenalty_C — mcpPenalty_C","text":"mcpPenalty_C","code":""},{"path":"/reference/mcpPenalty_C.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"mcpPenalty_C — mcpPenalty_C","text":"","code":"mcpPenalty_C(par, lambda_p, theta)"},{"path":"/reference/mcpPenalty_C.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"mcpPenalty_C — mcpPenalty_C","text":"par single parameter value lambda_p lambda value parameter theta theta value parameter","code":""},{"path":"/reference/mcpPenalty_C.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"mcpPenalty_C — mcpPenalty_C","text":"penalty value","code":""},{"path":"/reference/mgSEM.html","id":null,"dir":"Reference","previous_headings":"","what":"mgSEM class — mgSEM","title":"mgSEM class — mgSEM","text":"internal mgSEM representation","code":""},{"path":"/reference/mgSEM.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"mgSEM class — mgSEM","text":"new Creates new mgSEM. addModel add model. Expects Rcpp::List addTransformation adds transforamtions model implied Computes implied means covariance matrix fit Fits model. Returns -2 log likelihood getParameters Returns data frame model parameters. getParameterLabels Returns vector unique parameter labels used internally. getGradients Returns matrix scores. getScores Returns matrix scores. yet implemented getHessian Returns hessian model. Expects labels parameters values parameters well boolean indicating raw. Finally, double (eps) controls precision approximation. computeTransformations compute transformations. setTransformationGradientStepSize change step size gradient computation transformations","code":""},{"path":"/reference/mixedPenalty.html","id":null,"dir":"Reference","previous_headings":"","what":"mixedPenalty — mixedPenalty","title":"mixedPenalty — mixedPenalty","text":"Provides possibility impose different penalties different parameters.","code":""},{"path":"/reference/mixedPenalty.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"mixedPenalty — mixedPenalty","text":"","code":"mixedPenalty(   lavaanModel,   method = \"ista\",   modifyModel = lessSEM::modifyModel(),   control = lessSEM::controlIsta() )"},{"path":"/reference/mixedPenalty.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"mixedPenalty — mixedPenalty","text":"lavaanModel model class lavaan method optimizer used? ista supported. modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. element generated controlIsta controlGlmnet functions. See ?controlIsta ?controlGlmnet details.","code":""},{"path":"/reference/mixedPenalty.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"mixedPenalty — mixedPenalty","text":"Model class regularizedSEM","code":""},{"path":"/reference/mixedPenalty.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"mixedPenalty — mixedPenalty","text":"Builds basis adding different penalty functions different parameters model. Identical regsem, models specified using lavaan. Currently, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. Models fitted glmnet ista optimizer. Note optimizers differ penalties support. following table provides overview: default, ista used. Check vignette(topic = \"Mixed-Penalties\", package = \"lessSEM\") details. Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/mixedPenalty.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"mixedPenalty — mixedPenalty","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +       l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +       l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Optional: Plot the model # semPlot::semPaths(lavaanModel,  #                   what = \"est\", #                   fade = FALSE)  # We can add multiple penalties as follows:  regularized <- lavaanModel |>   # create template for regularized model with mixed penalty:   mixedPenalty() |>   # add lasso penalty on loadings l6 - l10:   addLasso(regularized = paste0(\"l\", 6:10),             lambdas = seq(0,1,.1)) |>   # add scad penalty on loadings l11 - l15:   addScad(regularized = paste0(\"l\", 11:15),            lambdas = seq(0,1,.1),           thetas = 3.1) |>   # fit the model:   fit()  # elements of regularized can be accessed with the @ operator: regularized@parameters[1,]  # AIC and BIC: AIC(regularized) BIC(regularized)  # The best parameters can also be extracted with: coef(regularized, criterion = \"AIC\") coef(regularized, criterion = \"BIC\")  # The tuningParameterConfiguration corresponds to the rows # in the lambda, theta, and alpha matrices in regularized@tuningParamterConfigurations. # Configuration 3, for example, is given by regularized@tuningParameterConfigurations$lambda[3,] regularized@tuningParameterConfigurations$theta[3,] regularized@tuningParameterConfigurations$alpha[3,]  # Note that lambda, theta, and alpha may correspond to tuning parameters # of different penalties for different parameters (e.g., lambda for l6 is the lambda # of the lasso penalty, while lambda for l12 is the lambda of the scad penalty)."},{"path":"/reference/modifyModel.html","id":null,"dir":"Reference","previous_headings":"","what":"modifyModel — modifyModel","title":"modifyModel — modifyModel","text":"Modify model lavaan fit needs","code":""},{"path":"/reference/modifyModel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"modifyModel — modifyModel","text":"","code":"modifyModel(   addMeans = FALSE,   activeSet = NULL,   dataSet = NULL,   transformations = NULL,   transformationList = list(),   transformationGradientStepSize = 1e-06 )"},{"path":"/reference/modifyModel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"modifyModel — modifyModel","text":"addMeans lavaanModel meanstructure = FALSE, addMeans = TRUE add mean structure. FALSE set means observed variables observed means. activeSet Option use subset individuals data set. Logical vector length N indicating subjects remain sample. dataSet option replace data set lavaan model different data set. Can useful cross-validation transformations allows transformations parameters - useful measurement invariance tests etc. transformationList optional list used within transformations. NOTE: must used Rcpp::List. transformationGradientStepSize step size used compute gradients transformations","code":""},{"path":"/reference/modifyModel.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"modifyModel — modifyModel","text":"Object class modifyModel","code":""},{"path":"/reference/newTau.html","id":null,"dir":"Reference","previous_headings":"","what":"newTau — newTau","title":"newTau — newTau","text":"assign new value parameter tau used approximate optimization. regularized value tau evaluated zeroed directly impacts AIC, BIC, etc.","code":""},{"path":"/reference/newTau.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"newTau — newTau","text":"","code":"newTau(regularizedSEM, tau)"},{"path":"/reference/newTau.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"newTau — newTau","text":"regularizedSEM object fitted approximate optimization tau new tau value","code":""},{"path":"/reference/newTau.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"newTau — newTau","text":"regularizedSEM, new regularizedSEM@fits$nonZeroParameters","code":""},{"path":"/reference/plot-cvRegularizedSEM-missing-method.html","id":null,"dir":"Reference","previous_headings":"","what":"plots the cross-validation fits — plot,cvRegularizedSEM,missing-method","title":"plots the cross-validation fits — plot,cvRegularizedSEM,missing-method","text":"plots cross-validation fits","code":""},{"path":"/reference/plot-cvRegularizedSEM-missing-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"plots the cross-validation fits — plot,cvRegularizedSEM,missing-method","text":"","code":"# S4 method for cvRegularizedSEM,missing plot(x, y, ...)"},{"path":"/reference/plot-cvRegularizedSEM-missing-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"plots the cross-validation fits — plot,cvRegularizedSEM,missing-method","text":"x object class cvRegularizedSEM y used ... used","code":""},{"path":"/reference/plot-gpRegularized-missing-method.html","id":null,"dir":"Reference","previous_headings":"","what":"plots the regularized and unregularized parameters for all levels of lambda — plot,gpRegularized,missing-method","title":"plots the regularized and unregularized parameters for all levels of lambda — plot,gpRegularized,missing-method","text":"plots regularized unregularized parameters levels lambda","code":""},{"path":"/reference/plot-gpRegularized-missing-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"plots the regularized and unregularized parameters for all levels of lambda — plot,gpRegularized,missing-method","text":"","code":"# S4 method for gpRegularized,missing plot(x, y, ...)"},{"path":"/reference/plot-gpRegularized-missing-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"plots the regularized and unregularized parameters for all levels of lambda — plot,gpRegularized,missing-method","text":"x object class gpRegularized y used ... use regularizedOnly=FALSE plot parameters","code":""},{"path":"/reference/plot-regularizedSEM-missing-method.html","id":null,"dir":"Reference","previous_headings":"","what":"plots the regularized and unregularized parameters for all levels of lambda — plot,regularizedSEM,missing-method","title":"plots the regularized and unregularized parameters for all levels of lambda — plot,regularizedSEM,missing-method","text":"plots regularized unregularized parameters levels lambda","code":""},{"path":"/reference/plot-regularizedSEM-missing-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"plots the regularized and unregularized parameters for all levels of lambda — plot,regularizedSEM,missing-method","text":"","code":"# S4 method for regularizedSEM,missing plot(x, y, ...)"},{"path":"/reference/plot-regularizedSEM-missing-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"plots the regularized and unregularized parameters for all levels of lambda — plot,regularizedSEM,missing-method","text":"x object class gpRegularized y used ... use regularizedOnly=FALSE plot parameters","code":""},{"path":"/reference/regsem2LavaanParameters.html","id":null,"dir":"Reference","previous_headings":"","what":"regsem2LavaanParameters — regsem2LavaanParameters","title":"regsem2LavaanParameters — regsem2LavaanParameters","text":"helper function: regsem lavaan use slightly different parameter labels. function can used translate parameter labels cv_regsem object lavaan labels","code":""},{"path":"/reference/regsem2LavaanParameters.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"regsem2LavaanParameters — regsem2LavaanParameters","text":"","code":"regsem2LavaanParameters(regsemModel, lavaanModel)"},{"path":"/reference/regsem2LavaanParameters.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"regsem2LavaanParameters — regsem2LavaanParameters","text":"regsemModel model class regsem lavaanModel model class lavaan","code":""},{"path":"/reference/regsem2LavaanParameters.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"regsem2LavaanParameters — regsem2LavaanParameters","text":"regsem parameters lavaan labels","code":""},{"path":"/reference/regularizedSEM-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Class for regularized SEM — regularizedSEM-class","title":"Class for regularized SEM — regularizedSEM-class","text":"Class regularized SEM","code":""},{"path":"/reference/regularizedSEM-class.html","id":"slots","dir":"Reference","previous_headings":"","what":"Slots","title":"Class for regularized SEM — regularizedSEM-class","text":"penalty penalty used (e.g., \"lasso\") parameters data.frame parameter estimates fits data.frame fit results parameterLabels character vector names parameters weights vector weights given parameters penalty regularized character vector names regularized parameters transformations model transformations, transformed parameters returned internalOptimization list elements used internally inputArguments list elements passed user general","code":""},{"path":"/reference/regularizedSEMMixedPenalty-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Class for regularized SEM — regularizedSEMMixedPenalty-class","title":"Class for regularized SEM — regularizedSEMMixedPenalty-class","text":"Class regularized SEM","code":""},{"path":"/reference/regularizedSEMMixedPenalty-class.html","id":"slots","dir":"Reference","previous_headings":"","what":"Slots","title":"Class for regularized SEM — regularizedSEMMixedPenalty-class","text":"penalty penalty used (e.g., \"lasso\") tuningParameterConfigurations list settings lambda, theta, alpha tuning parameters. parameters data.frame parameter estimates fits data.frame fit results parameterLabels character vector names parameters weights vector weights given parameters penalty regularized character vector names regularized parameters transformations model transformations, transformed parameters returned internalOptimization list elements used internally inputArguments list elements passed user general","code":""},{"path":"/reference/regularizedSEMWithCustomPenalty-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Class for regularized SEM using Rsolnp — regularizedSEMWithCustomPenalty-class","title":"Class for regularized SEM using Rsolnp — regularizedSEMWithCustomPenalty-class","text":"Class regularized SEM using Rsolnp","code":""},{"path":"/reference/regularizedSEMWithCustomPenalty-class.html","id":"slots","dir":"Reference","previous_headings":"","what":"Slots","title":"Class for regularized SEM using Rsolnp — regularizedSEMWithCustomPenalty-class","text":"parameters data.frame parameter estimates fits data.frame fit results parameterLabels character vector names parameters internalOptimization list elements used internally inputArguments list elements passed user general","code":""},{"path":"/reference/ridge.html","id":null,"dir":"Reference","previous_headings":"","what":"ridge — ridge","title":"ridge — ridge","text":"Implements ridge regularization structural equation models. penalty function given : $$p( x_j) = \\lambda x_j^2$$ Note ridge regularization set parameters zero result shrinkage towards zero.","code":""},{"path":"/reference/ridge.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"ridge — ridge","text":"","code":"ridge(   lavaanModel,   regularized,   lambdas,   method = \"glmnet\",   modifyModel = lessSEM::modifyModel(),   control = lessSEM::controlGlmnet() )"},{"path":"/reference/ridge.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"ridge — ridge","text":"lavaanModel model class lavaan regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda method optimizer used? Currently implemented ista glmnet. ista, control argument can used switch related procedures (currently gist). modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. element generated controlIsta controlGlmnet functions. See ?controlIsta ?controlGlmnet details.","code":""},{"path":"/reference/ridge.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"ridge — ridge","text":"Model class regularizedSEM","code":""},{"path":"/reference/ridge.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"ridge — ridge","text":"Identical regsem, models specified using lavaan. Currently, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. Ridge regularization: Hoerl, . E., & Kennard, R. W. (1970). Ridge Regression: Biased Estimation Nonorthogonal Problems. Technometrics, 12(1), 55–67. https://doi.org/10.1080/00401706.1970.10488634 Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classiﬁcation. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/ridge.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"ridge — ridge","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +      l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +      l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Optional: Plot the model # semPlot::semPaths(lavaanModel, #                   what = \"est\", #                   fade = FALSE)  lsem <- ridge(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   # names of the regularized parameters:   regularized = paste0(\"l\", 6:15),   lambdas = seq(0,1,length.out = 20))  # use the plot-function to plot the regularized parameters: plot(lsem)  # the coefficients can be accessed with: coef(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters[1,]   #### Advanced ### # Switching the optimizer # # Use the \"method\" argument to switch the optimizer. The control argument # must also be changed to the corresponding function: lsemIsta <- ridge(   lavaanModel = lavaanModel,   regularized = paste0(\"l\", 6:15),   lambdas = seq(0,1,length.out = 20),   method = \"ista\",   control = controlIsta())  # Note: The results are basically identical: lsemIsta@parameters - lsem@parameters"},{"path":"/reference/ridgeBfgs.html","id":null,"dir":"Reference","previous_headings":"","what":"ridgeBfgs — ridgeBfgs","title":"ridgeBfgs — ridgeBfgs","text":"function allows regularization models built lavaan ridge penalty. elements can accessed \"@\" operator (see examples).","code":""},{"path":"/reference/ridgeBfgs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"ridgeBfgs — ridgeBfgs","text":"","code":"ridgeBfgs(   lavaanModel,   regularized,   lambdas = NULL,   modifyModel = lessSEM::modifyModel(),   control = lessSEM::controlBFGS() )"},{"path":"/reference/ridgeBfgs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"ridgeBfgs — ridgeBfgs","text":"lavaanModel model class lavaan regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. element generated controlBFGS function. See ?controlBFGS details.","code":""},{"path":"/reference/ridgeBfgs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"ridgeBfgs — ridgeBfgs","text":"Model class regularizedSEM","code":""},{"path":"/reference/ridgeBfgs.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"ridgeBfgs — ridgeBfgs","text":"details, see: Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9","code":""},{"path":"/reference/ridgeBfgs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"ridgeBfgs — ridgeBfgs","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +      l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +      l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Optional: Plot the model # semPlot::semPaths(lavaanModel, #                   what = \"est\", #                   fade = FALSE)  # names of the regularized parameters: regularized = paste0(\"l\", 6:15)  lsem <- ridgeBfgs(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   regularized = regularized,   lambdas = seq(0,1,length.out = 50))  plot(lsem)  # the coefficients can be accessed with: coef(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters[1,]"},{"path":"/reference/ridgeGradient.html","id":null,"dir":"Reference","previous_headings":"","what":"ridgeGradient — ridgeGradient","title":"ridgeGradient — ridgeGradient","text":"ridge gradient function","code":""},{"path":"/reference/ridgeGradient.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"ridgeGradient — ridgeGradient","text":"","code":"ridgeGradient(parameters, tuningParameters, penaltyFunctionArguments)"},{"path":"/reference/ridgeGradient.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"ridgeGradient — ridgeGradient","text":"parameters vector labeled parameter values tuningParameters list field lambda (tuning parameter value) penaltyFunctionArguments list field regularizedParameterLabels (labels regularized parameters)","code":""},{"path":"/reference/ridgeGradient.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"ridgeGradient — ridgeGradient","text":"gradient values","code":""},{"path":"/reference/ridgeHessian.html","id":null,"dir":"Reference","previous_headings":"","what":"ridgeHessian — ridgeHessian","title":"ridgeHessian — ridgeHessian","text":"ridge Hessian function","code":""},{"path":"/reference/ridgeHessian.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"ridgeHessian — ridgeHessian","text":"","code":"ridgeHessian(parameters, tuningParameters, penaltyFunctionArguments)"},{"path":"/reference/ridgeHessian.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"ridgeHessian — ridgeHessian","text":"parameters vector labeled parameter values tuningParameters list field lambda (tuning parameter value) penaltyFunctionArguments list field regularizedParameterLabels (labels regularized parameters)","code":""},{"path":"/reference/ridgeHessian.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"ridgeHessian — ridgeHessian","text":"Hessian matrix","code":""},{"path":"/reference/ridgeValue.html","id":null,"dir":"Reference","previous_headings":"","what":"ridgeValue — ridgeValue","title":"ridgeValue — ridgeValue","text":"ridge penalty function","code":""},{"path":"/reference/ridgeValue.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"ridgeValue — ridgeValue","text":"","code":"ridgeValue(parameters, tuningParameters, penaltyFunctionArguments)"},{"path":"/reference/ridgeValue.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"ridgeValue — ridgeValue","text":"parameters vector labeled parameter values tuningParameters list field lambda (tuning parameter value) penaltyFunctionArguments list field regularizedParameterLabels (labels regularized parameters)","code":""},{"path":"/reference/ridgeValue.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"ridgeValue — ridgeValue","text":"penalty function value","code":""},{"path":"/reference/scad.html","id":null,"dir":"Reference","previous_headings":"","what":"scad — scad","title":"scad — scad","text":"Implements scad regularization structural equation models. penalty function given :  $$p( x_j) = \\begin{cases} \\lambda |x_j| & \\text{} |x_j| \\leq \\theta\\\\ \\frac{-x_j^2 + 2\\theta\\lambda |x_j| - \\lambda^2}{2(\\theta -1)} &  \\text{} \\lambda < |x_j| \\leq \\lambda\\theta \\\\ (\\theta + 1) \\lambda^2/2 & \\text{} |x_j| \\geq \\theta\\lambda\\\\ \\end{cases}$$ \\(\\theta > 2\\).","code":""},{"path":"/reference/scad.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"scad — scad","text":"","code":"scad(   lavaanModel,   regularized,   lambdas,   thetas,   modifyModel = lessSEM::modifyModel(),   control = lessSEM::controlIsta() )"},{"path":"/reference/scad.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"scad — scad","text":"lavaanModel model class lavaan regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda thetas parameters whose absolute value threshold penalized constant (theta) modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. element generated controlIsta (see ?controlIsta)","code":""},{"path":"/reference/scad.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"scad — scad","text":"Model class regularizedSEM","code":""},{"path":"/reference/scad.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"scad — scad","text":"Identical regsem, models specified using lavaan. Currently, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. scad regularization: Fan, J., & Li, R. (2001). Variable selection via nonconcave penalized likelihood oracle properties. Journal American Statistical Association, 96(456), 1348–1360. https://doi.org/10.1198/016214501753382273 Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classiﬁcation. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/scad.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"scad — scad","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +      l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +      l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Optional: Plot the model # semPlot::semPaths(lavaanModel, #                   what = \"est\", #                   fade = FALSE)  lsem <- scad(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   # names of the regularized parameters:   regularized = paste0(\"l\", 6:15),   lambdas = seq(0,1,length.out = 20),   thetas = seq(2.01,5,length.out = 5))  # the coefficients can be accessed with: coef(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters[1,]  # optional: plotting the paths requires installation of plotly # plot(lsem)"},{"path":"/reference/scadPenalty_C.html","id":null,"dir":"Reference","previous_headings":"","what":"scadPenalty_C — scadPenalty_C","title":"scadPenalty_C — scadPenalty_C","text":"scadPenalty_C","code":""},{"path":"/reference/scadPenalty_C.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"scadPenalty_C — scadPenalty_C","text":"","code":"scadPenalty_C(par, lambda_p, theta)"},{"path":"/reference/scadPenalty_C.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"scadPenalty_C — scadPenalty_C","text":"par single parameter value lambda_p lambda value parameter theta theta value parameter","code":""},{"path":"/reference/scadPenalty_C.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"scadPenalty_C — scadPenalty_C","text":"penalty value","code":""},{"path":"/reference/setupMulticore.html","id":null,"dir":"Reference","previous_headings":"","what":"setupMulticore — setupMulticore","title":"setupMulticore — setupMulticore","text":"setup multi-core support","code":""},{"path":"/reference/setupMulticore.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"setupMulticore — setupMulticore","text":"","code":"setupMulticore(control)"},{"path":"/reference/setupMulticore.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"setupMulticore — setupMulticore","text":"control object created controlBFGS, controlIsta controlGlmnet function","code":""},{"path":"/reference/setupMulticore.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"setupMulticore — setupMulticore","text":"nothing","code":""},{"path":"/reference/show-Rcpp_SEMCpp-method.html","id":null,"dir":"Reference","previous_headings":"","what":"show — show,Rcpp_SEMCpp-method","title":"show — show,Rcpp_SEMCpp-method","text":"show","code":""},{"path":"/reference/show-Rcpp_SEMCpp-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"show — show,Rcpp_SEMCpp-method","text":"","code":"# S4 method for Rcpp_SEMCpp show(object)"},{"path":"/reference/show-Rcpp_SEMCpp-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"show — show,Rcpp_SEMCpp-method","text":"object object class Rcpp_SEMCpp","code":""},{"path":"/reference/show-Rcpp_mgSEM-method.html","id":null,"dir":"Reference","previous_headings":"","what":"show — show,Rcpp_mgSEM-method","title":"show — show,Rcpp_mgSEM-method","text":"show","code":""},{"path":"/reference/show-Rcpp_mgSEM-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"show — show,Rcpp_mgSEM-method","text":"","code":"# S4 method for Rcpp_mgSEM show(object)"},{"path":"/reference/show-Rcpp_mgSEM-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"show — show,Rcpp_mgSEM-method","text":"object object class Rcpp_mgSEM","code":""},{"path":"/reference/show-cvRegularizedSEM-method.html","id":null,"dir":"Reference","previous_headings":"","what":"Show method for objects of class cvRegularizedSEM. — show,cvRegularizedSEM-method","title":"Show method for objects of class cvRegularizedSEM. — show,cvRegularizedSEM-method","text":"Show method objects class cvRegularizedSEM.","code":""},{"path":"/reference/show-cvRegularizedSEM-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Show method for objects of class cvRegularizedSEM. — show,cvRegularizedSEM-method","text":"","code":"# S4 method for cvRegularizedSEM show(object)"},{"path":"/reference/show-cvRegularizedSEM-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Show method for objects of class cvRegularizedSEM. — show,cvRegularizedSEM-method","text":"object object class cvRegularizedSEM","code":""},{"path":"/reference/show-gpRegularized-method.html","id":null,"dir":"Reference","previous_headings":"","what":"show — show,gpRegularized-method","title":"show — show,gpRegularized-method","text":"show","code":""},{"path":"/reference/show-gpRegularized-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"show — show,gpRegularized-method","text":"","code":"# S4 method for gpRegularized show(object)"},{"path":"/reference/show-gpRegularized-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"show — show,gpRegularized-method","text":"object object class gpRegularized","code":""},{"path":"/reference/show-lessSEMCoef-method.html","id":null,"dir":"Reference","previous_headings":"","what":"show — show,lessSEMCoef-method","title":"show — show,lessSEMCoef-method","text":"show","code":""},{"path":"/reference/show-lessSEMCoef-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"show — show,lessSEMCoef-method","text":"","code":"# S4 method for lessSEMCoef show(object)"},{"path":"/reference/show-lessSEMCoef-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"show — show,lessSEMCoef-method","text":"object object class lessSEMCoef","code":""},{"path":"/reference/show-logLikelihood-method.html","id":null,"dir":"Reference","previous_headings":"","what":"show — show,logLikelihood-method","title":"show — show,logLikelihood-method","text":"show","code":""},{"path":"/reference/show-logLikelihood-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"show — show,logLikelihood-method","text":"","code":"# S4 method for logLikelihood show(object)"},{"path":"/reference/show-logLikelihood-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"show — show,logLikelihood-method","text":"object object class logLikelihood","code":""},{"path":"/reference/show-regularizedSEM-method.html","id":null,"dir":"Reference","previous_headings":"","what":"show — show,regularizedSEM-method","title":"show — show,regularizedSEM-method","text":"show","code":""},{"path":"/reference/show-regularizedSEM-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"show — show,regularizedSEM-method","text":"","code":"# S4 method for regularizedSEM show(object)"},{"path":"/reference/show-regularizedSEM-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"show — show,regularizedSEM-method","text":"object object class regularizedSEM","code":""},{"path":"/reference/show-regularizedSEMMixedPenalty-method.html","id":null,"dir":"Reference","previous_headings":"","what":"show — show,regularizedSEMMixedPenalty-method","title":"show — show,regularizedSEMMixedPenalty-method","text":"show","code":""},{"path":"/reference/show-regularizedSEMMixedPenalty-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"show — show,regularizedSEMMixedPenalty-method","text":"","code":"# S4 method for regularizedSEMMixedPenalty show(object)"},{"path":"/reference/show-regularizedSEMMixedPenalty-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"show — show,regularizedSEMMixedPenalty-method","text":"object object class regularizedSEM","code":""},{"path":"/reference/simulateExampleData.html","id":null,"dir":"Reference","previous_headings":"","what":"simulateExampleData — simulateExampleData","title":"simulateExampleData — simulateExampleData","text":"simulate data simple CFA model","code":""},{"path":"/reference/simulateExampleData.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"simulateExampleData — simulateExampleData","text":"","code":"simulateExampleData(   N = 100,   loadings = c(rep(1, 5), rep(0.4, 5), rep(0, 5)),   percentMissing = 0 )"},{"path":"/reference/simulateExampleData.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"simulateExampleData — simulateExampleData","text":"N number persons data set loadings loadings latent variable manifest observations percentMissing percentage missing data","code":""},{"path":"/reference/simulateExampleData.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"simulateExampleData — simulateExampleData","text":"data set single-factor CFA.","code":""},{"path":"/reference/simulateExampleData.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"simulateExampleData — simulateExampleData","text":"","code":"y <- lessSEM::simulateExampleData()"},{"path":"/reference/smoothAdaptiveLASSOGradient.html","id":null,"dir":"Reference","previous_headings":"","what":"smoothAdaptiveLASSOGradient — smoothAdaptiveLASSOGradient","title":"smoothAdaptiveLASSOGradient — smoothAdaptiveLASSOGradient","text":"smoothed version non-differentiable adaptive LASSO gradient","code":""},{"path":"/reference/smoothAdaptiveLASSOGradient.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"smoothAdaptiveLASSOGradient — smoothAdaptiveLASSOGradient","text":"","code":"smoothAdaptiveLASSOGradient(   parameters,   tuningParameters,   penaltyFunctionArguments )"},{"path":"/reference/smoothAdaptiveLASSOGradient.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"smoothAdaptiveLASSOGradient — smoothAdaptiveLASSOGradient","text":"parameters vector labeled parameter values tuningParameters list fields lambdas (vector one tuning parameter value parameter) penaltyFunctionArguments list fields regularizedParameterLabels (labels regularized parameters), eps (controls smooth approximation non-differential penalty functions (e.g., lasso, adaptive lasso, elastic net). Smaller values result closer approximation, may also cause larger issues optimization.)","code":""},{"path":"/reference/smoothAdaptiveLASSOGradient.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"smoothAdaptiveLASSOGradient — smoothAdaptiveLASSOGradient","text":"gradient values","code":""},{"path":"/reference/smoothAdaptiveLASSOHessian.html","id":null,"dir":"Reference","previous_headings":"","what":"smoothAdaptiveLASSOHessian — smoothAdaptiveLASSOHessian","title":"smoothAdaptiveLASSOHessian — smoothAdaptiveLASSOHessian","text":"smoothed version non-differentiable adaptive LASSO Hessian","code":""},{"path":"/reference/smoothAdaptiveLASSOHessian.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"smoothAdaptiveLASSOHessian — smoothAdaptiveLASSOHessian","text":"","code":"smoothAdaptiveLASSOHessian(   parameters,   tuningParameters,   penaltyFunctionArguments )"},{"path":"/reference/smoothAdaptiveLASSOHessian.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"smoothAdaptiveLASSOHessian — smoothAdaptiveLASSOHessian","text":"parameters vector labeled parameter values tuningParameters list fields lambdas (vector one tuning parameter value parameter) penaltyFunctionArguments list fields regularizedParameterLabels (labels regularized parameters), eps (controls smooth approximation non-differential penalty functions (e.g., lasso, adaptive lasso, elastic net). Smaller values result closer approximation, may also cause larger issues optimization.)","code":""},{"path":"/reference/smoothAdaptiveLASSOHessian.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"smoothAdaptiveLASSOHessian — smoothAdaptiveLASSOHessian","text":"Hessian matrix","code":""},{"path":"/reference/smoothAdaptiveLASSOValue.html","id":null,"dir":"Reference","previous_headings":"","what":"smoothAdaptiveLASSOValue — smoothAdaptiveLASSOValue","title":"smoothAdaptiveLASSOValue — smoothAdaptiveLASSOValue","text":"smoothed version non-differentiable adaptive LASSO penalty","code":""},{"path":"/reference/smoothAdaptiveLASSOValue.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"smoothAdaptiveLASSOValue — smoothAdaptiveLASSOValue","text":"","code":"smoothAdaptiveLASSOValue(   parameters,   tuningParameters,   penaltyFunctionArguments )"},{"path":"/reference/smoothAdaptiveLASSOValue.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"smoothAdaptiveLASSOValue — smoothAdaptiveLASSOValue","text":"parameters vector labeled parameter values tuningParameters list fields lambdas (vector one tuning parameter value parameter) penaltyFunctionArguments list fields regularizedParameterLabels (labels regularized parameters), eps (controls smooth approximation non-differential penalty functions (e.g., lasso, adaptive lasso, elastic net). Smaller values result closer approximation, may also cause larger issues optimization.)","code":""},{"path":"/reference/smoothAdaptiveLASSOValue.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"smoothAdaptiveLASSOValue — smoothAdaptiveLASSOValue","text":"penalty function value","code":""},{"path":"/reference/smoothAdaptiveLasso.html","id":null,"dir":"Reference","previous_headings":"","what":"smoothAdaptiveLasso — smoothAdaptiveLasso","title":"smoothAdaptiveLasso — smoothAdaptiveLasso","text":"function allows regularization models built lavaan smooth adaptive lasso penalty. returned object S4 class; elements can accessed \"@\" operator (see examples).","code":""},{"path":"/reference/smoothAdaptiveLasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"smoothAdaptiveLasso — smoothAdaptiveLasso","text":"","code":"smoothAdaptiveLasso(   lavaanModel,   regularized,   weights = NULL,   lambdas,   epsilon,   tau,   modifyModel = lessSEM::modifyModel(),   control = lessSEM::controlBFGS() )"},{"path":"/reference/smoothAdaptiveLasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"smoothAdaptiveLasso — smoothAdaptiveLasso","text":"lavaanModel model class lavaan regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object weights labeled vector weights parameters model. unsure parameters called, use getLavaanParameters(model) lavaan model object. set NULL, default weights used: inverse absolute values unregularized parameter estimates lambdas numeric vector: values tuning parameter lambda epsilon epsilon > 0; controls smoothness approximation. Larger values = smoother tau parameters threshold tau seen zeroed modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. element generated controlBFGS function. See ?controlBFGS details.","code":""},{"path":"/reference/smoothAdaptiveLasso.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"smoothAdaptiveLasso — smoothAdaptiveLasso","text":"Model class regularizedSEM","code":""},{"path":"/reference/smoothAdaptiveLasso.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"smoothAdaptiveLasso — smoothAdaptiveLasso","text":"details, see: Zou, H. (2006). Adaptive Lasso Oracle Properties. Journal American Statistical Association, 101(476), 1418–1429. https://doi.org/10.1198/016214506000000735 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 Lee, S.-., Lee, H., Abbeel, P., & Ng, . Y. (2006). Efficient L1 Regularized Logistic Regression. Proceedings Twenty-First National Conference Artificial Intelligence (AAAI-06), 401–408.","code":""},{"path":"/reference/smoothAdaptiveLasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"smoothAdaptiveLasso — smoothAdaptiveLasso","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +      l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +      l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Optional: Plot the model # semPlot::semPaths(lavaanModel, #                   what = \"est\", #                   fade = FALSE)  # names of the regularized parameters: regularized = paste0(\"l\", 6:15)  # define adaptive lasso weights: # We use the inverse of the absolute unregularized parameters # (this is the default in adaptiveLasso and can also specified # by setting weights = NULL) weights <- 1/abs(getLavaanParameters(lavaanModel)) weights[!names(weights) %in% regularized] <- 0  lsem <- smoothAdaptiveLasso(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   regularized = regularized,   weights = weights,   epsilon = 1e-10,   tau = 1e-4,   lambdas = seq(0,1,length.out = 50))  # use the plot-function to plot the regularized parameters: plot(lsem)  # the coefficients can be accessed with: coef(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters[1,]  # AIC and BIC: AIC(lsem) BIC(lsem)  # The best parameters can also be extracted with: coef(lsem, criterion = \"AIC\") coef(lsem, criterion = \"BIC\")"},{"path":"/reference/smoothCappedL1Value.html","id":null,"dir":"Reference","previous_headings":"","what":"smoothCappedL1Value — smoothCappedL1Value","title":"smoothCappedL1Value — smoothCappedL1Value","text":"smoothed version capped L1 penalty","code":""},{"path":"/reference/smoothCappedL1Value.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"smoothCappedL1Value — smoothCappedL1Value","text":"","code":"smoothCappedL1Value(parameters, tuningParameters, penaltyFunctionArguments)"},{"path":"/reference/smoothCappedL1Value.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"smoothCappedL1Value — smoothCappedL1Value","text":"parameters vector labeled parameter values tuningParameters list field lambda (tuning parameter value) penaltyFunctionArguments list field regularizedParameterLabels (labels regularized parameters), eps (controls smooth approximation non-differential penalty functions (e.g., lasso, adaptive lasso, elastic net). Smaller values result closer approximation, may also cause larger issues optimization.)","code":""},{"path":"/reference/smoothCappedL1Value.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"smoothCappedL1Value — smoothCappedL1Value","text":"penalty function value","code":""},{"path":"/reference/smoothElasticNet.html","id":null,"dir":"Reference","previous_headings":"","what":"smoothElasticNet — smoothElasticNet","title":"smoothElasticNet — smoothElasticNet","text":"function allows regularization models built lavaan smooth elastic net penalty. elements can accessed \"@\" operator (see examples).","code":""},{"path":"/reference/smoothElasticNet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"smoothElasticNet — smoothElasticNet","text":"","code":"smoothElasticNet(   lavaanModel,   regularized,   lambdas = NULL,   nLambdas = NULL,   alphas,   epsilon,   tau,   modifyModel = lessSEM::modifyModel(),   control = lessSEM::controlBFGS() )"},{"path":"/reference/smoothElasticNet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"smoothElasticNet — smoothElasticNet","text":"lavaanModel model class lavaan regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda nLambdas alternative lambda: alpha = 1, lessSEM can automatically compute first lambda value sets regularized parameters zero. generate nLambda values 0 computed lambda. alphas numeric vector values tuning parameter alpha. Must 0 1. 0 = ridge, 1 = lasso. epsilon epsilon > 0; controls smoothness approximation. Larger values = smoother tau parameters threshold tau seen zeroed modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. element generated controlBFGS function. See ?controlBFGS details.","code":""},{"path":"/reference/smoothElasticNet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"smoothElasticNet — smoothElasticNet","text":"Model class regularizedSEM","code":""},{"path":"/reference/smoothElasticNet.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"smoothElasticNet — smoothElasticNet","text":"details, see: Zou, H., & Hastie, T. (2005). Regularization variable selection via elastic net. Journal Royal Statistical Society: Series B, 67(2), 301–320. https://doi.org/10.1111/j.1467-9868.2005.00503.x details regularization technique. Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 Lee, S.-., Lee, H., Abbeel, P., & Ng, . Y. (2006). Efficient L1 Regularized Logistic Regression. Proceedings Twenty-First National Conference Artificial Intelligence (AAAI-06), 401–408.","code":""},{"path":"/reference/smoothElasticNet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"smoothElasticNet — smoothElasticNet","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +      l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +      l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Optional: Plot the model # semPlot::semPaths(lavaanModel, #                   what = \"est\", #                   fade = FALSE)  # names of the regularized parameters: regularized = paste0(\"l\", 6:15)  lsem <- smoothElasticNet(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   regularized = regularized,   epsilon = 1e-10,   tau = 1e-4,   lambdas = seq(0,1,length.out = 50),   alphas = seq(0,1,length.out = 4))  # the coefficients can be accessed with: coef(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters[1,]"},{"path":"/reference/smoothElasticNetGradient.html","id":null,"dir":"Reference","previous_headings":"","what":"smoothElasticNetGradient — smoothElasticNetGradient","title":"smoothElasticNetGradient — smoothElasticNetGradient","text":"smoothed version non-differentiable elastic LASSO gradient","code":""},{"path":"/reference/smoothElasticNetGradient.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"smoothElasticNetGradient — smoothElasticNetGradient","text":"","code":"smoothElasticNetGradient(   parameters,   tuningParameters,   penaltyFunctionArguments )"},{"path":"/reference/smoothElasticNetGradient.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"smoothElasticNetGradient — smoothElasticNetGradient","text":"parameters vector labeled parameter values tuningParameters list fields lambda (tuning parameter value), alpha (0<alpha<1. Controls weight ridge lasso terms. alpha = 1 lasso, alpha = 0 ridge) penaltyFunctionArguments list fields regularizedParameterLabels (labels regularized parameters), eps (controls smooth approximation non-differential penalty functions (e.g., lasso, adaptive lasso, elastic net). Smaller values result closer approximation, may also cause larger issues optimization.)","code":""},{"path":"/reference/smoothElasticNetGradient.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"smoothElasticNetGradient — smoothElasticNetGradient","text":"gradient values","code":""},{"path":"/reference/smoothElasticNetHessian.html","id":null,"dir":"Reference","previous_headings":"","what":"smoothElasticNetHessian — smoothElasticNetHessian","title":"smoothElasticNetHessian — smoothElasticNetHessian","text":"smoothed version non-differentiable elastic LASSO Hessian","code":""},{"path":"/reference/smoothElasticNetHessian.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"smoothElasticNetHessian — smoothElasticNetHessian","text":"","code":"smoothElasticNetHessian(parameters, tuningParameters, penaltyFunctionArguments)"},{"path":"/reference/smoothElasticNetHessian.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"smoothElasticNetHessian — smoothElasticNetHessian","text":"parameters vector labeled parameter values tuningParameters list fields lambda (tuning parameter value), alpha (0<alpha<1. Controls weight ridge lasso terms. alpha = 1 lasso, alpha = 0 ridge) penaltyFunctionArguments list fields regularizedParameterLabels (labels regularized parameters), eps (controls smooth approximation non-differential penalty functions (e.g., lasso, adaptive lasso, elastic net). Smaller values result closer approximation, may also cause larger issues optimization.)","code":""},{"path":"/reference/smoothElasticNetHessian.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"smoothElasticNetHessian — smoothElasticNetHessian","text":"Hessian matrix","code":""},{"path":"/reference/smoothElasticNetValue.html","id":null,"dir":"Reference","previous_headings":"","what":"smoothElasticNetValue — smoothElasticNetValue","title":"smoothElasticNetValue — smoothElasticNetValue","text":"smoothed version non-differentiable elastic LASSO penalty","code":""},{"path":"/reference/smoothElasticNetValue.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"smoothElasticNetValue — smoothElasticNetValue","text":"","code":"smoothElasticNetValue(parameters, tuningParameters, penaltyFunctionArguments)"},{"path":"/reference/smoothElasticNetValue.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"smoothElasticNetValue — smoothElasticNetValue","text":"parameters vector labeled parameter values tuningParameters list fields lambda (tuning parameter value), alpha (0<alpha<1. Controls weight ridge lasso terms. alpha = 1 lasso, alpha = 0 ridge) penaltyFunctionArguments list fields regularizedParameterLabels (labels regularized parameters), eps (controls smooth approximation non-differential penalty functions (e.g., lasso, adaptive lasso, elastic net). Smaller values result closer approximation, may also cause larger issues optimization.)","code":""},{"path":"/reference/smoothElasticNetValue.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"smoothElasticNetValue — smoothElasticNetValue","text":"penalty function value","code":""},{"path":"/reference/smoothLASSOGradient.html","id":null,"dir":"Reference","previous_headings":"","what":"smoothLASSOGradient — smoothLASSOGradient","title":"smoothLASSOGradient — smoothLASSOGradient","text":"smoothed version non-differentiable LASSO gradient","code":""},{"path":"/reference/smoothLASSOGradient.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"smoothLASSOGradient — smoothLASSOGradient","text":"","code":"smoothLASSOGradient(parameters, tuningParameters, penaltyFunctionArguments)"},{"path":"/reference/smoothLASSOGradient.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"smoothLASSOGradient — smoothLASSOGradient","text":"parameters vector labeled parameter values tuningParameters list field lambda (tuning parameter value) penaltyFunctionArguments list field regularizedParameterLabels (labels regularized parameters), eps (controls smooth approximation non-differential penalty functions (e.g., lasso, adaptive lasso, elastic net). Smaller values result closer approximation, may also cause larger issues optimization.)","code":""},{"path":"/reference/smoothLASSOGradient.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"smoothLASSOGradient — smoothLASSOGradient","text":"gradient values","code":""},{"path":"/reference/smoothLASSOHessian.html","id":null,"dir":"Reference","previous_headings":"","what":"smoothLASSOHessian — smoothLASSOHessian","title":"smoothLASSOHessian — smoothLASSOHessian","text":"smoothed version non-differentiable LASSO Hessian","code":""},{"path":"/reference/smoothLASSOHessian.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"smoothLASSOHessian — smoothLASSOHessian","text":"","code":"smoothLASSOHessian(parameters, tuningParameters, penaltyFunctionArguments)"},{"path":"/reference/smoothLASSOHessian.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"smoothLASSOHessian — smoothLASSOHessian","text":"parameters vector labeled parameter values tuningParameters list field lambda (tuning parameter value) penaltyFunctionArguments list field regularizedParameterLabels (labels regularized parameters), eps (controls smooth approximation non-differential penalty functions (e.g., lasso, adaptive lasso, elastic net). Smaller values result closer approximation, may also cause larger issues optimization.)","code":""},{"path":"/reference/smoothLASSOHessian.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"smoothLASSOHessian — smoothLASSOHessian","text":"Hessian matrix","code":""},{"path":"/reference/smoothLASSOValue.html","id":null,"dir":"Reference","previous_headings":"","what":"smoothLASSOValue — smoothLASSOValue","title":"smoothLASSOValue — smoothLASSOValue","text":"smoothed version non-differentiable LASSO penalty","code":""},{"path":"/reference/smoothLASSOValue.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"smoothLASSOValue — smoothLASSOValue","text":"","code":"smoothLASSOValue(parameters, tuningParameters, penaltyFunctionArguments)"},{"path":"/reference/smoothLASSOValue.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"smoothLASSOValue — smoothLASSOValue","text":"parameters vector labeled parameter values tuningParameters list field lambda (tuning parameter value) penaltyFunctionArguments list field regularizedParameterLabels (labels regularized parameters), eps (controls smooth approximation non-differential penalty functions (e.g., lasso, adaptive lasso, elastic net). Smaller values result closer approximation, may also cause larger issues optimization.)","code":""},{"path":"/reference/smoothLASSOValue.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"smoothLASSOValue — smoothLASSOValue","text":"penalty function value","code":""},{"path":"/reference/smoothLasso.html","id":null,"dir":"Reference","previous_headings":"","what":"smoothLasso — smoothLasso","title":"smoothLasso — smoothLasso","text":"function allows regularization models built lavaan smoothed lasso penalty. returned object S4 class; elements can accessed \"@\" operator (see examples). recommend using function. Use lasso() instead.","code":""},{"path":"/reference/smoothLasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"smoothLasso — smoothLasso","text":"","code":"smoothLasso(   lavaanModel,   regularized,   lambdas,   epsilon,   tau,   modifyModel = lessSEM::modifyModel(),   control = lessSEM::controlBFGS() )"},{"path":"/reference/smoothLasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"smoothLasso — smoothLasso","text":"lavaanModel model class lavaan regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda epsilon epsilon > 0; controls smoothness approximation. Larger values = smoother tau parameters threshold tau seen zeroed modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. element generated controlBFGS function. See ?controlBFGS details.","code":""},{"path":"/reference/smoothLasso.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"smoothLasso — smoothLasso","text":"Model class regularizedSEM","code":""},{"path":"/reference/smoothLasso.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"smoothLasso — smoothLasso","text":"details, see: Lee, S.-., Lee, H., Abbeel, P., & Ng, . Y. (2006). Efficient L1 Regularized Logistic Regression. Proceedings Twenty-First National Conference Artificial Intelligence (AAAI-06), 401–408. Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793","code":""},{"path":"/reference/smoothLasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"smoothLasso — smoothLasso","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +      l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +      l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Optional: Plot the model # semPlot::semPaths(lavaanModel, #                   what = \"est\", #                   fade = FALSE)  lsem <- smoothLasso(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   # names of the regularized parameters:   regularized = paste0(\"l\", 6:15),   epsilon = 1e-10,   tau = 1e-4,   lambdas = seq(0,1,length.out = 50))  # use the plot-function to plot the regularized parameters: plot(lsem)  # the coefficients can be accessed with: coef(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters[1,]  # AIC and BIC: AIC(lsem) BIC(lsem)  # The best parameters can also be extracted with: coef(lsem, criterion = \"AIC\") coef(lsem, criterion = \"BIC\")"},{"path":"/reference/smoothLspValue.html","id":null,"dir":"Reference","previous_headings":"","what":"smoothLspValue — smoothLspValue","title":"smoothLspValue — smoothLspValue","text":"smoothed version lsp penalty","code":""},{"path":"/reference/smoothLspValue.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"smoothLspValue — smoothLspValue","text":"","code":"smoothLspValue(parameters, tuningParameters, penaltyFunctionArguments)"},{"path":"/reference/smoothLspValue.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"smoothLspValue — smoothLspValue","text":"parameters vector labeled parameter values tuningParameters list field lambda (tuning parameter value) penaltyFunctionArguments list field regularizedParameterLabels (labels regularized parameters), eps (controls smooth approximation non-differential penalty functions (e.g., lasso, adaptive lasso, elastic net). Smaller values result closer approximation, may also cause larger issues optimization.)","code":""},{"path":"/reference/smoothLspValue.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"smoothLspValue — smoothLspValue","text":"penalty function value","code":""},{"path":"/reference/smoothMcpValue.html","id":null,"dir":"Reference","previous_headings":"","what":"smoothMcpValue — smoothMcpValue","title":"smoothMcpValue — smoothMcpValue","text":"smoothed version mcp penalty","code":""},{"path":"/reference/smoothMcpValue.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"smoothMcpValue — smoothMcpValue","text":"","code":"smoothMcpValue(parameters, tuningParameters, penaltyFunctionArguments)"},{"path":"/reference/smoothMcpValue.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"smoothMcpValue — smoothMcpValue","text":"parameters vector labeled parameter values tuningParameters list field lambda (tuning parameter value) penaltyFunctionArguments list field regularizedParameterLabels (labels regularized parameters), eps (controls smooth approximation non-differential penalty functions (e.g., lasso, adaptive lasso, elastic net). Smaller values result closer approximation, may also cause larger issues optimization.)","code":""},{"path":"/reference/smoothMcpValue.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"smoothMcpValue — smoothMcpValue","text":"penalty function value","code":""},{"path":"/reference/smoothScadValue.html","id":null,"dir":"Reference","previous_headings":"","what":"smoothScadValue — smoothScadValue","title":"smoothScadValue — smoothScadValue","text":"smoothed version scad penalty","code":""},{"path":"/reference/smoothScadValue.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"smoothScadValue — smoothScadValue","text":"","code":"smoothScadValue(parameters, tuningParameters, penaltyFunctionArguments)"},{"path":"/reference/smoothScadValue.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"smoothScadValue — smoothScadValue","text":"parameters vector labeled parameter values tuningParameters list field lambda (tuning parameter value) penaltyFunctionArguments list field regularizedParameterLabels (labels regularized parameters), eps (controls smooth approximation non-differential penalty functions (e.g., lasso, adaptive lasso, elastic net). Smaller values result closer approximation, may also cause larger issues optimization.)","code":""},{"path":"/reference/smoothScadValue.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"smoothScadValue — smoothScadValue","text":"penalty function value","code":""},{"path":"/reference/summary-cvRegularizedSEM-method.html","id":null,"dir":"Reference","previous_headings":"","what":"summary method for objects of class cvRegularizedSEM. — summary,cvRegularizedSEM-method","title":"summary method for objects of class cvRegularizedSEM. — summary,cvRegularizedSEM-method","text":"summary method objects class cvRegularizedSEM.","code":""},{"path":"/reference/summary-cvRegularizedSEM-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"summary method for objects of class cvRegularizedSEM. — summary,cvRegularizedSEM-method","text":"","code":"# S4 method for cvRegularizedSEM summary(object)"},{"path":"/reference/summary-cvRegularizedSEM-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"summary method for objects of class cvRegularizedSEM. — summary,cvRegularizedSEM-method","text":"object object class cvRegularizedSEM","code":""},{"path":"/reference/summary-gpRegularized-method.html","id":null,"dir":"Reference","previous_headings":"","what":"summary — summary,gpRegularized-method","title":"summary — summary,gpRegularized-method","text":"summary","code":""},{"path":"/reference/summary-gpRegularized-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"summary — summary,gpRegularized-method","text":"","code":"# S4 method for gpRegularized summary(object)"},{"path":"/reference/summary-gpRegularized-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"summary — summary,gpRegularized-method","text":"object object class gpRegularized","code":""},{"path":"/reference/summary-regularizedSEM-method.html","id":null,"dir":"Reference","previous_headings":"","what":"summary — summary,regularizedSEM-method","title":"summary — summary,regularizedSEM-method","text":"summary","code":""},{"path":"/reference/summary-regularizedSEM-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"summary — summary,regularizedSEM-method","text":"","code":"# S4 method for regularizedSEM summary(object)"},{"path":"/reference/summary-regularizedSEM-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"summary — summary,regularizedSEM-method","text":"object object class regularizedSEM","code":""},{"path":"/reference/summary-regularizedSEMMixedPenalty-method.html","id":null,"dir":"Reference","previous_headings":"","what":"summary — summary,regularizedSEMMixedPenalty-method","title":"summary — summary,regularizedSEMMixedPenalty-method","text":"summary","code":""},{"path":"/reference/summary-regularizedSEMMixedPenalty-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"summary — summary,regularizedSEMMixedPenalty-method","text":"","code":"# S4 method for regularizedSEMMixedPenalty summary(object)"},{"path":"/reference/summary-regularizedSEMMixedPenalty-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"summary — summary,regularizedSEMMixedPenalty-method","text":"object object class regularizedSEMMixedPenalty","code":""},{"path":"/reference/summary-regularizedSEMWithCustomPenalty-method.html","id":null,"dir":"Reference","previous_headings":"","what":"summary — summary,regularizedSEMWithCustomPenalty-method","title":"summary — summary,regularizedSEMWithCustomPenalty-method","text":"summary","code":""},{"path":"/reference/summary-regularizedSEMWithCustomPenalty-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"summary — summary,regularizedSEMWithCustomPenalty-method","text":"","code":"# S4 method for regularizedSEMWithCustomPenalty summary(object)"},{"path":"/reference/summary-regularizedSEMWithCustomPenalty-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"summary — summary,regularizedSEMWithCustomPenalty-method","text":"object object class regularizedSEMWithCustomPenalty","code":""}]
