[{"path":"/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"GNU General Public License","title":"GNU General Public License","text":"Version 2, June 1991Copyright © 1989, 1991 Free Software Foundation, Inc.,51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA Everyone permitted copy distribute verbatim copies license document, changing allowed.","code":""},{"path":"/LICENSE.html","id":"preamble","dir":"","previous_headings":"","what":"Preamble","title":"GNU General Public License","text":"licenses software designed take away freedom share change . contrast, GNU General Public License intended guarantee freedom share change free software–make sure software free users. General Public License applies Free Software Foundation’s software program whose authors commit using . (Free Software Foundation software covered GNU Lesser General Public License instead.) can apply programs, . speak free software, referring freedom, price. General Public Licenses designed make sure freedom distribute copies free software (charge service wish), receive source code can get want , can change software use pieces new free programs; know can things. protect rights, need make restrictions forbid anyone deny rights ask surrender rights. restrictions translate certain responsibilities distribute copies software, modify . example, distribute copies program, whether gratis fee, must give recipients rights . must make sure , , receive can get source code. must show terms know rights. protect rights two steps: (1) copyright software, (2) offer license gives legal permission copy, distribute /modify software. Also, author’s protection , want make certain everyone understands warranty free software. software modified someone else passed , want recipients know original, problems introduced others reflect original authors’ reputations. Finally, free program threatened constantly software patents. wish avoid danger redistributors free program individually obtain patent licenses, effect making program proprietary. prevent , made clear patent must licensed everyone’s free use licensed . precise terms conditions copying, distribution modification follow.","code":""},{"path":"/LICENSE.html","id":"terms-and-conditions-for-copying-distribution-and-modification","dir":"","previous_headings":"","what":"TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION","title":"GNU General Public License","text":"0. License applies program work contains notice placed copyright holder saying may distributed terms General Public License. “Program”, , refers program work, “work based Program” means either Program derivative work copyright law: say, work containing Program portion , either verbatim modifications /translated another language. (Hereinafter, translation included without limitation term “modification”.) licensee addressed “”. Activities copying, distribution modification covered License; outside scope. act running Program restricted, output Program covered contents constitute work based Program (independent made running Program). Whether true depends Program . 1. may copy distribute verbatim copies Program’s source code receive , medium, provided conspicuously appropriately publish copy appropriate copyright notice disclaimer warranty; keep intact notices refer License absence warranty; give recipients Program copy License along Program. may charge fee physical act transferring copy, may option offer warranty protection exchange fee. 2. may modify copy copies Program portion , thus forming work based Program, copy distribute modifications work terms Section 1 , provided also meet conditions: ) must cause modified files carry prominent notices stating changed files date change. b) must cause work distribute publish, whole part contains derived Program part thereof, licensed whole charge third parties terms License. c) modified program normally reads commands interactively run, must cause , started running interactive use ordinary way, print display announcement including appropriate copyright notice notice warranty (else, saying provide warranty) users may redistribute program conditions, telling user view copy License. (Exception: Program interactive normally print announcement, work based Program required print announcement.) requirements apply modified work whole. identifiable sections work derived Program, can reasonably considered independent separate works , License, terms, apply sections distribute separate works. distribute sections part whole work based Program, distribution whole must terms License, whose permissions licensees extend entire whole, thus every part regardless wrote . Thus, intent section claim rights contest rights work written entirely ; rather, intent exercise right control distribution derivative collective works based Program. addition, mere aggregation another work based Program Program (work based Program) volume storage distribution medium bring work scope License. 3. may copy distribute Program (work based , Section 2) object code executable form terms Sections 1 2 provided also one following: ) Accompany complete corresponding machine-readable source code, must distributed terms Sections 1 2 medium customarily used software interchange; , b) Accompany written offer, valid least three years, give third party, charge cost physically performing source distribution, complete machine-readable copy corresponding source code, distributed terms Sections 1 2 medium customarily used software interchange; , c) Accompany information received offer distribute corresponding source code. (alternative allowed noncommercial distribution received program object code executable form offer, accord Subsection b .) source code work means preferred form work making modifications . executable work, complete source code means source code modules contains, plus associated interface definition files, plus scripts used control compilation installation executable. However, special exception, source code distributed need include anything normally distributed (either source binary form) major components (compiler, kernel, ) operating system executable runs, unless component accompanies executable. distribution executable object code made offering access copy designated place, offering equivalent access copy source code place counts distribution source code, even though third parties compelled copy source along object code. 4. may copy, modify, sublicense, distribute Program except expressly provided License. attempt otherwise copy, modify, sublicense distribute Program void, automatically terminate rights License. However, parties received copies, rights, License licenses terminated long parties remain full compliance. 5. required accept License, since signed . However, nothing else grants permission modify distribute Program derivative works. actions prohibited law accept License. Therefore, modifying distributing Program (work based Program), indicate acceptance License , terms conditions copying, distributing modifying Program works based . 6. time redistribute Program (work based Program), recipient automatically receives license original licensor copy, distribute modify Program subject terms conditions. may impose restrictions recipients’ exercise rights granted herein. responsible enforcing compliance third parties License. 7. , consequence court judgment allegation patent infringement reason (limited patent issues), conditions imposed (whether court order, agreement otherwise) contradict conditions License, excuse conditions License. distribute satisfy simultaneously obligations License pertinent obligations, consequence may distribute Program . example, patent license permit royalty-free redistribution Program receive copies directly indirectly , way satisfy License refrain entirely distribution Program. portion section held invalid unenforceable particular circumstance, balance section intended apply section whole intended apply circumstances. purpose section induce infringe patents property right claims contest validity claims; section sole purpose protecting integrity free software distribution system, implemented public license practices. Many people made generous contributions wide range software distributed system reliance consistent application system; author/donor decide willing distribute software system licensee impose choice. section intended make thoroughly clear believed consequence rest License. 8. distribution /use Program restricted certain countries either patents copyrighted interfaces, original copyright holder places Program License may add explicit geographical distribution limitation excluding countries, distribution permitted among countries thus excluded. case, License incorporates limitation written body License. 9. Free Software Foundation may publish revised /new versions General Public License time time. new versions similar spirit present version, may differ detail address new problems concerns. version given distinguishing version number. Program specifies version number License applies “later version”, option following terms conditions either version later version published Free Software Foundation. Program specify version number License, may choose version ever published Free Software Foundation. 10. wish incorporate parts Program free programs whose distribution conditions different, write author ask permission. software copyrighted Free Software Foundation, write Free Software Foundation; sometimes make exceptions . decision guided two goals preserving free status derivatives free software promoting sharing reuse software generally.","code":""},{"path":"/LICENSE.html","id":"no-warranty","dir":"","previous_headings":"","what":"NO WARRANTY","title":"GNU General Public License","text":"11. PROGRAM LICENSED FREE CHARGE, WARRANTY PROGRAM, EXTENT PERMITTED APPLICABLE LAW. EXCEPT OTHERWISE STATED WRITING COPYRIGHT HOLDERS /PARTIES PROVIDE PROGRAM “” WITHOUT WARRANTY KIND, EITHER EXPRESSED IMPLIED, INCLUDING, LIMITED , IMPLIED WARRANTIES MERCHANTABILITY FITNESS PARTICULAR PURPOSE. ENTIRE RISK QUALITY PERFORMANCE PROGRAM . PROGRAM PROVE DEFECTIVE, ASSUME COST NECESSARY SERVICING, REPAIR CORRECTION. 12. EVENT UNLESS REQUIRED APPLICABLE LAW AGREED WRITING COPYRIGHT HOLDER, PARTY MAY MODIFY /REDISTRIBUTE PROGRAM PERMITTED , LIABLE DAMAGES, INCLUDING GENERAL, SPECIAL, INCIDENTAL CONSEQUENTIAL DAMAGES ARISING USE INABILITY USE PROGRAM (INCLUDING LIMITED LOSS DATA DATA RENDERED INACCURATE LOSSES SUSTAINED THIRD PARTIES FAILURE PROGRAM OPERATE PROGRAMS), EVEN HOLDER PARTY ADVISED POSSIBILITY DAMAGES. END TERMS CONDITIONS","code":""},{"path":"/LICENSE.html","id":"how-to-apply-these-terms-to-your-new-programs","dir":"","previous_headings":"","what":"How to Apply These Terms to Your New Programs","title":"GNU General Public License","text":"develop new program, want greatest possible use public, best way achieve make free software everyone can redistribute change terms. , attach following notices program. safest attach start source file effectively convey exclusion warranty; file least “copyright” line pointer full notice found. Also add information contact electronic paper mail. program interactive, make output short notice like starts interactive mode: hypothetical commands show w show c show appropriate parts General Public License. course, commands use may called something show w show c; even mouse-clicks menu items–whatever suits program. also get employer (work programmer) school, , sign “copyright disclaimer” program, necessary. sample; alter names: General Public License permit incorporating program proprietary programs. program subroutine library, may consider useful permit linking proprietary applications library. want , use GNU Lesser General Public License instead License.","code":"<one line to give the program's name and a brief idea of what it does.> Copyright (C) <year>  <name of author>  This program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 2 of the License, or (at your option) any later version.  This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.  You should have received a copy of the GNU General Public License along with this program; if not, write to the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA. Gnomovision version 69, Copyright (C) year name of author Gnomovision comes with ABSOLUTELY NO WARRANTY; for details type `show w'. This is free software, and you are welcome to redistribute it under certain conditions; type `show c' for details. Yoyodyne, Inc., hereby disclaims all copyright interest in the program `Gnomovision' (which makes passes at compilers) written by James Hacker.  <signature of Ty Coon>, 1 April 1989 Ty Coon, President of Vice"},{"path":"/articles/Definition-Variables-and-Multi-Group-SEM.html","id":"first-step-setting-up-a-multi-group-model","dir":"Articles","previous_headings":"","what":"First Step: Setting up a Multi-Group Model","title":"Definition-Variables-and-Multi-Group-SEM","text":"set multi-group model lessSEM, first fit separate models groups lavaan:","code":"library(lavaan)  # For simplicity, we will use a subset of the Holzinger Swineford data set  # that is also used at https://lavaan.ugent.be/tutorial/groups.html # to demonstrate multi-group SEM  # To use mutli-group SEM in lessSEM, we have to set up a separate model # for each of the groups:  # - Pasteur: Children attending the Pasteur school # - Grant_White: Children attending the Grant-White school data(HolzingerSwineford1939)  ## Pasteur ## Pasteur <- subset(HolzingerSwineford1939, school == \"Pasteur\")  model_Pasteur <- paste0('      visual  =~ l1_Pasteur*x1 + l2_Pasteur*x2 + l3_Pasteur*x3     x1 ~~ v1*x1     x2 ~~ v2*x2     x3 ~~ v3*x3          visual ~~ lv1*visual     x1 ~ m1*1     x2 ~ m2*1     x3 ~ m3*1') fit_Pasteur <- sem(model = model_Pasteur,                     data = Pasteur,                     std.lv = TRUE)  ## Grant-White Grant_White <- subset(HolzingerSwineford1939, school == \"Grant-White\")  model_Grant_White <- paste0('      visual  =~ l1_Grant_White*x1 + l2_Grant_White*x2 + l3_Grant_White*x3     x1 ~~ v1*x1     x2 ~~ v2*x2     x3 ~~ v3*x3          visual ~~ lv1*visual     x1 ~ m1*1     x2 ~ m2*1     x3 ~ m3*1') fit_Grant_White <- sem(model = model_Grant_White,                         data = Grant_White,                         std.lv = TRUE)"},{"path":"/articles/Definition-Variables-and-Multi-Group-SEM.html","id":"second-step-pass-the-model-to-lesssem","dir":"Articles","previous_headings":"","what":"Second Step: Pass the Model to lessSEM","title":"Definition-Variables-and-Multi-Group-SEM","text":"Now group-specific models, can pass lessSEM: Let’s look parameters: ’s curious! group-specific parameters, parameters provided group-specific names! Important: set multi-group model lessSEM, lessSEM assume parameters names also values. includes parameters may estimated, names provided lavaan (e.g., variances).","code":"library(lessSEM)  # We will just estimate the parameters using the BFGS optimizer without any  # regularization. # Note that we pass the two models as a vector. lessSEM # will then set up the multi-group model fit <- bfgs(lavaanModel = c(fit_Pasteur, fit_Grant_White)) coef(fit) #>                                                                                #>   Tuning         ||--||  Estimates                                             #>  ------- ------- ||--|| ---------- ---------- ---------- ---------- ---------- #>   lambda   alpha ||--|| l1_Pasteur l2_Pasteur l3_Pasteur         v1         v2 #>  ======= ======= ||--|| ========== ========== ========== ========== ========== #>   0.0000  0.0000 ||--||     0.7240     0.5610     0.8824     0.8449     1.0711 #>                                                                            #>                                                                            #>  ---------- ---------- ---------- ---------- -------------- -------------- #>          v3         m1         m2         m3 l1_Grant_White l2_Grant_White #>  ========== ========== ========== ========== ============== ============== #>      0.6108     4.9212     6.0770     2.2281         0.7088         0.5536 #>                 #>                 #>  -------------- #>  l3_Grant_White #>  ============== #>          0.7360"},{"path":"/articles/Definition-Variables-and-Multi-Group-SEM.html","id":"different-models-with-shared-parameter-labels","dir":"Articles","previous_headings":"Second Step: Pass the Model to lessSEM","what":"Different Models with Shared Parameter Labels","title":"Definition-Variables-and-Multi-Group-SEM","text":"parameters labels multiple models constrained equality across models! careful, can result annoying mistakes. demonstrate , use two different models may share parameter names. Let’s first compare fit separate models multi-group model. lessSEM estimate models truly separately expect fit : Obviously, two fits . may happened? Looking parameter estimates multi-group model shows two models, fitPolDem fitHS share parameter labels! instance, intercepts x1 called x1~1 models. Therefore, lessSEM assumed wanted parameters exactly value models.","code":"# Model from ?lavaan::sem model <- '    # latent variable definitions      ind60 =~ x1 + x2 + x3      dem60 =~ y1 + a*y2 + b*y3 + c*y4      dem65 =~ y5 + a*y6 + b*y7 + c*y8    # regressions     dem60 ~ ind60     dem65 ~ ind60 + dem60    # residual correlations     y1 ~~ y5     y2 ~~ y4 + y6     y3 ~~ y7     y4 ~~ y8     y6 ~~ y8 '  fitPolDem <- sem(model,                   data = PoliticalDemocracy,                  meanstructure = TRUE)  # Model from ?lavaan::cfa HS.model <- ' visual  =~ x1 + x2 + x3               textual =~ x4 + x5 + x6               speed   =~ x7 + x8 + x9 '  fitHS <- cfa(HS.model,              data = HolzingerSwineford1939,              meanstructure = TRUE)  ## lessSEM does not care if the models passed to the function are similar  # or even use different data sets. Of course, it probably does not make much sense # to estimate two different models at the same time, but lessSEM won't stop you # from trying... fit <- bfgs(lavaanModel = c(fitPolDem, fitHS)) # fit for separate models -2*logLik(fitPolDem) + (-2)*logLik(fitHS) #> 'log Lik.' 10573.13 (df=39)  # fit for multi-group model: fit@fits$m2LL #> [1] 10813.96 coef(fit) #>                                                                                #>   Tuning         ||--||  Estimates                                             #>  ------- ------- ||--|| ---------- ---------- ---------- ---------- ---------- #>   lambda   alpha ||--||  ind60=~x2  ind60=~x3          a          b          c #>  ======= ======= ||--|| ========== ========== ========== ========== ========== #>   0.0000  0.0000 ||--||     1.8164     1.5582     1.1879     1.1706     1.2511 #>                                                                       #>                                                                       #>  ----------- ----------- ----------- ---------- ---------- ---------- #>  dem60~ind60 dem65~ind60 dem65~dem60     y1~~y5     y2~~y4     y2~~y6 #>  =========== =========== =========== ========== ========== ========== #>       1.4062      0.4696      0.8755     0.5389     1.4270     2.2120 #>                                                                               #>                                                                               #>  ---------- ---------- ---------- ---------- ---------- ---------- ---------- #>      y3~~y7     y4~~y8     y6~~y8     x1~~x1     x2~~x2     x3~~x3     y1~~y1 #>  ========== ========== ========== ========== ========== ========== ========== #>      0.7425     0.3718     1.3734     0.0000     1.3895     1.2086     1.8544 #>                                                                               #>                                                                               #>  ---------- ---------- ---------- ---------- ---------- ---------- ---------- #>      y2~~y2     y3~~y3     y4~~y4     y5~~y5     y6~~y6     y7~~y7     y8~~y8 #>  ========== ========== ========== ========== ========== ========== ========== #>      7.5981     4.9592     3.2006     2.2664     4.9911     3.6046     3.3135 #>                                                                          #>                                                                          #>  ------------ ------------ ------------ ---------- ---------- ---------- #>  ind60~~ind60 dem60~~dem60 dem65~~dem65       x1~1       x2~1       x3~1 #>  ============ ============ ============ ========== ========== ========== #>        0.5305       3.8021       0.2003     5.0409     5.8482     2.5445 #>                                                                               #>                                                                               #>  ---------- ---------- ---------- ---------- ---------- ---------- ---------- #>        y1~1       y2~1       y3~1       y4~1       y5~1       y6~1       y7~1 #>  ========== ========== ========== ========== ========== ========== ========== #>      5.4463     4.2330     6.5420     4.4295     5.1139     2.9502     6.1703 #>                                                                                 #>                                                                                 #>  ---------- ---------- ---------- ----------- ----------- ---------- ---------- #>        y8~1 visual=~x2 visual=~x3 textual=~x5 textual=~x6  speed=~x8  speed=~x9 #>  ========== ========== ========== =========== =========== ========== ========== #>      4.0150     0.2791     0.4461      1.1126      0.9221     1.1746     1.0056 #>                                                                    #>                                                                    #>  ---------- ---------- ---------- ---------- ---------- ---------- #>      x4~~x4     x5~~x5     x6~~x6     x7~~x7     x8~~x8     x9~~x9 #>  ========== ========== ========== ========== ========== ========== #>      0.3681     0.4434     0.3610     0.7750     0.4590     0.6025 #>                                                                             #>                                                                             #>  -------------- ---------------- ------------ --------------- ------------- #>  visual~~visual textual~~textual speed~~speed visual~~textual visual~~speed #>  ============== ================ ============ =============== ============= #>          1.3695           0.9839       0.4084          0.4666        0.2615 #>                                                                        #>                                                                        #>  -------------- ---------- ---------- ---------- ---------- ---------- #>  textual~~speed       x4~1       x5~1       x6~1       x7~1       x8~1 #>  ============== ========== ========== ========== ========== ========== #>          0.1746     3.0967     4.3804     2.2186     4.2060     5.5507 #>             #>             #>  ---------- #>        x9~1 #>  ========== #>      5.3943"},{"path":"/articles/Definition-Variables-and-Multi-Group-SEM.html","id":"regularized-multi-group-models","dir":"Articles","previous_headings":"","what":"Regularized Multi-Group Models","title":"Definition-Variables-and-Multi-Group-SEM","text":"multi-group models can regularized similar standard SEM: Instead using bfgs-function, use (instance), lasso-function: coefficients can extracted usual:","code":"fit <- lasso(lavaanModel = c(fit_Pasteur, fit_Grant_White),              regularized = c(\"l1_Pasteur\"),               nLambdas = 20) coef(fit, criterion = \"AIC\") #>                                                                                #>   Tuning         ||--||  Estimates                                             #>  ------- ------- ||--|| ---------- ---------- ---------- ---------- ---------- #>   lambda   alpha ||--|| l1_Pasteur l2_Pasteur l3_Pasteur         v1         v2 #>  ======= ======= ||--|| ========== ========== ========== ========== ========== #>   0.0000  1.0000 ||--||     0.7239     0.5609     0.8826     0.8450     1.0712 #>                                                                            #>                                                                            #>  ---------- ---------- ---------- ---------- -------------- -------------- #>          v3         m1         m2         m3 l1_Grant_White l2_Grant_White #>  ========== ========== ========== ========== ============== ============== #>      0.6107     4.9212     6.0769     2.2280         0.7087         0.5536 #>                 #>                 #>  -------------- #>  l3_Grant_White #>  ============== #>          0.7361"},{"path":"/articles/Definition-Variables-and-Multi-Group-SEM.html","id":"regularizing-differences-between-parameters-using-lesssem","dir":"Articles","previous_headings":"Regularized Multi-Group Models","what":"Regularizing Differences Between Parameters using lessSEM","title":"Definition-Variables-and-Multi-Group-SEM","text":"regularized multi-group models shine automatically testing group-differences. proposed Huang (2018) provides convenient way decide parameters group-specific. end, differences parameters must regularized. Say, interested loading l1 wonder indeed need separate loadings students attending Pasteur school (l1_Pasteur) Grant-White school (l1_Grant_White). Using Pasteur school baseline group (see Huang, 2018, details), can define l1_Grant_White = l1_Pasteur + l1_delta, l1_delta difference two schools. l1_delta zero, schools loading (.e., measurement invariance). Within lessSEM, can regularize differences using transformations (see vignette(topic = \"Parameter-transformations\", package = \"lessSEM\") details). Therefore, first step define transformation: Next, pass transformation model: Now, let’s look parameter estimates: l1_delta parameter set zero, can assume measurement invariance. Note won’t find l1_Grant_White parameters model. l1_Grant_White deterministic function actual parameters l1_Pasteur l1_delta. want find value l1_Grant_White, look : Note lslx (Huang, 2020) supports different penalties delta parameter (l1_delta) baseline parameter (l1_Pasteur). currently supported lessSEM.","code":"transformation <- \" parameters: l1_Pasteur, l1_Grant_White, l1_delta l1_Grant_White = l1_Pasteur + l1_delta; \" fit <- lasso(lavaanModel = c(fit_Pasteur, fit_Grant_White),              regularized = c(\"l1_delta\"), # we want to regularize the difference!              nLambdas = 20,              modifyModel = modifyModel(transformations = transformation)) coef(fit, criterion = \"AIC\")@estimates[,c(\"l1_Pasteur\", \"l1_delta\")] #> l1_Pasteur   l1_delta  #>   0.716718   0.000000 fit@transformations #>          lambda alpha l1_Grant_White #> 1  0.0056517504     1      0.7167576 #> 2  0.0053542898     1      0.7167180 #> 3  0.0050568293     1      0.7166009 #> 4  0.0047593687     1      0.7161368 #> 5  0.0044619082     1      0.7155761 #> 6  0.0041644476     1      0.7151688 #> 7  0.0038669871     1      0.7147807 #> 8  0.0035695265     1      0.7141878 #> 9  0.0032720660     1      0.7137695 #> 10 0.0029746055     1      0.7132645 #> 11 0.0026771449     1      0.7128818 #> 12 0.0023796844     1      0.7125411 #> 13 0.0020822238     1      0.7120493 #> 14 0.0017847633     1      0.7115889 #> 15 0.0014873027     1      0.7110626 #> 16 0.0011898422     1      0.7107554 #> 17 0.0008923816     1      0.7102373 #> 18 0.0005949211     1      0.7097089 #> 19 0.0002974605     1      0.7094042 #> 20 0.0000000000     1      0.7088852"},{"path":"/articles/Definition-Variables-and-Multi-Group-SEM.html","id":"cross-validation","dir":"Articles","previous_headings":"","what":"Cross-Validation","title":"Definition-Variables-and-Multi-Group-SEM","text":"Automatic cross-validation multi-group models , instance, cvLasso yet implemented. can difficult decide split data set submodel. want use cross-validation, (unfortunately) set procedure manually.","code":""},{"path":"/articles/Definition-Variables-and-Multi-Group-SEM.html","id":"definition-variables","dir":"Articles","previous_headings":"","what":"Definition Variables","title":"Definition-Variables-and-Multi-Group-SEM","text":"Models definition variables basically multi-group models, sole exception group-specific parameters estimated fixed specific values. main interest setting multi-group SEM lessSEM don’t care details, lessTemplates package (https://github.com/jhorzek/lessTemplates) provides means easily set models (see SEMWithDefinitionVariables function lessTemplates). following, look detail definition variables can used lessSEM","code":""},{"path":"/articles/Definition-Variables-and-Multi-Group-SEM.html","id":"the-details","dir":"Articles","previous_headings":"Definition Variables","what":"The details …","title":"Definition-Variables-and-Multi-Group-SEM","text":"Unfortunately, lavaan allow us set models \\(N=1\\), however. required many definition variable applications, latent growth curve models subject-specific measurement occasions. following, use workaround. Let’s first simulate data: Note times random subject-specific. need separate model subject. lavaan won’t let us set models, instead set models using entire data set replace data post-hoc. Exemplarily, makes sense look one models: Note loadings slope fixed time points person provided data. Now can pass models lessSEM: parameters given :","code":"#### Population parameters #### intercept_mu <- 0 intercept_sigma <- 1 slope_mu <- .3 slope_sigma <- 1  #### data set #### N <- 50 intercepts <- rnorm(n = N,                      mean = intercept_mu,                      sd = intercept_sigma) slopes <- rnorm(n = N,                  mean = slope_mu,                  sd = slope_sigma) times <- matrix(seq(0,5,1),                 nrow = N,                 ncol = 6,                 byrow = TRUE) +   cbind(0,matrix(round(runif(n = N*5, min = -.2,max = .2),2),                  nrow = N,                  ncol = 5,                  byrow = TRUE)) # we add some jitter to make the times person-specific  lgcData <- matrix(NA, nrow = N, ncol = ncol(times), dimnames = list(NULL, paste0(\"x\", 0:5)))  for(i in 1:N){   lgcData[i,] <- intercepts[i] + times[i,]* slopes[i] + rnorm(ncol(lgcData),0,.3) } lgcData <- as.data.frame(lgcData)  head(lgcData) #>            x0         x1         x2        x3        x4        x5 #> 1  0.26801876 -0.5353028 -1.0262295 -2.533564 -2.890246 -4.655061 #> 2  0.26260181 -0.4802635 -2.0790754 -2.686983 -3.677022 -4.540501 #> 3 -0.71054447 -0.5477048 -0.5747618 -1.231821 -1.229388 -1.418401 #> 4  0.14564354  1.0690979  1.8919916  2.842025  2.953471  4.654879 #> 5  0.09501816  1.6350899  2.6331440  4.729328  5.423381  6.799296 #> 6  2.51013193  3.0917904  4.1193617  5.749542  6.222599  7.716919  head(times) #>      [,1] [,2] [,3] [,4] [,5] [,6] #> [1,]    0 0.94 2.13 3.01 3.91 5.12 #> [2,]    0 0.84 2.13 2.91 4.10 5.19 #> [3,]    0 0.83 2.14 3.12 3.95 4.93 #> [4,]    0 0.88 2.03 3.16 4.01 5.03 #> [5,]    0 1.07 2.01 3.00 3.81 4.82 #> [6,]    0 1.17 2.11 2.88 4.06 5.06 models <- c() for(i in 1:N){ model_i <- paste0(   \" int =~ 1*x0 + 1*x1 + 1*x2 + 1*x3 + 1*x4 + 1*x5 slope =~ \",times[i,1],\"*x0 +           \",times[i,2],\"*x1 +           \",times[i,3],\"*x2 +            \",times[i,4],\"*x3 +           \",times[i,5],\"*x4 +           \",times[i,6],\"*x5                    int ~ intMean*1          slope ~ slopeMean*1                    int ~~ intVar*int + 0*slope          slope ~~ slopeVar*slope  x0 ~~ v*x0 x1 ~~ v*x1 x2 ~~ v*x2 x3 ~~ v*x3 x4 ~~ v*x4 x5 ~~ v*x5  x0 ~ 0*1 x1 ~ 0*1 x2 ~ 0*1 x3 ~ 0*1 x4 ~ 0*1 x5 ~ 0*1 \"   )  fit_i <- sem(model = model_i,               data = lgcData,               do.fit = FALSE) internalData <- lavInspect(fit_i, \"data\") # replace the data set fit_i@Data@X[[1]] <- as.matrix(lgcData[i,colnames(internalData),drop = FALSE])  models <- c(models,              fit_i) } cat(model_i) #>  #> int =~ 1*x0 + 1*x1 + 1*x2 + 1*x3 + 1*x4 + 1*x5 #> slope =~ 0*x0 +  #>          1.13*x1 +  #>          1.81*x2 +   #>          3.06*x3 +  #>          4.12*x4 +  #>          4.98*x5 #>           #>          int ~ intMean*1 #>          slope ~ slopeMean*1 #>           #>          int ~~ intVar*int + 0*slope #>          slope ~~ slopeVar*slope #>  #> x0 ~~ v*x0 #> x1 ~~ v*x1 #> x2 ~~ v*x2 #> x3 ~~ v*x3 #> x4 ~~ v*x4 #> x5 ~~ v*x5 #>  #> x0 ~ 0*1 #> x1 ~ 0*1 #> x2 ~ 0*1 #> x3 ~ 0*1 #> x4 ~ 0*1 #> x5 ~ 0*1 fit <- bfgs(lavaanModel = models) coef(fit) #>                                                                                #>   Tuning         ||--||  Estimates                                             #>  ------- ------- ||--|| ---------- ---------- ---------- ---------- ---------- #>   lambda   alpha ||--||    intMean  slopeMean     intVar   slopeVar          v #>  ======= ======= ||--|| ========== ========== ========== ========== ========== #>   0.0000  0.0000 ||--||    -0.0152     0.2921     0.7703     0.9411     0.0851"},{"path":"/articles/Definition-Variables-and-Multi-Group-SEM.html","id":"bibliography","dir":"Articles","previous_headings":"","what":"Bibliography","title":"Definition-Variables-and-Multi-Group-SEM","text":"Huang, P.-H. (2018). penalized likelihood method multi-group structural equation modelling. British Journal Mathematical Statistical Psychology, 71(3), 499–522. https://doi.org/10.1111/bmsp.12130 Huang, P.-H. (2020). lslx: Semi-confirmatory structural equation modeling via penalized likelihood. Journal Statistical Software, 93(7). https://doi.org/10.18637/jss.v093.i07","code":""},{"path":"/articles/Estimator-Settings.html","id":"maximum-likelihood-estimation","dir":"Articles","previous_headings":"","what":"Maximum Likelihood Estimation","title":"Estimator-Settings","text":"default, lavaan use maximum likelihood estimation listwise deletion missing data. pass “default” model lessSEM, procedures used well: can also use meanstructure = TRUE lessSEM automatically add meanstructure well.","code":"library(lessSEM)  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +       l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +       l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            std.lv = TRUE)  lsem <- lasso(   lavaanModel = lavaanModel,   regularized = paste0(\"l\", 6:15),   nLambdas = 50)"},{"path":"/articles/Estimator-Settings.html","id":"missing-data","dir":"Articles","previous_headings":"Maximum Likelihood Estimation","what":"Missing data","title":"Estimator-Settings","text":"Missing data common problem real data analysis. Different procedures developed address issue, full-information-maximum-likelihood among famous ones. lavaan, can use procedure setting missing = \"ml\":","code":"library(lessSEM)  dataset <- simulateExampleData(percentMissing = 20)  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +       l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +       l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            std.lv = TRUE,                             # note: we change the missing procedure                            missing = \"ml\")  lsem <- lasso(   lavaanModel = lavaanModel,   regularized = paste0(\"l\", 6:15),   nLambdas = 50)"},{"path":"/articles/Estimator-Settings.html","id":"weighted-least-squares","dir":"Articles","previous_headings":"","what":"Weighted Least Squares","title":"Estimator-Settings","text":"Warning WLS development fully supported moment. Weighted least squares estimation alternative maximum likelihood estimation prominent case non-normal data. , lavaan covers wide range different weighted least squares estimators can selected estimator = x option. estimators differ weight matrix. lessSEM extracts weights automatically lavaan. following weighted least squares variants supported: estimator = \"wls\", estimator = \"dwls\", estimator = \"gls\", estimator = \"uls\". , lessSEM try copy procedure used lavaan automatically: Changing estimator just requires replacing estimator = \"wls\" weighted least squares variants mentioned : Currently, procedure select final parameters supported lessSEM box cross-validation. AIC BIC supported.","code":"library(lessSEM)  # Note: WLS needs much larger sample sizes dataset <- simulateExampleData(N = 1000)  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +       l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +       l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            std.lv = TRUE,                            estimator = \"wls\")  lsem <- lasso(   lavaanModel = lavaanModel,   regularized = paste0(\"l\", 6:15),   nLambdas = 50) library(lessSEM)  # Note: WLS needs much larger sample sizes dataset <- simulateExampleData(N = 1000)  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +       l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +       l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            std.lv = TRUE,                            estimator = \"uls\")  lsem <- lasso(   lavaanModel = lavaanModel,   regularized = paste0(\"l\", 6:15),   nLambdas = 50)"},{"path":"/articles/Estimator-Settings.html","id":"ordered-data","dir":"Articles","previous_headings":"Weighted Least Squares","what":"Ordered data","title":"Estimator-Settings","text":"lavaan supports ordered data. yet implemented lessSEM. Check lslx implementation regularized SEM categorical data (Huang, 2020).","code":""},{"path":"/articles/Estimator-Settings.html","id":"bibliography","dir":"Articles","previous_headings":"","what":"Bibliography","title":"Estimator-Settings","text":"Huang, P.-H. (2020). Penalized Least Squares Structural Equation Modeling Ordinal Responses. Multivariate Behavioral Research, 1–19. https://doi.org/10.1080/00273171.2020.1820309 Rosseel, Y. (2012). lavaan: R package structural equation modeling. Journal Statistical Software, 48(2), 1–36. https://doi.org/10.18637/jss.v048.i02","code":""},{"path":"/articles/General-Purpose-Optimization.html","id":"the-example","dir":"Articles","previous_headings":"","what":"The example","title":"General-Purpose-Optimization","text":"Let’s start setting linear regression model. end, simulate data set:","code":"set.seed(123)  # first, we simulate data for our # linear regression. N <- 100 # number of persons p <- 10 # number of predictors X <- matrix(rnorm(N*p), nrow = N, ncol = p) # design matrix b <- c(rep(1,4),        rep(0,6)) # true regression weights y <- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)"},{"path":"/articles/General-Purpose-Optimization.html","id":"the-first-approach-interfacing-from-r","dir":"Articles","previous_headings":"","what":"The first approach: Interfacing from R","title":"General-Purpose-Optimization","text":"now try implement lasso regularized linear regression using gpLasso interface. interface similar optim. use , must define fitting function R: Additionally, need labeled vector starting values: Note defined one parameter variables X. also want estimate intercept. end, extend X: Finally, need decide parameters regularized values lambda. want regularize everything except intercept: Now, ready estimate model: Note specify gradients function. case, lessSEM use numDeriv compute gradients. However, know specify gradients, can result faster estimation: short comparison running models 5 times : Runtime seconds without gradients: Runtime seconds gradients: ’s quite speedup! Note can also pass C++ function gpLasso similar approach : Run model : runtime seconds C++ : even lower !","code":"# defining the sum-squared-errors: sseFun <- function(par, y, X, N){   # par is the parameter vector   # y is the observed dependent variable   # X is the design matrix   # N is the sample size   pred <- X %*% matrix(par, ncol = 1) #be explicit here:   # we need par to be a column vector   sse <- sum((y - pred)^2)   # we scale with .5/N to get the same results as glmnet   return((.5/N)*sse) } par <- rep(0, p+1) names(par) <- paste0(\"b\", 0:p) print(par) #>  b0  b1  b2  b3  b4  b5  b6  b7  b8  b9 b10  #>   0   0   0   0   0   0   0   0   0   0   0 Xext <- cbind(1,X) head(Xext) #>      [,1]        [,2]        [,3]       [,4]       [,5]        [,6]        [,7] #> [1,]    1 -0.56047565 -0.71040656  2.1988103 -0.7152422 -0.07355602 -0.60189285 #> [2,]    1 -0.23017749  0.25688371  1.3124130 -0.7526890 -1.16865142 -0.99369859 #> [3,]    1  1.55870831 -0.24669188 -0.2651451 -0.9385387 -0.63474826  1.02678506 #> [4,]    1  0.07050839 -0.34754260  0.5431941 -1.0525133 -0.02884155  0.75106130 #> [5,]    1  0.12928774 -0.95161857 -0.4143399 -0.4371595  0.67069597 -1.50916654 #> [6,]    1  1.71506499 -0.04502772 -0.4762469  0.3311792 -1.65054654 -0.09514745 #>             [,8]       [,9]      [,10]      [,11] #> [1,]  1.07401226 -0.7282191  0.3562833 -1.0141142 #> [2,] -0.02734697 -1.5404424 -0.6580102 -0.7913139 #> [3,] -0.03333034 -0.6930946  0.8552022  0.2995937 #> [4,] -1.51606762  0.1188494  1.1529362  1.6390519 #> [5,]  0.79038534 -1.3647095  0.2762746  1.0846170 #> [6,] -0.21073418  0.5899827  0.1441047 -0.6245675 (regularized <- paste0(\"b\", 1:p)) #>  [1] \"b1\"  \"b2\"  \"b3\"  \"b4\"  \"b5\"  \"b6\"  \"b7\"  \"b8\"  \"b9\"  \"b10\" lambdas  <- seq(0,.1,length.out = 20) library(lessSEM) l1 <- gpLasso(par = par,                regularized = regularized,                fn = sseFun,                lambdas = lambdas,                X = Xext,               y = y,               N = length(y) ) head(l1@parameters) #>        lambda alpha theta         b0        b1        b2        b3       b4 #> 1 0.000000000     1     0 0.02738463 1.0129197 0.9991454 0.9705726 1.027626 #> 2 0.005263158     1     0 0.02935332 1.0043734 0.9908935 0.9626257 1.025138 #> 3 0.010526316     1     0 0.02995012 0.9967094 0.9846670 0.9552794 1.021892 #> 4 0.015789474     1     0 0.03010662 0.9897328 0.9789426 0.9481493 1.018672 #> 5 0.021052632     1     0 0.03029718 0.9827287 0.9732055 0.9409870 1.015363 #> 6 0.026315789     1     0 0.03112519 0.9753355 0.9670621 0.9338615 1.011553 #>            b5           b6           b7          b8           b9         b10 #> 1 0.014036259 -0.007461001 0.0185899756 0.021930761 -0.009900029 0.027401040 #> 2 0.003365776  0.000000000 0.0143410112 0.015434958 -0.007939546 0.022297680 #> 3 0.000000000  0.000000000 0.0096219826 0.010707808 -0.005256709 0.017465071 #> 4 0.000000000  0.000000000 0.0049334068 0.006364151 -0.002393543 0.012713199 #> 5 0.000000000  0.000000000 0.0001769463 0.002036748  0.000000000 0.007969729 #> 6 0.000000000  0.000000000 0.0000000000 0.000000000  0.000000000 0.003304089 sseGrad <- function(par, y, X, N){      gradients = (-2.0*t(X) %*% y + 2.0*t(X)%*%X%*%matrix(par,ncol = 1))      gradients = (.5/length(y))*gradients   return(t(gradients)) } l1 <- gpLasso(par = par,                regularized = regularized,                fn = sseFun,                gr = sseGrad,               lambdas = lambdas,                X = Xext,               y = y,               N = length(y) ) head(l1@parameters) #>        lambda alpha theta         b0        b1        b2        b3       b4 #> 1 0.000000000     1     0 0.02738538 1.0129192 0.9991446 0.9705722 1.027627 #> 2 0.005263158     1     0 0.02935293 1.0043730 0.9908934 0.9626257 1.025138 #> 3 0.010526316     1     0 0.02995044 0.9967092 0.9846670 0.9552790 1.021892 #> 4 0.015789474     1     0 0.03010657 0.9897326 0.9789426 0.9481492 1.018672 #> 5 0.021052632     1     0 0.03029714 0.9827286 0.9732060 0.9409871 1.015363 #> 6 0.026315789     1     0 0.03112488 0.9753367 0.9670621 0.9338616 1.011553 #>            b5           b6           b7          b8           b9         b10 #> 1 0.014035495 -0.007458693 0.0185899698 0.021930012 -0.009901646 0.027400755 #> 2 0.003365787  0.000000000 0.0143411586 0.015434965 -0.007939552 0.022297718 #> 3 0.000000000  0.000000000 0.0096216331 0.010707813 -0.005256789 0.017465164 #> 4 0.000000000  0.000000000 0.0049331390 0.006364195 -0.002393728 0.012713246 #> 5 0.000000000  0.000000000 0.0001771743 0.002036275  0.000000000 0.007969899 #> 6 0.000000000  0.000000000 0.0000000000 0.000000000  0.000000000 0.003303889 #> [1] 0.2565546 0.2580526 0.2550647 0.2574337 0.2705841 #> [1] 0.02246594 0.02206922 0.02292442 0.02452898 0.02433062 library(RcppArmadillo) library(Rcpp) linreg <- ' // [[Rcpp::depends(RcppArmadillo)]] #include <RcppArmadillo.h>  // [[Rcpp::export]] double fitfunction(const arma::colvec parameters, const arma::mat X, const arma::colvec y, const int N){      // compute the sum of squared errors:     arma::mat sse = arma::trans(y-X*parameters)*(y-X*parameters);          // other packages, such as glmnet, scale the sse with      // 1/(2*N), where N is the sample size. We will do that here as well          sse *= 1.0/(2.0 * N);          // note: We must return a double, but the sse is a matrix     // To get a double, just return the single value that is in      // this matrix:       return(sse(0,0)); }  // [[Rcpp::export]] arma::rowvec gradientfunction(const arma::colvec parameters, const arma::mat X, const arma::colvec y, const int N){      // note: we want to return our gradients as row-vector; therefore,   // we have to transpose the resulting column-vector:     arma::rowvec gradients = arma::trans(-2.0*X.t() * y + 2.0*X.t()*X*parameters);          // other packages, such as glmnet, scale the sse with      // 1/(2*N), where N is the sample size. We will do that here as well          gradients *= (.5/N);          return(gradients); }'  Rcpp::sourceCpp(code = linreg) l1 <- gpLasso(par = par,                regularized = regularized,                fn = fitfunction,                gr = gradientfunction,               lambdas = lambdas,                X = Xext,               y = y,               N = length(y) ) head(l1@parameters) #>        lambda alpha theta         b0        b1        b2        b3       b4 #> 1 0.000000000     1     0 0.02738582 1.0129189 0.9991449 0.9705722 1.027626 #> 2 0.005263158     1     0 0.02935312 1.0043727 0.9908927 0.9626260 1.025139 #> 3 0.010526316     1     0 0.02995020 0.9967090 0.9846669 0.9552793 1.021892 #> 4 0.015789474     1     0 0.03010671 0.9897324 0.9789426 0.9481494 1.018672 #> 5 0.021052632     1     0 0.03029732 0.9827286 0.9732061 0.9409868 1.015363 #> 6 0.026315789     1     0 0.03112459 0.9753371 0.9670621 0.9338616 1.011553 #>            b5           b6           b7          b8           b9         b10 #> 1 0.014035802 -0.007459445 0.0185900832 0.021930312 -0.009899840 0.027400855 #> 2 0.003365641  0.000000000 0.0143415484 0.015434674 -0.007939592 0.022297256 #> 3 0.000000000  0.000000000 0.0096218807 0.010707664 -0.005256972 0.017464975 #> 4 0.000000000  0.000000000 0.0049329762 0.006364249 -0.002393580 0.012713275 #> 5 0.000000000  0.000000000 0.0001769284 0.002036489  0.000000000 0.007970293 #> 6 0.000000000  0.000000000 0.0000000000 0.000000000  0.000000000 0.003304421 #> [1] 0.01670718 0.01647496 0.01658535 0.01681352 0.01695132"},{"path":"/articles/General-Purpose-Optimization.html","id":"the-second-approach-using-c-function-pointers","dir":"Articles","previous_headings":"","what":"The second approach: Using C++ function pointers","title":"General-Purpose-Optimization","text":"using Rcpp functions defined quite fast linear regression, can still fairly slow involved models (e.g., SEM). due optimizer go back forth R C++. reduce overhead, can use second approach. , instead passing Rcpp function executed R, pass pointer underlying C++ functions. approach constrained one presented : must define , fitting function gradient function Rcpp. rely numDeriv ! fitting function gradient function allowed two parameters : const Rcpp::NumericVector& (parameters) Rcpp::List& (everything else). seems restrictive, note can virtually pass anything want list. must create pointers fit gradient function. difficult, however provide guidance . may bit overwhelming first, go step step.","code":""},{"path":"/articles/General-Purpose-Optimization.html","id":"creating-a-fitting-function-and-a-gradient-function","dir":"Articles","previous_headings":"The second approach: Using C++ function pointers","what":"1. Creating a fitting function and a gradient function","title":"General-Purpose-Optimization","text":"already defined fitting function gradient function linear regression model example . However, often know gradients closed form. don’t gradient function, can try numerical approximation. details can found .","code":""},{"path":"/articles/General-Purpose-Optimization.html","id":"adapting-the-functions-to-the-constraints","dir":"Articles","previous_headings":"The second approach: Using C++ function pointers","what":"2. Adapting the functions to the constraints","title":"General-Purpose-Optimization","text":"Note fitting function gradient function comply constraints mentioned . , take two parameters arguments (const arma::colvec parameters, const arma::mat X, const arma::colvec y, const int N), arguments const Rcpp::NumericVector& Rcpp::List&. can make work? parameter vector const Rcpp::NumericVector& hold elements arma::colvec pararameters old function. Rcpp::List& must contain elements (X,y,N). Let’s start creating list, call data: Next, change functions make things work: ’s , functions transformed!","code":"data <- list(\"X\" = Xext,              \"y\" = y,              \"N\" = length(y)) linreg <- ' // [[Rcpp::depends(RcppArmadillo)]] #include <RcppArmadillo.h>  // [[Rcpp::export]] double fitfunction(const Rcpp::NumericVector& parameters, Rcpp::List& data){   // our function now only takes the two specified arguments: a   // const Rcpp::NumericVector& and an Rcpp::List&.   // We have to extract all elements from the list:   arma::colvec y = Rcpp::as<arma::colvec>(data[\"y\"]); // the dependent variable   arma::mat X = Rcpp::as<arma::mat>(data[\"X\"]); // the design matrix   int N = Rcpp::as<int>(data[\"N\"]); // the sample size      // Next, we want to get the parameters as a column-vector:     arma::colvec b = Rcpp::as<arma::colvec>(parameters);        // compute the sum of squared errors:     arma::mat sse = arma::trans(y-X*b)*(y-X*b);          // other packages, such as glmnet, scale the sse with      // 1/(2*N), where N is the sample size. We will do that here as well          sse *= 1.0/(2.0 * N);          // note: We must return a double, but the sse is a matrix     // To get a double, just return the single value that is in      // this matrix:       return(sse(0,0)); }  // [[Rcpp::export]] arma::rowvec gradientfunction(const Rcpp::NumericVector& parameters, Rcpp::List& data){     // our function now only takes the two specified arguments: a   // const Rcpp::NumericVector& and an Rcpp::List&.   // We have to extract all elements from the list:   arma::colvec y = Rcpp::as<arma::colvec>(data[\"y\"]); // the dependent variable   arma::mat X = Rcpp::as<arma::mat>(data[\"X\"]); // the design matrix   int N = Rcpp::as<int>(data[\"N\"]); // the sample size      // Next, we want to get the parameters as a column-vector:     arma::colvec b = Rcpp::as<arma::colvec>(parameters);      // note: we want to return our gradients as row-vector; therefore,   // we have to transpose the resulting column-vector:     arma::rowvec gradients = arma::trans(-2.0*X.t() * y + 2.0*X.t()*X*b);          // other packages, such as glmnet, scale the sse with      // 1/(2*N), where N is the sample size. We will do that here as well          gradients *= (.5/N);          return(gradients); } '"},{"path":"/articles/General-Purpose-Optimization.html","id":"step-3-creating-pointers-to-our-functions","dir":"Articles","previous_headings":"The second approach: Using C++ function pointers","what":"Step 3: Creating pointers to our functions","title":"General-Purpose-Optimization","text":"get’s really tricky! can’t just pass functions C++. However, can create pointers. generated C++ can tricky get right. simplify process, created function helps setting things : Let’s follow instructions add lines C++ functions: Compile functions using Rcpp: Great! Now way, can create pointers functions:","code":"cat(lessSEM::makePtrs(fitFunName = \"fitfunction\", # name of the function in C++                       gradFunName = \"gradientfunction\" # name of the function in C++ ) ) #>  #> // INSTRUCTIONS: ADD THE FOLLOWING LINES TO YOUR C++ FUNCTIONS #>  #> // IF RCPPARMADILLO IS NOT IMPORTED YET, UNCOMMENT THE FOLLOWING TWO LINES #> // // [[Rcpp::depends(RcppArmadillo)]] #> // #include <RcppArmadillo.h> #>  #> // Dirk Eddelbuettel at #> // https://gallery.rcpp.org/articles/passing-cpp-function-pointers/ #>  #> typedef double (*fitFunPtr)(const Rcpp::NumericVector&, //parameters #>                 Rcpp::List& //additional elements #> ); #> typedef Rcpp::XPtr<fitFunPtr> fitFunPtr_t; #>  #> typedef arma::rowvec (*gradientFunPtr)(const Rcpp::NumericVector&, //parameters #>                       Rcpp::List& //additional elements #> ); #> typedef Rcpp::XPtr<gradientFunPtr> gradientFunPtr_t; #>  #> // [[Rcpp::export]] #> fitFunPtr_t fitfunctionPtr() { #>         return(fitFunPtr_t(new fitFunPtr(&fitfunction))); #> } #>  #> // [[Rcpp::export]] #> gradientFunPtr_t gradientfunctionPtr() { #>         return(gradientFunPtr_t(new gradientFunPtr(&gradientfunction))); #> } linreg <- ' // [[Rcpp::depends(RcppArmadillo)]] #include <RcppArmadillo.h>  // [[Rcpp::export]] double fitfunction(const Rcpp::NumericVector& parameters, Rcpp::List& data){   // our function now only takes the two specified arguments: a   // const Rcpp::NumericVector& and an Rcpp::List&.   // We have to extract all elements from the list:   arma::colvec y = Rcpp::as<arma::colvec>(data[\"y\"]); // the dependent variable   arma::mat X = Rcpp::as<arma::mat>(data[\"X\"]); // the design matrix   int N = Rcpp::as<int>(data[\"N\"]); // the sample size      // Next, we want to get the parameters as a column-vector:     arma::colvec b = Rcpp::as<arma::colvec>(parameters);        // compute the sum of squared errors:     arma::mat sse = arma::trans(y-X*b)*(y-X*b);          // other packages, such as glmnet, scale the sse with      // 1/(2*N), where N is the sample size. We will do that here as well          sse *= 1.0/(2.0 * N);          // note: We must return a double, but the sse is a matrix     // To get a double, just return the single value that is in      // this matrix:       return(sse(0,0)); }  // [[Rcpp::export]] arma::rowvec gradientfunction(const Rcpp::NumericVector& parameters, Rcpp::List& data){     // our function now only takes the two specified arguments: a   // const Rcpp::NumericVector& and an Rcpp::List&.   // We have to extract all elements from the list:   arma::colvec y = Rcpp::as<arma::colvec>(data[\"y\"]); // the dependent variable   arma::mat X = Rcpp::as<arma::mat>(data[\"X\"]); // the design matrix   int N = Rcpp::as<int>(data[\"N\"]); // the sample size      // Next, we want to get the parameters as a column-vector:     arma::colvec b = Rcpp::as<arma::colvec>(parameters);      // note: we want to return our gradients as row-vector; therefore,   // we have to transpose the resulting column-vector:     arma::rowvec gradients = arma::trans(-2.0*X.t() * y + 2.0*X.t()*X*b);          // other packages, such as glmnet, scale the sse with      // 1/(2*N), where N is the sample size. We will do that here as well          gradients *= (.5/N);          return(gradients); }  /// THE FOLLOWING PART IS NEW:  // INSTRUCTIONS: ADD THE FOLLOWING LINES TO YOUR C++ FUNCTIONS  // IF RCPPARMADILLO IS NOT IMPORTED YET, UNCOMMENT THE FOLLOWING TWO LINES // // [[Rcpp::depends(RcppArmadillo)]] // #include <RcppArmadillo.h>  // Dirk Eddelbuettel at // https://gallery.rcpp.org/articles/passing-cpp-function-pointers/ typedef double (*fitFunPtr)(const Rcpp::NumericVector&, //parameters                 Rcpp::List& //additional elements ); typedef Rcpp::XPtr<fitFunPtr> fitFunPtr_t;  typedef arma::rowvec (*gradientFunPtr)(const Rcpp::NumericVector&, //parameters                       Rcpp::List& //additional elements ); typedef Rcpp::XPtr<gradientFunPtr> gradientFunPtr_t;  // [[Rcpp::export]] fitFunPtr_t fitfunctionPtr() {         return(fitFunPtr_t(new fitFunPtr(&fitfunction))); }  // [[Rcpp::export]] gradientFunPtr_t gradientfunctionPtr() {         return(gradientFunPtr_t(new gradientFunPtr(&gradientfunction))); } ' Rcpp::sourceCpp(code = linreg) ffp <- fitfunctionPtr() # create the pointer to the fitting function # Note that the name of this function will depend on the name of your fitting function. # For instance, if your fitting function is called sse, then the pointer will be created  # with ffp <- ssePtr() gfp <- gradientfunctionPtr() # create the pointer to the gradient function # Note that the name of this function will depend on the name of your gradient function. # For instance, if your gradient function is called sseGradient, then the pointer will be created  # with gfp <- sseGradientPtr()"},{"path":"/articles/General-Purpose-Optimization.html","id":"optimizing-the-model","dir":"Articles","previous_headings":"The second approach: Using C++ function pointers","what":"Optimizing the model","title":"General-Purpose-Optimization","text":"last step call general purpose optimization. end, use gpLassoCpp function: Benchmarking approach results : , reduced runtime even !","code":"l1 <- gpLassoCpp(par = par,                   regularized = regularized,                   # important: pass the poinnters!                  fn = ffp,                   gr = gfp,                   lambdas = lambdas,                   # finally, pass the list which the fitting function and the                   # gradient function need:                  additionalArguments = data ) head(l1@parameters) #>        lambda alpha theta         b0        b1        b2        b3       b4 #> 1 0.000000000     1     0 0.02738583 1.0129200 0.9991446 0.9705725 1.027626 #> 2 0.005263158     1     0 0.02935279 1.0043739 0.9908927 0.9626259 1.025139 #> 3 0.010526316     1     0 0.02995027 0.9967093 0.9846669 0.9552792 1.021892 #> 4 0.015789474     1     0 0.03010682 0.9897334 0.9789425 0.9481493 1.018673 #> 5 0.021052632     1     0 0.03029725 0.9827287 0.9732059 0.9409868 1.015363 #> 6 0.026315789     1     0 0.03112477 0.9753367 0.9670621 0.9338617 1.011553 #>            b5           b6           b7          b8           b9         b10 #> 1 0.014034458 -0.007459281 0.0185900689 0.021931006 -0.009900374 0.027400852 #> 2 0.003365898  0.000000000 0.0143413737 0.015434574 -0.007939162 0.022297256 #> 3 0.000000000  0.000000000 0.0096218864 0.010707576 -0.005256811 0.017464871 #> 4 0.000000000  0.000000000 0.0049333060 0.006363934 -0.002393508 0.012713075 #> 5 0.000000000  0.000000000 0.0001771637 0.002036388  0.000000000 0.007969876 #> 6 0.000000000  0.000000000 0.0000000000 0.000000000  0.000000000 0.003303873 #> [1] 0.01383734 0.01342845 0.01955676 0.01282501 0.01307154"},{"path":"/articles/General-Purpose-Optimization.html","id":"the-third-and-fourth-approach-including-the-header-files","dir":"Articles","previous_headings":"","what":"The third and fourth approach: Including the header files","title":"General-Purpose-Optimization","text":"approach requires elaborate setup created whole package demonstrate . find information vignette -optimizer-interface lessLM package. just want optimizers don’t want depend lessSEM package, recommend copy lesstimate C++ library packages inst/include folder. come parameter estimates: run times even lower:","code":"#>              b0        b1        b2        b3       b4          b5           b6 #> [1,] 0.02734701 1.0129361 0.9991629 0.9705501 1.027728 0.013993181 -0.007491533 #> [2,] 0.02939675 1.0043635 0.9908681 0.9626493 1.025035 0.003400965  0.000000000 #> [3,] 0.02998680 0.9967117 0.9846504 0.9552960 1.021803 0.000000000  0.000000000 #> [4,] 0.03006777 0.9897315 0.9789595 0.9481301 1.018774 0.000000000  0.000000000 #> [5,] 0.03032345 0.9827556 0.9731799 0.9409662 1.015441 0.000000000  0.000000000 #> [6,] 0.03111085 0.9753325 0.9670615 0.9338506 1.011607 0.000000000  0.000000000 #>                b7          b8           b9         b10 #> [1,] 0.0186210155 0.021974963 -0.009975776 0.027466466 #> [2,] 0.0143089363 0.015388336 -0.007860992 0.022231196 #> [3,] 0.0095927088 0.010679371 -0.005183552 0.017406513 #> [4,] 0.0049636831 0.006397664 -0.002474334 0.012779758 #> [5,] 0.0001374354 0.002100692  0.000000000 0.008033475 #> [6,] 0.0000000000 0.000000000  0.000000000 0.003352431 #> [1] 0.002083540 0.001147270 0.001112223 0.001109362 0.001105547"},{"path":"/articles/Mixed-Penalties.html","id":"getting-started","dir":"Articles","previous_headings":"","what":"Getting Started","title":"Mixed Penalties","text":"following model, allow cross-loadings (c2-c4). want regularize , cross-loadings regression coefficients (r1 - r3) Next, add separate lasso penalties loadings regressions: Note can use pipe-operator add multiple penalties. don’t ; following also work: fit model, use fit- function: check parameter regularized penalty, can look penalty statement resulting object: can access best parameters according BIC : tuningParameterConfiguration refers rows lambda, theta, alpha matrices resulted best fit: case, best model cross-loadings, regressions remained unregularized: lambda cross-loadings large (1), lambda regressions 0 (regularization).","code":"model <- '    # latent variable definitions      ind60 =~ x1 + x2 + x3 + c2*y2 + c3*y3 + c4*y4      dem60 =~ y1 + y2 + y3 + y4      dem65 =~ y5 + y6 + y7 + c*y8    # regressions     dem60 ~ r1*ind60     dem65 ~ r2*ind60 + r3*dem60 '  lavaanModel <- sem(model,                    data = PoliticalDemocracy) mp <- lavaanModel |>   mixedPenalty() |>   addLasso(regularized = c(\"c2\", \"c3\", \"c4\"),             lambdas = seq(0,1,.1)) |>   addLasso(regularized = c(\"r1\", \"r2\", \"r3\"),             lambdas = seq(0,1,.2)) mp <- lavaanModel |>   mixedPenalty() |>   addLasso(regularized = c(\"c2\", \"c3\", \"c4\"),             lambdas = seq(0,1,.1)) |>   addScad(regularized = c(\"r1\", \"r2\", \"r3\"),            lambdas = seq(0,1,.2),           thetas = 3.7) fitMp <- fit(mp) fitMp@penalty #>    ind60=~x2    ind60=~x3           c2           c3           c4    dem60=~y2  #>       \"none\"       \"none\"      \"lasso\"      \"lasso\"      \"lasso\"       \"none\"  #>    dem60=~y3    dem60=~y4    dem65=~y6    dem65=~y7            c           r1  #>       \"none\"       \"none\"       \"none\"       \"none\"       \"none\"       \"scad\"  #>           r2           r3       x1~~x1       x2~~x2       x3~~x3       y2~~y2  #>       \"scad\"       \"scad\"       \"none\"       \"none\"       \"none\"       \"none\"  #>       y3~~y3       y4~~y4       y1~~y1       y5~~y5       y6~~y6       y7~~y7  #>       \"none\"       \"none\"       \"none\"       \"none\"       \"none\"       \"none\"  #>       y8~~y8 ind60~~ind60 dem60~~dem60 dem65~~dem65  #>       \"none\"       \"none\"       \"none\"       \"none\" coef(fitMp, criterion = \"BIC\") #>                                                                       #>                        Tuning ||--||  Estimates                       #>  ---------------------------- ||--|| ---------- ---------- ---------- #>  tuningParameterConfiguration ||--||  ind60=~x2  ind60=~x3         c2 #>  ============================ ||--|| ========== ========== ========== #>                       11.0000 ||--||     2.1817     1.8188          . #>                                                                               #>                                                                               #>  ---------- ---------- ---------- ---------- ---------- ---------- ---------- #>          c3         c4  dem60=~y2  dem60=~y3  dem60=~y4  dem65=~y6  dem65=~y7 #>  ========== ========== ========== ========== ========== ========== ========== #>           .          .     1.3540     1.0440     1.2995     1.2585     1.2825 #>                                                                               #>                                                                               #>  ---------- ---------- ---------- ---------- ---------- ---------- ---------- #>           c         r1         r2         r3     x1~~x1     x2~~x2     x3~~x3 #>  ========== ========== ========== ========== ========== ========== ========== #>      1.3098     1.4738     0.4533     0.8644     0.0818     0.1184     0.4673 #>                                                                               #>                                                                               #>  ---------- ---------- ---------- ---------- ---------- ---------- ---------- #>      y2~~y2     y3~~y3     y4~~y4     y1~~y1     y5~~y5     y6~~y6     y7~~y7 #>  ========== ========== ========== ========== ========== ========== ========== #>      6.4896     5.3399     2.8871     1.9419     2.3901     4.3428     3.5096 #>                                                    #>                                                    #>  ---------- ------------ ------------ ------------ #>      y8~~y8 ind60~~ind60 dem60~~dem60 dem65~~dem65 #>  ========== ============ ============ ============ #>      2.9403       0.4482       3.8717       0.1149 getTuningParameterConfiguration(regularizedSEMMixedPenalty = fitMp,                                  tuningParameterConfiguration = 11) #>                 parameter penalty lambda alpha #> ind60=~x2       ind60=~x2    none      0     0 #> ind60=~x3       ind60=~x3    none      0     0 #> c2                     c2   lasso      1     1 #> c3                     c3   lasso      1     1 #> c4                     c4   lasso      1     1 #> dem60=~y2       dem60=~y2    none      0     0 #> dem60=~y3       dem60=~y3    none      0     0 #> dem60=~y4       dem60=~y4    none      0     0 #> dem65=~y6       dem65=~y6    none      0     0 #> dem65=~y7       dem65=~y7    none      0     0 #> c                       c    none      0     0 #> r1                     r1    scad      0     0 #> r2                     r2    scad      0     0 #> r3                     r3    scad      0     0 #> x1~~x1             x1~~x1    none      0     0 #> x2~~x2             x2~~x2    none      0     0 #> x3~~x3             x3~~x3    none      0     0 #> y2~~y2             y2~~y2    none      0     0 #> y3~~y3             y3~~y3    none      0     0 #> y4~~y4             y4~~y4    none      0     0 #> y1~~y1             y1~~y1    none      0     0 #> y5~~y5             y5~~y5    none      0     0 #> y6~~y6             y6~~y6    none      0     0 #> y7~~y7             y7~~y7    none      0     0 #> y8~~y8             y8~~y8    none      0     0 #> ind60~~ind60 ind60~~ind60    none      0     0 #> dem60~~dem60 dem60~~dem60    none      0     0 #> dem65~~dem65 dem65~~dem65    none      0     0"},{"path":"/articles/Mixed-Penalties.html","id":"using-ista","dir":"Articles","previous_headings":"","what":"Using ista","title":"Mixed Penalties","text":"glmnet optimizer typically considerably faster ista. However, sometimes glmnet may run issues. case, can help switch ista: fit model, use fit- function: tuningParameterConfiguration refers rows lambda, theta, alpha matrices resulted best fit: short run-time comparison ista glmnet lasso-regularized model : Five repetitions using ista took 20.149 seconds, glmnet took 1.156 seconds. , can use glmnet model, recommend .","code":"mp <- lavaanModel |>   # Change the optimizer and the control object:   mixedPenalty(method = \"ista\",                control = controlIsta()) |>   addLasso(regularized = c(\"c2\", \"c3\", \"c4\"),                  lambdas = seq(0,1,.1)) |>   addLasso(regularized = c(\"r1\", \"r2\", \"r3\"),                  lambdas = seq(0,1,.2)) fitMp <- fit(mp) coef(fitMp, criterion = \"BIC\") #>                                                                       #>                        Tuning ||--||  Estimates                       #>  ---------------------------- ||--|| ---------- ---------- ---------- #>  tuningParameterConfiguration ||--||  ind60=~x2  ind60=~x3         c2 #>  ============================ ||--|| ========== ========== ========== #>                       11.0000 ||--||     2.1818     1.8188          . #>                                                                               #>                                                                               #>  ---------- ---------- ---------- ---------- ---------- ---------- ---------- #>          c3         c4  dem60=~y2  dem60=~y3  dem60=~y4  dem65=~y6  dem65=~y7 #>  ========== ========== ========== ========== ========== ========== ========== #>           .          .     1.3541     1.0441     1.2997     1.2586     1.2825 #>                                                                               #>                                                                               #>  ---------- ---------- ---------- ---------- ---------- ---------- ---------- #>           c         r1         r2         r3     x1~~x1     x2~~x2     x3~~x3 #>  ========== ========== ========== ========== ========== ========== ========== #>      1.3099     1.4740     0.4532     0.8643     0.0818     0.1184     0.4672 #>                                                                               #>                                                                               #>  ---------- ---------- ---------- ---------- ---------- ---------- ---------- #>      y2~~y2     y3~~y3     y4~~y4     y1~~y1     y5~~y5     y6~~y6     y7~~y7 #>  ========== ========== ========== ========== ========== ========== ========== #>      6.4892     5.3396     2.8861     1.9420     2.3904     4.3421     3.5090 #>                                                    #>                                                    #>  ---------- ------------ ------------ ------------ #>      y8~~y8 ind60~~ind60 dem60~~dem60 dem65~~dem65 #>  ========== ============ ============ ============ #>      2.9392       0.4482       3.8710       0.1159 getTuningParameterConfiguration(regularizedSEMMixedPenalty = fitMp,                                  tuningParameterConfiguration = 11) #>                 parameter penalty lambda theta alpha #> ind60=~x2       ind60=~x2    none      0     0     0 #> ind60=~x3       ind60=~x3    none      0     0     0 #> c2                     c2   lasso      1     0     1 #> c3                     c3   lasso      1     0     1 #> c4                     c4   lasso      1     0     1 #> dem60=~y2       dem60=~y2    none      0     0     0 #> dem60=~y3       dem60=~y3    none      0     0     0 #> dem60=~y4       dem60=~y4    none      0     0     0 #> dem65=~y6       dem65=~y6    none      0     0     0 #> dem65=~y7       dem65=~y7    none      0     0     0 #> c                       c    none      0     0     0 #> r1                     r1   lasso      0     0     1 #> r2                     r2   lasso      0     0     1 #> r3                     r3   lasso      0     0     1 #> x1~~x1             x1~~x1    none      0     0     0 #> x2~~x2             x2~~x2    none      0     0     0 #> x3~~x3             x3~~x3    none      0     0     0 #> y2~~y2             y2~~y2    none      0     0     0 #> y3~~y3             y3~~y3    none      0     0     0 #> y4~~y4             y4~~y4    none      0     0     0 #> y1~~y1             y1~~y1    none      0     0     0 #> y5~~y5             y5~~y5    none      0     0     0 #> y6~~y6             y6~~y6    none      0     0     0 #> y7~~y7             y7~~y7    none      0     0     0 #> y8~~y8             y8~~y8    none      0     0     0 #> ind60~~ind60 ind60~~ind60    none      0     0     0 #> dem60~~dem60 dem60~~dem60    none      0     0     0 #> dem65~~dem65 dem65~~dem65    none      0     0     0"},{"path":"/articles/Mixed-Penalties.html","id":"bibliography","dir":"Articles","previous_headings":"","what":"Bibliography","title":"Mixed Penalties","text":"Geminiani, E., Marra, G., & Moustaki, . (2021). Single- multiple-group penalized factor analysis: trust-region algorithm approach integrated automatic multiple tuning parameter selection. Psychometrika, 86(1), 65–95. https://doi.org/10.1007/s11336-021-09751-8","code":""},{"path":"/articles/Parameter-transformations.html","id":"motivation","dir":"Articles","previous_headings":"","what":"Motivation","title":"Parameter-transformations","text":"longitudinal SEM, important investigate parameters stay time (e.g., measurement invariance loadings). can difficult decide may require setting many different models manually. , regularization techniques can handy. instance, seminal political democracy example, model typically set follows (see ?lavaan::sem): Note loadings , b, c assumed stay time. , measurement invariance assumed! Relaxing assumption, define model follows: , loading estimated separately. results complex model. know model use? many procedures answer question (e.g., using modification indexes, setting separate models hand, etc.). following, show regularization used (see e.g., Belzak & Bauer, 2020; Jacobucci & Grimm, 2018).","code":"modelSyntax <- '    # latent variable definitions      ind60 =~ x1 + x2 + x3      dem60 =~ y1 + a*y2 + b*y3 + c*y4      dem65 =~ y5 + a*y6 + b*y7 + c*y8    # regressions     dem60 ~ ind60     dem65 ~ ind60 + dem60    # residual correlations     y1 ~~ y5     y2 ~~ y4      y3 ~~ y7     y4 ~~ y8     y6 ~~ y8 ' modelSyntax <- '    # latent variable definitions      ind60 =~ x1 + x2 + x3      dem60 =~ y1 + a1*y2 + b1*y3 + c1*y4      dem65 =~ y5 + a2*y6 + b2*y7 + c2*y8    # regressions     dem60 ~ ind60     dem65 ~ ind60 + dem60    # residual correlations     y1 ~~ y5     y2 ~~ y4      y3 ~~ y7     y4 ~~ y8     y6 ~~ y8 '"},{"path":"/articles/Parameter-transformations.html","id":"using-regularization","dir":"Articles","previous_headings":"","what":"Using Regularization","title":"Parameter-transformations","text":"First, note measurement invariance can rephrased \\(a_1-a_2 = 0\\), \\(b_1-b_2 = 0\\), \\(c_1-c_2 = 0\\). Thus, regularizing differences parameters may allow testing measurement invariance (e.g., Belzak & Bauer, 2020; Liang et al., 2018; Muthen & Asparouhov, 2013). fact, used Bayesian SEM test approximate measurement invariance (Liang et al., 2018; Muthen & Asparouhov, 2013). Similar procedures also developed Huang (2018) multi-group differences parameter estimates Fisher et al. (2022) vector autoregressive models. Furthermore, Jacobucci & Grimm (2018) proposed regularizing differences latent change score models test equivalence autoproportion parameters time using two-step procedure. end, implemented diff_lasso regsem (Jacobucci et al., 2019). diff_lasso available lessSEM. Instead, lessSEM provides flexible workaround: parameter transformations. make work, re-define parameters. Redefine: \\[ \\begin{align} a_2 &= a_1 + \\Delta a_2\\\\ b_2 &= b_1 + \\Delta b_2\\\\ c_2 &= c_1 + \\Delta c_2 \\end{align} \\] regularizing \\(\\Delta a_2\\), \\(\\Delta b_2\\), \\(\\Delta c_2\\) towards zero, can enforce measurement invariance time.","code":""},{"path":"/articles/Parameter-transformations.html","id":"setting-up-the-model","dir":"Articles","previous_headings":"","what":"Setting up the Model","title":"Parameter-transformations","text":"first start flexible model want test: Note model defined estimates parameters time-point specific. , measurement invariance assumed. Now, want redefine parameters outlined : \\[ \\begin{align} a_2 &= a_1 + \\Delta a_2\\\\ b_2 &= b_1 + \\Delta b_2\\\\ c_2 &= c_1 + \\Delta c_2 \\end{align} \\] lessSEM redefinitions called transformations can passed penalty functions (e.g., lasso) using modifyModel command. First, create definition transformations: Next, pass transformations variable penalty function: Let’s look parameter estimates: Note differences parameters get smaller larger \\(\\lambda\\) values. can also plot differences: check measurement invariance can assumed, can select best model using information criteria: Note differences zeroed – , model full measurement invariance fit best. can also access transformed parameters: Limitation: , take account variables may different scales; thorough use method scale data first.","code":"library(lavaan) modelSyntax <- '    # latent variable definitions      ind60 =~ x1 + x2 + x3      dem60 =~ y1 + a1*y2 + b1*y3 + c1*y4      dem65 =~ y5 + a2*y6 + b2*y7 + c2*y8    # regressions     dem60 ~ ind60     dem65 ~ ind60 + dem60    # residual correlations     y1 ~~ y5     y2 ~~ y4      y3 ~~ y7     y4 ~~ y8     y6 ~~ y8 '  lavaanFit <- sem(model = modelSyntax,                  data = PoliticalDemocracy) transformations <- \" // IMPORTANT: Our transformations always have to start with the follwing line: parameters: a1, a2, b1, b2, c1, c2, delta_a2, delta_b2, delta_c2  // In the line above, we defined the names of the parameters which we // want to use in our transformations. EACH AND EVERY PARAMETER USED IN // THE FOLLOWING MUST BE STATED ABOVE. The line must always start with // the keyword 'parameters' followed by a colon. The parameters must be // separated by commata. // Comments can be added by using double backslash as shown here.  // Now we can state our transformations:  a2 = a1 + delta_a2; // Note: Each declaration must end with a semi-colon! b2 = b1 + delta_b2; c2 = c1 + delta_c2; \" lassoFit <- lasso(lavaanModel = lavaanFit,                    regularized = c(\"delta_a2\", \"delta_b2\", \"delta_c2\"),# we want to regularize                    # the differences between the parameters                   nLambdas = 100,                   # Our model modification must make use of the modifyModel - function:                   modifyModel = modifyModel(transformations = transformations) ) coef(lassoFit)@estimates[seq(1,100,10),c(\"a1\", \"b1\", \"c1\", \"delta_a2\", \"delta_b2\", \"delta_c2\")] #>             a1       b1       c1     delta_a2    delta_b2 delta_c2 #>  [1,] 1.210993 1.167936 1.234011  0.000000000 0.000000000        0 #>  [2,] 1.211114 1.166390 1.234046  0.000000000 0.002702789        0 #>  [3,] 1.212584 1.150753 1.234650  0.000000000 0.030080792        0 #>  [4,] 1.214347 1.135698 1.235563  0.000000000 0.057049529        0 #>  [5,] 1.216419 1.121154 1.236776  0.000000000 0.083716111        0 #>  [6,] 1.218762 1.107040 1.238259  0.000000000 0.110187427        0 #>  [7,] 1.221414 1.093407 1.240046  0.000000000 0.136498978        0 #>  [8,] 1.226629 1.080063 1.242051 -0.003960392 0.162516933        0 #>  [9,] 1.246435 1.067714 1.244329 -0.032144145 0.186302860        0 #> [10,] 1.266524 1.055803 1.247136 -0.060258715 0.210141745        0 plot(lassoFit) coef(lassoFit, criterion = \"BIC\") #>                                                                                #>   Tuning         ||--||  Estimates                                             #>  ------- ------- ||--|| ---------- ---------- ---------- ---------- ---------- #>   lambda   alpha ||--||  ind60=~x2  ind60=~x3         a1         b1         c1 #>  ======= ======= ||--|| ========== ========== ========== ========== ========== #>   0.2216  1.0000 ||--||     2.1825     1.8189     1.2110     1.1679     1.2340 #>                                                                       #>                                                                       #>  ----------- ----------- ----------- ---------- ---------- ---------- #>  dem60~ind60 dem65~ind60 dem65~dem60     y1~~y5     y2~~y4     y3~~y7 #>  =========== =========== =========== ========== ========== ========== #>       1.4534      0.5935      0.8659     0.5552     1.5947     0.7807 #>                                                                               #>                                                                               #>  ---------- ---------- ---------- ---------- ---------- ---------- ---------- #>      y4~~y8     y6~~y8     x1~~x1     x2~~x2     x3~~x3     y1~~y1     y2~~y2 #>  ========== ========== ========== ========== ========== ========== ========== #>      0.6537     1.5350     0.0820     0.1177     0.4675     1.7929     7.3843 #>                                                                                 #>                                                                                 #>  ---------- ---------- ---------- ---------- ---------- ---------- ------------ #>      y3~~y3     y4~~y4     y5~~y5     y6~~y6     y7~~y7     y8~~y8 ind60~~ind60 #>  ========== ========== ========== ========== ========== ========== ============ #>      5.0175     3.4074     2.2857     4.8977     3.5510     3.4511       0.4480 #>                                                                               #>                                                             ||--||  Transform #>  ------------ ------------ ---------- ---------- ---------- ||--|| ---------- #>  dem60~~dem60 dem65~~dem65   delta_a2   delta_b2   delta_c2 ||--||         a2 #>  ============ ============ ========== ========== ========== ||--|| ========== #>        3.9408       0.2034          .          .          . ||--||     1.2110 #>                        #>                        #>  ---------- ---------- #>          b2         c2 #>  ========== ========== #>      1.1679     1.2340 head(lassoFit@transformations) #>      lambda alpha       a2       b2       c2 #> 1 0.2410976     1 1.210993 1.167936 1.234011 #> 2 0.2386623     1 1.210996 1.167933 1.234011 #> 3 0.2362270     1 1.210997 1.167935 1.234010 #> 4 0.2337916     1 1.210992 1.167933 1.234006 #> 5 0.2313563     1 1.210991 1.167933 1.234001 #> 6 0.2289210     1 1.210987 1.167932 1.234000"},{"path":"/articles/Parameter-transformations.html","id":"some-guidelines","dir":"Articles","previous_headings":"","what":"Some Guidelines","title":"Parameter-transformations","text":"using transformations, please make sure give parameters names compatible standard naming conventions R. default names lavaan (e.g., f=~y1 loadings) supported. , parameters used transformations given names lavaan syntax. example , used following syntax: Importantly, parameters used transformation (a1, b1, c1, a2, b2, c2) labeled lavaan syntax. counter example: syntax specifies model, use lavaan-specific naming convention parameters. a1, example, named dem60=~y2. names compatible current implementation transformations used lessSEM.","code":"modelSyntax <- '    # latent variable definitions      ind60 =~ x1 + x2 + x3      dem60 =~ y1 + a1*y2 + b1*y3 + c1*y4      dem65 =~ y5 + a2*y6 + b2*y7 + c2*y8    # regressions     dem60 ~ ind60     dem65 ~ ind60 + dem60    # residual correlations     y1 ~~ y5     y2 ~~ y4      y3 ~~ y7     y4 ~~ y8     y6 ~~ y8 ' modelSyntax <- '    # latent variable definitions      ind60 =~ x1 + x2 + x3      dem60 =~ y1 + y2 + y3 + y4      dem65 =~ y5 + y6 + y7 + y8    # regressions     dem60 ~ ind60     dem65 ~ ind60 + dem60    # residual correlations     y1 ~~ y5     y2 ~~ y4      y3 ~~ y7     y4 ~~ y8     y6 ~~ y8 '"},{"path":"/articles/Parameter-transformations.html","id":"further-examples","dir":"Articles","previous_headings":"","what":"Further Examples","title":"Parameter-transformations","text":"Another example transformations useful detecting non-stationarity autoregressive cross-lagged parameters (e.g., Liang et al., 2018). following, demonstrate autoregressive model. model defined : \\[ \\begin{align} \\eta_t &= a_t\\eta_{t-1} + \\zeta_t\\\\ \\begin{bmatrix} y_{1,t}\\\\ y_{2,t}\\\\ y_{3,t}\\\\ \\end{bmatrix} &= \\begin{bmatrix} l_1\\\\ l_2\\\\ l_3\\\\ \\end{bmatrix} \\eta_t + \\pmb\\varepsilon \\end{align} \\] often assumed autoregressive effect \\(a_t\\) constant time; , autoregressive effect used time points. strong assumption may want test . One way using procedure outlined , define \\(a_t = a_1 + \\Delta a_t\\) (see Jacobucci & Grimm, 2018 similar procedure latent change score models). case, autoregressive effect composed first autoregressive effect (\\(a_1\\)) difference parameters (\\(\\Delta a_t\\)). regularizing \\(\\Delta a_t\\), can enforce stationarity. drawback approach outlined first autoregressive effect treated differently rest: , \\(a_1\\) serve baseline \\(a_2\\) \\(a_5\\)? take slightly different approach basically identical fused lasso proposed Tibshirani et al. (2005). Let’s define autoregressive effect \\[a_t = a_{t-1} + \\Delta a_t\\] Note \\(\\Delta a_t\\) longer difference respect initial autoregressive effect \\(a_1\\) difference respect directly preceding time point. regularizing \\(\\Delta a_t\\), can now detect sudden changes parameter – e.g., due intervention. can also thought regime switching model, underlying model changes time (see also Ou et al., 2019 regime switching models). regularization procedure, want detect process changes. won’t go details set model , can find source file (e.g., GitHub). simulated data set 200 individuals measured 10 time points. autoregressive effect \\(a_t\\) changes \\(t=4\\) \\(.6\\) \\(.2\\). data looks follows: lavaan model defined follows: fit model using lavaan: Note constraints autoregressive effects implemented – effect (a1-a9) estimated separately. now define transformations follows: Finally, can fit model: Extracting best fitting model: true autoregressive effects given estimates result perfect, lessSEM correctly identified change autoregressive parameter.","code":"head(data) #>           y1_t1       y2_t1       y3_t1      y1_t2      y2_t2      y3_t2 #> [1,]  0.2627294 -0.07745288 -0.06014729  1.1052896 -0.5125258  0.1352253 #> [2,] -0.5795123  0.27989039 -1.76295329 -1.1646535  0.0607009 -0.7770722 #> [3,]  0.5746526  0.67203557  1.30772224  0.5620735  1.3873269  1.2415508 #> [4,]  1.6376973  1.30880439  1.35159277  0.4681794  1.5370870  0.6961462 #> [5,]  0.4995286 -0.23722728  0.61244148  0.3469646  0.3052115  0.2639443 #> [6,] -0.9766373  0.94700260 -0.59421930 -0.3197840  0.8221090 -0.6808501 #>           y1_t3       y2_t3      y3_t3       y1_t4      y2_t4      y3_t4 #> [1,]  0.6775861  0.48382200  0.5163607  0.06936298  1.6842420  0.5604284 #> [2,] -0.4571539 -0.30575453  0.7552239  0.06618071 -1.5803092  0.1672394 #> [3,]  0.8255529 -0.09010965  1.5877450  0.75756487  1.3579232 -0.1379793 #> [4,]  0.1872212 -1.57377418  1.1081811 -0.50832183  1.1968847 -1.6473363 #> [5,]  1.3768225  0.02394048  1.1573912  1.30350064  0.3049751 -0.0146624 #> [6,] -0.3434095 -0.90348981 -1.1760506 -1.72146444 -1.2828716 -0.4379788 #>           y1_t5      y2_t5      y3_t5      y1_t6      y2_t6       y3_t6 #> [1,] -1.6171372 -1.5100658 -2.1212448 -0.9014921 -0.9732405 -0.89209997 #> [2,]  0.1045129  0.3483139 -0.3669688 -0.6347357 -1.1095700  0.01821975 #> [3,] -1.1968405 -0.3623344 -0.8732056 -0.8290071 -0.3555817 -0.98268524 #> [4,]  0.4423361 -1.0372594 -0.8153181 -1.6003121 -2.1169045 -0.46541710 #> [5,]  1.3351806  0.1229075  0.8274846  0.4508487  0.1328104  0.65883759 #> [6,]  1.0261730  0.1686185  0.6083559 -0.7427341  0.4514472 -1.15435404 #>           y1_t7      y2_t7      y3_t7      y1_t8       y2_t8      y3_t8 #> [1,] -1.4697023 -0.9088437 -2.1031570  0.9915954  0.26021125  1.2988304 #> [2,]  0.3845160  1.0140172  1.8585956 -1.1280623  0.62646559 -0.8323908 #> [3,] -1.8270782 -0.2335533 -1.6441484 -0.2693417  0.09757043  0.5752644 #> [4,]  1.3226249  1.7154104  1.8113749  0.3459617  0.29614603  0.2463911 #> [5,] -0.3915346 -0.5549143 -0.2476716 -1.3659303  1.14721685 -0.1968243 #> [6,]  1.7453347  1.2488910  0.6649188 -0.6131110 -0.27533871  0.1895920 #>            y1_t9      y2_t9      y3_t9      y1_t10      y2_t10     y3_t10 #> [1,]  1.27416201  1.2010039  1.1426722  0.04668192 -0.24123204 -0.4421784 #> [2,] -0.41574193 -0.6890046 -1.4714503 -0.01788118  0.02058262  0.3191417 #> [3,]  0.37098052 -0.3486185 -0.4273464 -0.87844796 -1.36443913 -1.0240934 #> [4,]  0.55566216  1.2444869  0.1788493  0.81351163 -0.39685221  3.3330365 #> [5,] -1.58477183 -1.8546910  0.7432728  0.89776939  1.05624257  0.3635211 #> [6,]  0.02486185 -0.5151950 -0.2018260  1.13097234  0.74034559  2.5292256 #> eta2 ~ a1*eta1 #> eta3 ~ a2*eta2 #> eta4 ~ a3*eta3 #> eta5 ~ a4*eta4 #> eta6 ~ a5*eta5 #> eta7 ~ a6*eta6 #> eta8 ~ a7*eta7 #> eta9 ~ a8*eta8 #> eta10 ~ a9*eta9 #>  #>  #> eta1 ~~ eta1 #> eta2 ~~ v*eta2 #> eta3 ~~ v*eta3 #> eta4 ~~ v*eta4 #> eta5 ~~ v*eta5 #> eta6 ~~ v*eta6 #> eta7 ~~ v*eta7 #> eta8 ~~ v*eta8 #> eta9 ~~ v*eta9 #> eta10 ~~ v*eta10 #>  #> eta1 =~ 1*y1_t1 + l2*y2_t1 + l3*y3_t1 #> y1_t1 ~~ mvar1*y1_t1 #> y2_t1 ~~ mvar2*y2_t1 #> y3_t1 ~~ mvar3*y3_t1 #> eta2 =~ 1*y1_t2 + l2*y2_t2 + l3*y3_t2 #> y1_t2 ~~ mvar1*y1_t2 #> y2_t2 ~~ mvar2*y2_t2 #> y3_t2 ~~ mvar3*y3_t2 #> eta3 =~ 1*y1_t3 + l2*y2_t3 + l3*y3_t3 #> y1_t3 ~~ mvar1*y1_t3 #> y2_t3 ~~ mvar2*y2_t3 #> y3_t3 ~~ mvar3*y3_t3 #> eta4 =~ 1*y1_t4 + l2*y2_t4 + l3*y3_t4 #> y1_t4 ~~ mvar1*y1_t4 #> y2_t4 ~~ mvar2*y2_t4 #> y3_t4 ~~ mvar3*y3_t4 #> eta5 =~ 1*y1_t5 + l2*y2_t5 + l3*y3_t5 #> y1_t5 ~~ mvar1*y1_t5 #> y2_t5 ~~ mvar2*y2_t5 #> y3_t5 ~~ mvar3*y3_t5 #> eta6 =~ 1*y1_t6 + l2*y2_t6 + l3*y3_t6 #> y1_t6 ~~ mvar1*y1_t6 #> y2_t6 ~~ mvar2*y2_t6 #> y3_t6 ~~ mvar3*y3_t6 #> eta7 =~ 1*y1_t7 + l2*y2_t7 + l3*y3_t7 #> y1_t7 ~~ mvar1*y1_t7 #> y2_t7 ~~ mvar2*y2_t7 #> y3_t7 ~~ mvar3*y3_t7 #> eta8 =~ 1*y1_t8 + l2*y2_t8 + l3*y3_t8 #> y1_t8 ~~ mvar1*y1_t8 #> y2_t8 ~~ mvar2*y2_t8 #> y3_t8 ~~ mvar3*y3_t8 #> eta9 =~ 1*y1_t9 + l2*y2_t9 + l3*y3_t9 #> y1_t9 ~~ mvar1*y1_t9 #> y2_t9 ~~ mvar2*y2_t9 #> y3_t9 ~~ mvar3*y3_t9 #> eta10 =~ 1*y1_t10 + l2*y2_t10 + l3*y3_t10 #> y1_t10 ~~ mvar1*y1_t10 #> y2_t10 ~~ mvar2*y2_t10 #> y3_t10 ~~ mvar3*y3_t10 lavaanFit <- sem(model = lavaanSyntax,                   data = data,                  orthogonal.y = TRUE,                   orthogonal.x = TRUE,                  missing = \"ml\") coef(lavaanFit) #>         a1         a2         a3         a4         a5         a6         a7  #>      0.484      0.535      0.531      0.135      0.124      0.040      0.173  #>         a8         a9 eta1~~eta1          v          v          v          v  #>      0.296      0.185      1.108      0.804      0.804      0.804      0.804  #>          v          v          v          v          v         l2         l3  #>      0.804      0.804      0.804      0.804      0.804      0.567      0.672  #>      mvar1      mvar2      mvar3         l2         l3      mvar1      mvar2  #>      0.049      0.744      0.652      0.567      0.672      0.049      0.744  #>      mvar3         l2         l3      mvar1      mvar2      mvar3         l2  #>      0.652      0.567      0.672      0.049      0.744      0.652      0.567  #>         l3      mvar1      mvar2      mvar3         l2         l3      mvar1  #>      0.672      0.049      0.744      0.652      0.567      0.672      0.049  #>      mvar2      mvar3         l2         l3      mvar1      mvar2      mvar3  #>      0.744      0.652      0.567      0.672      0.049      0.744      0.652  #>         l2         l3      mvar1      mvar2      mvar3         l2         l3  #>      0.567      0.672      0.049      0.744      0.652      0.567      0.672  #>      mvar1      mvar2      mvar3         l2         l3      mvar1      mvar2  #>      0.049      0.744      0.652      0.567      0.672      0.049      0.744  #>      mvar3         l2         l3      mvar1      mvar2      mvar3    y1_t1~1  #>      0.652      0.567      0.672      0.049      0.744      0.652     -0.155  #>    y2_t1~1    y3_t1~1    y1_t2~1    y2_t2~1    y3_t2~1    y1_t3~1    y2_t3~1  #>     -0.088     -0.175     -0.157     -0.049     -0.085     -0.150     -0.030  #>    y3_t3~1    y1_t4~1    y2_t4~1    y3_t4~1    y1_t5~1    y2_t5~1    y3_t5~1  #>     -0.157     -0.095     -0.022     -0.080      0.037     -0.023      0.058  #>    y1_t6~1    y2_t6~1    y3_t6~1    y1_t7~1    y2_t7~1    y3_t7~1    y1_t8~1  #>      0.050      0.065     -0.002     -0.047      0.068     -0.038     -0.022  #>    y2_t8~1    y3_t8~1    y1_t9~1    y2_t9~1    y3_t9~1   y1_t10~1   y2_t10~1  #>     -0.028     -0.048     -0.077     -0.058     -0.041      0.013     -0.012  #>   y3_t10~1  #>     -0.004 #> parameters: a1, a2, a3, a4, a5, a6, a7, a8, a9, delta2, delta3, delta4, delta5, delta6, delta7, delta8, delta9 #>  #> a2 = a1 + delta2; #> a3 = a2 + delta3; #> a4 = a3 + delta4; #> a5 = a4 + delta5; #> a6 = a5 + delta6; #> a7 = a6 + delta7; #> a8 = a7 + delta8; #> a9 = a8 + delta9; lassoFit <- lasso(lavaanModel = lavaanFit,                    regularized = paste0(\"delta\", 2:9),# we want to regularize                    # the differences between the parameters                   nLambdas = 100,                   # glmnet is considerably faster here:                   method = \"glmnet\",                   control = controlGlmnet(),                   # Our model modification must make use of the modifyModel - function:                   modifyModel = modifyModel(transformations = transformations) ) coef(lassoFit, criterion = \"BIC\") #>                                                                                #>   Tuning         ||--||  Estimates                                             #>  ------- ------- ||--|| ---------- ---------- ---------- ---------- ---------- #>   lambda   alpha ||--||         a1 eta1~~eta1          v         l2         l3 #>  ======= ======= ||--|| ========== ========== ========== ========== ========== #>   0.2790  1.0000 ||--||     0.4665     1.1055     0.8051     0.5697     0.6754 #>                                                                               #>                                                                               #>  ---------- ---------- ---------- ---------- ---------- ---------- ---------- #>       mvar1      mvar2      mvar3    y1_t1~1    y2_t1~1    y3_t1~1    y1_t2~1 #>  ========== ========== ========== ========== ========== ========== ========== #>      0.0533     0.7428     0.6500    -0.1546    -0.0878    -0.1746    -0.1565 #>                                                                               #>                                                                               #>  ---------- ---------- ---------- ---------- ---------- ---------- ---------- #>     y2_t2~1    y3_t2~1    y1_t3~1    y2_t3~1    y3_t3~1    y1_t4~1    y2_t4~1 #>  ========== ========== ========== ========== ========== ========== ========== #>     -0.0493    -0.0849    -0.1499    -0.0302    -0.1572    -0.0952    -0.0224 #>                                                                               #>                                                                               #>  ---------- ---------- ---------- ---------- ---------- ---------- ---------- #>     y3_t4~1    y1_t5~1    y2_t5~1    y3_t5~1    y1_t6~1    y2_t6~1    y3_t6~1 #>  ========== ========== ========== ========== ========== ========== ========== #>     -0.0798     0.0373    -0.0231     0.0576     0.0502     0.0650    -0.0020 #>                                                                               #>                                                                               #>  ---------- ---------- ---------- ---------- ---------- ---------- ---------- #>     y1_t7~1    y2_t7~1    y3_t7~1    y1_t8~1    y2_t8~1    y3_t8~1    y1_t9~1 #>  ========== ========== ========== ========== ========== ========== ========== #>     -0.0466     0.0678    -0.0383    -0.0221    -0.0279    -0.0476    -0.0775 #>                                                                               #>                                                                               #>  ---------- ---------- ---------- ---------- ---------- ---------- ---------- #>     y2_t9~1    y3_t9~1   y1_t10~1   y2_t10~1   y3_t10~1     delta2     delta3 #>  ========== ========== ========== ========== ========== ========== ========== #>     -0.0579    -0.0407     0.0131    -0.0118    -0.0042          .          . #>                                                                           #>                                                                    ||--|| #>  ---------- ---------- ---------- ---------- ---------- ---------- ||--|| #>      delta4     delta5     delta6     delta7     delta8     delta9 ||--|| #>  ========== ========== ========== ========== ========== ========== ||--|| #>     -0.2806          .          .          .          .          . ||--|| #>                                                                               #>   Transform                                                                   #>  ---------- ---------- ---------- ---------- ---------- ---------- ---------- #>          a2         a3         a4         a5         a6         a7         a8 #>  ========== ========== ========== ========== ========== ========== ========== #>      0.4665     0.4665     0.1859     0.1859     0.1859     0.1859     0.1859 #>             #>             #>  ---------- #>          a9 #>  ========== #>      0.1859 #> [1] 0.6 0.6 0.6 0.2 0.2 0.2 0.2 0.2 0.2 #> [1] 0.4665219 0.4665219 0.4665219 0.1858729 0.1858729 0.1858729 0.1858729 #> [8] 0.1858729 0.1858729"},{"path":"/articles/Parameter-transformations.html","id":"looking-under-the-hood","dir":"Articles","previous_headings":"","what":"Looking under the hood","title":"Parameter-transformations","text":"transformations used implemented using RcppArmadillo. allows lot complicated transformations outlined . general, lessSEM take transformations try translate C++. Let’s assume model given first example: defined transformations : transformation passed lessSEM, lessSEM first try figure parameters already model ones new. case a1, a2, b1, b2, c1, c2 already known, delta_a2, delta_b2, delta_c2 new. lessSEM now add new parameters internal parameter vector. Next, lessSEM scan names parameters appear left hand side equation (a2, b2, c2) case. tell lessSEM parameters functions parameters (.e., transformations). Knowing a2 function parameters tell lessSEM, a2 longer estimated. Instead, parameters make a2 estimated: a1 delta_a2. see action, can create C++ function without compilation: First, let’s look extended parameter vector: Note delta_a2, delta_b2, delta_c2 added. transformations parameters: estimated computed based model parameters. Finally, C++ function returned: importantly, note first step extract required parameters parameter vector (e.g., double a1 = parameterValues[\"a1\"];). Next, parameters directly available use transformations. can simply write a2 = a1 + delta_a2;. Finally, transformed parameters returned. pass function lessSEM, also create pointer function, beyond scope . point may wondering complicated transformations promised . Importantly, can use functions implemented Rcpp RcppArmadillo can applied variables type double within transformations without -depth knowledge C++. instance, RcppArmadillo comes exponential-function (exp), pow log function. Making use , can implement univariate continuous time SEM (e.g., Voelkle & Oud, 2012). Far superior versions model implemented ctsem dynr) Arnold et al. (submission) recently derived close form solutions gradients models outperform lessSEM considerably terms runtime. use model , remove change autoregressive effect. code simulate data set can found source file (e.g., GitHub). initial model , however autoregressive effect constrained equality time manifest means. also added initial mean latent variable \\(\\eta\\) changed names variables make using ctsem data easier: Now, define transformations latent variables turn model continuous time SEM: Let’s look parameter estimates: comparison, run model ctsem: parameter ctA model corresponds DRIFT parameter ctsem summary parameter ctV corresponds root DIFFUSION parameter ctsem summary:","code":"modelSyntax <- '    # latent variable definitions      ind60 =~ x1 + x2 + x3      dem60 =~ y1 + a1*y2 + b1*y3 + c1*y4      dem65 =~ y5 + a2*y6 + b2*y7 + c2*y8    # regressions     dem60 ~ ind60     dem65 ~ ind60 + dem60    # residual correlations     y1 ~~ y5     y2 ~~ y4      y3 ~~ y7     y4 ~~ y8     y6 ~~ y8 '  lavaanFit <- sem(model = modelSyntax,                   data = PoliticalDemocracy) transformations <- \" parameters: a1, a2, b1, b2, c1, c2, delta_a2, delta_b2, delta_c2  a2 = a1 + delta_a2; b2 = b1 + delta_b2; c2 = c1 + delta_c2; \" transformationFunction <- lessSEM:::.compileTransformations(syntax = transformations,                                                              parameterLabels = names(getLavaanParameters(lavaanFit)),                                                             compile = FALSE) transformationFunction$parameters #> [1] \"a1\"       \"a2\"       \"b1\"       \"b2\"       \"c1\"       \"c2\"       \"delta_a2\" #> [8] \"delta_b2\" \"delta_c2\" transformationFunction$isTransformation #> [1] \"a2\" \"b2\" \"c2\" cat(transformationFunction$armaFunction) #>  #>   // [[Rcpp::depends(RcppArmadillo)]] #>   #include <RcppArmadillo.h> #>   // [[Rcpp::export]] #>   Rcpp::NumericVector transformationFunction(Rcpp::NumericVector& parameterValues, Rcpp::List transformationList) #>   { #>   using namespace Rcpp; #>   using namespace arma; #>    #>   // extract required parameters from parameterValues #>    #> double a1 = parameterValues[\"a1\"]; #> double a2 = parameterValues[\"a2\"]; #> double b1 = parameterValues[\"b1\"]; #> double b2 = parameterValues[\"b2\"]; #> double c1 = parameterValues[\"c1\"]; #> double c2 = parameterValues[\"c2\"]; #> double delta_a2 = parameterValues[\"delta_a2\"]; #> double delta_b2 = parameterValues[\"delta_b2\"]; #> double delta_c2 = parameterValues[\"delta_c2\"]; #>  #>  #> // add user defined functions #>  #>  #> a2 = a1 + delta_a2; #> b2 = b1 + delta_b2; #> c2 = c1 + delta_c2; #>  #>  #>  #> // update parameters #> parameterValues[\"a1\"] = a1; #> parameterValues[\"a2\"] = a2; #> parameterValues[\"b1\"] = b1; #> parameterValues[\"b2\"] = b2; #> parameterValues[\"c1\"] = c1; #> parameterValues[\"c2\"] = c2; #> parameterValues[\"delta_a2\"] = delta_a2; #> parameterValues[\"delta_b2\"] = delta_b2; #> parameterValues[\"delta_c2\"] = delta_c2; #>  #>    #>   return(parameterValues); #>   } #>  #>    #>    #>   // Dirk Eddelbuettel at #>   // https://gallery.rcpp.org/articles/passing-cpp-function-pointers/ #> typedef Rcpp::NumericVector (*transformationFunctionPtr)(Rcpp::NumericVector&, //parameters #> Rcpp::List // transformationList #> ); #>  #> typedef Rcpp::XPtr<transformationFunctionPtr> transformationFunctionPtr_t; #>  #> // [[Rcpp::export]] #> transformationFunctionPtr_t getPtr() { #>         return(transformationFunctionPtr_t(new transformationFunctionPtr(&transformationFunction))); #> } cat(lavaanSyntax) #> eta1 ~ a*eta0 #> eta2 ~ a*eta1 #> eta3 ~ a*eta2 #> eta4 ~ a*eta3 #> eta5 ~ a*eta4 #> eta6 ~ a*eta5 #> eta7 ~ a*eta6 #> eta8 ~ a*eta7 #> eta9 ~ a*eta8 #>  #>  #> eta0 ~~ eta0 #> eta1 ~~ v*eta1 #> eta2 ~~ v*eta2 #> eta3 ~~ v*eta3 #> eta4 ~~ v*eta4 #> eta5 ~~ v*eta5 #> eta6 ~~ v*eta6 #> eta7 ~~ v*eta7 #> eta8 ~~ v*eta8 #> eta9 ~~ v*eta9 #>  #>  #> eta0~1 #>  #> eta0 =~ 1*y1_T0 + l2*y2_T0 + l3*y3_T0 #> y1_T0 ~~ mvar1*y1_T0 #> y2_T0 ~~ mvar2*y2_T0 #> y3_T0 ~~ mvar3*y3_T0 #> y1_T0 ~ mMean1*1 #> y2_T0 ~ mMean2*1 #> y3_T0 ~ mMean3*1 #> eta1 =~ 1*y1_T1 + l2*y2_T1 + l3*y3_T1 #> y1_T1 ~~ mvar1*y1_T1 #> y2_T1 ~~ mvar2*y2_T1 #> y3_T1 ~~ mvar3*y3_T1 #> y1_T1 ~ mMean1*1 #> y2_T1 ~ mMean2*1 #> y3_T1 ~ mMean3*1 #> eta2 =~ 1*y1_T2 + l2*y2_T2 + l3*y3_T2 #> y1_T2 ~~ mvar1*y1_T2 #> y2_T2 ~~ mvar2*y2_T2 #> y3_T2 ~~ mvar3*y3_T2 #> y1_T2 ~ mMean1*1 #> y2_T2 ~ mMean2*1 #> y3_T2 ~ mMean3*1 #> eta3 =~ 1*y1_T3 + l2*y2_T3 + l3*y3_T3 #> y1_T3 ~~ mvar1*y1_T3 #> y2_T3 ~~ mvar2*y2_T3 #> y3_T3 ~~ mvar3*y3_T3 #> y1_T3 ~ mMean1*1 #> y2_T3 ~ mMean2*1 #> y3_T3 ~ mMean3*1 #> eta4 =~ 1*y1_T4 + l2*y2_T4 + l3*y3_T4 #> y1_T4 ~~ mvar1*y1_T4 #> y2_T4 ~~ mvar2*y2_T4 #> y3_T4 ~~ mvar3*y3_T4 #> y1_T4 ~ mMean1*1 #> y2_T4 ~ mMean2*1 #> y3_T4 ~ mMean3*1 #> eta5 =~ 1*y1_T5 + l2*y2_T5 + l3*y3_T5 #> y1_T5 ~~ mvar1*y1_T5 #> y2_T5 ~~ mvar2*y2_T5 #> y3_T5 ~~ mvar3*y3_T5 #> y1_T5 ~ mMean1*1 #> y2_T5 ~ mMean2*1 #> y3_T5 ~ mMean3*1 #> eta6 =~ 1*y1_T6 + l2*y2_T6 + l3*y3_T6 #> y1_T6 ~~ mvar1*y1_T6 #> y2_T6 ~~ mvar2*y2_T6 #> y3_T6 ~~ mvar3*y3_T6 #> y1_T6 ~ mMean1*1 #> y2_T6 ~ mMean2*1 #> y3_T6 ~ mMean3*1 #> eta7 =~ 1*y1_T7 + l2*y2_T7 + l3*y3_T7 #> y1_T7 ~~ mvar1*y1_T7 #> y2_T7 ~~ mvar2*y2_T7 #> y3_T7 ~~ mvar3*y3_T7 #> y1_T7 ~ mMean1*1 #> y2_T7 ~ mMean2*1 #> y3_T7 ~ mMean3*1 #> eta8 =~ 1*y1_T8 + l2*y2_T8 + l3*y3_T8 #> y1_T8 ~~ mvar1*y1_T8 #> y2_T8 ~~ mvar2*y2_T8 #> y3_T8 ~~ mvar3*y3_T8 #> y1_T8 ~ mMean1*1 #> y2_T8 ~ mMean2*1 #> y3_T8 ~ mMean3*1 #> eta9 =~ 1*y1_T9 + l2*y2_T9 + l3*y3_T9 #> y1_T9 ~~ mvar1*y1_T9 #> y2_T9 ~~ mvar2*y2_T9 #> y3_T9 ~~ mvar3*y3_T9 #> y1_T9 ~ mMean1*1 #> y2_T9 ~ mMean2*1 #> y3_T9 ~ mMean3*1 lavaanFit <- sem(model = lavaanSyntax,                   data = data,                  orthogonal.y = TRUE,                   orthogonal.x = TRUE,                  missing = \"ml\") getLavaanParameters(lavaanFit) #>            a   eta0~~eta0            v       eta0~1           l2           l3  #>  0.592770935  0.873870399  0.387065602 -0.091482670  0.578004929  0.745298413  #>        mvar1        mvar2        mvar3       mMean1       mMean2       mMean3  #>  0.127999058  0.750360430  0.577566347 -0.026015718  0.005858067 -0.027032832 transformations <- \" parameters: a, ctA, v, ctV // NOTE: We can define starting values for our parameters. This // is implemented with the 'start:' keyword: start: ctA = -.1, ctV = .1  // We changed the starting values for the ct parameters // because the auto-effect ctA should be negative.  a = exp(ctA); v = log((1.0/(2.0*ctA))*(exp(2.0*ctA)-1)*pow(ctV,2.0)); // we take // the log because lessSEM internally takes the exponential of // any variance parameter (v in our case) to avoid negative variances. \" lessSEMFit <- bfgs(lavaanModel = lavaanFit,                     # Our model modification must make use of the modifyModel - function:                    modifyModel = modifyModel(transformations = transformations) ) coef(lessSEMFit) #>                                                                                #>   Tuning         ||--||  Estimates                                             #>  ------- ------- ||--|| ---------- ---------- ---------- ---------- ---------- #>   lambda   alpha ||--|| eta0~~eta0     eta0~1         l2         l3      mvar1 #>  ======= ======= ||--|| ========== ========== ========== ========== ========== #>   0.0000  0.0000 ||--||     0.8736    -0.0902     0.5780     0.7453     0.1280 #>                                                                               #>                                                                               #>  ---------- ---------- ---------- ---------- ---------- ---------- ---------- #>       mvar2      mvar3     mMean1     mMean2     mMean3        ctA        ctV #>  ========== ========== ========== ========== ========== ========== ========== #>      0.7504     0.5775    -0.0267     0.0054    -0.0276    -0.5228     0.7899 #>                               #>  ||--||  Transform            #>  ||--|| ---------- ---------- #>  ||--||          a          v #>  ||--|| ========== ========== #>  ||--||     0.5928     0.3870 library(ctsemOMX) dataCt <- cbind(data,                 data.frame(\"dT1\" = rep(1,nrow(data)),                            \"dT2\" = rep(1,nrow(data)),                            \"dT3\" = rep(1,nrow(data)),                            \"dT4\" = rep(1,nrow(data)),                            \"dT5\" = rep(1,nrow(data)),                            \"dT6\" = rep(1,nrow(data)),                            \"dT7\" = rep(1,nrow(data)),                            \"dT8\" = rep(1,nrow(data)),                            \"dT9\" = rep(1,nrow(data)))) cModel <- ctModel(type = \"omx\",                    n.manifest = 3,                    n.latent = 1,                    Tpoints = 10,                   manifestNames = c(\"y1\",\"y2\", \"y3\"),                    latentNames = \"eta\",                   LAMBDA = matrix(c(1,                                     \"l2\",                                     \"l3\"),3,1,TRUE),                    DRIFT = matrix(\"a\",1,1) )  cFit <- ctFit(dat = dataCt, ctmodelobj = cModel) ctSummary <- summary(cFit) coef(lessSEMFit)@estimates[,c(\"ctA\", \"ctV\")] #>        ctA        ctV  #> -0.5228340  0.7899115  # drift value from ctsem: ctSummary$DRIFT #>            eta #> eta -0.5229469 # sqrt(diffusion) value from ctsem: sqrt(ctSummary$DIFFUSION) #>           eta #> eta 0.7900236"},{"path":"/articles/Parameter-transformations.html","id":"making-use-of-c","dir":"Articles","previous_headings":"Looking under the hood","what":"Making use of C++","title":"Parameter-transformations","text":"example , used univariate ctsem. , functions fairly simple needed log, exp, pow functions single variables. However, lessSEM creates C++ function, can build much powerful transformations familiar RcppArmadillo. following, therefore extend example multivariate continuous time SEM. use AnomAuth data set ctsem. data included ctsemOMX package: five measurement occasions unequally spaced. discrete time model, take care implementing model different autoregressive cross-lagged effects different time intervals: Setting model lavaan: transform parameters continuous time model, define transformations . go details . See Voelkle et al. (2012) underlying transformations. importantly, used matrices transformations. possible, lessSEM uses RcppArmadillo background therefore also provides users functions implemented therein. , can now fit model lessSEM comparison, also fit model ctsemOMX: following matrices drifts lessSEM model ctsemOMX model: lessSEM: ctsemOMX: diffusions given : lessSEM: ctsemOMX: Regularization used enforce sparsity (Orzek & Voelkle, review): , steps easier using dedicated packages ctsem ctsemOMX continuous time SEM regCtsem (Orzek & Voelkle, review) regularized continuous time SEM.","code":"data(\"AnomAuth\") head(AnomAuth) #>   Y1_T0 Y2_T0 Y1_T1 Y2_T1 Y1_T2 Y2_T2 Y1_T3 Y2_T3 Y1_T4 Y2_T4 dT1 dT2 dT3 dT4 #> 1  2.67  3.50  3.33   3.5    NA    NA    NA    NA    NA    NA   1   1   2   2 #> 2  3.33  3.25    NA    NA    NA    NA    NA    NA    NA    NA   1   1   2   2 #> 3  3.33  2.75  3.33   3.0  3.33   2.5  2.33     3  2.33     3   1   1   2   2 #> 4  3.33  3.25    NA    NA    NA    NA    NA    NA    NA    NA   1   1   2   2 #> 5  4.00  4.00    NA    NA    NA    NA    NA    NA    NA    NA   1   1   2   2 #> 6  3.67  4.00    NA    NA    NA    NA  4.00     4  4.00     4   1   1   2   2 # initial time point lavaanSyntax <-    \"eta1_T0 =~ 1 * Y1_T0 eta2_T0 =~ 1 * Y2_T0 Y1_T0 ~~ 0*Y1_T0 Y2_T0 ~~ 0*Y2_T0\\n\"  # variances lavaanSyntax <- c(lavaanSyntax,                   \"eta1_T0 ~~ v0_11 * eta1_T0 + v0_12 * eta2_T0\\neta2_T0 ~~ v0_22 * eta2_T0\\n\" )  # means lavaanSyntax <- c(lavaanSyntax,                   \"eta1_T0 ~ 1\\neta2_T0 ~ 1\\nY1_T0~mMean1*1\\nY2_T0~mMean2*1\\n\"  )  for(tp in c(0,1,2,3)){   if(tp < 2) {     a <- \"a1\"     v <- \"v1\"   }else{     a <-\"a2\"     v <- \"v2\"   }      # autoregressive and cross-lagged   lavaanSyntax <- c(lavaanSyntax,                     paste0(                       \"eta1_T\", tp+1, \" ~ \", a, \"_11 * eta1_T\", tp, \" + \", a, \"_12 * eta2_T\", tp,\"\\n\",                       \"eta2_T\", tp+1, \" ~ \", a, \"_21 * eta1_T\", tp, \" + \", a, \"_22 * eta2_T\", tp, \"\\n\"                     )   )      # variances   lavaanSyntax <- c(lavaanSyntax,                     paste0(                       \"eta1_T\", tp+1, \" ~~ \", v, \"_11 * eta1_T\", tp+1, \" + \", v, \"_12 * eta2_T\", tp+1,\"\\n\",                       \"eta2_T\", tp+1, \" ~~ \", v, \"_22 * eta2_T\", tp+1, \"\\n\"                     )   )      # loadings   lavaanSyntax <- c(lavaanSyntax,                     paste0(                       \"eta1_T\", tp+1, \" =~ 1 * Y1_T\", tp+1,\"\\n\",                       \"eta2_T\", tp+1, \" =~ 1 * Y2_T\", tp+1,\"\\n\"                     )   )      # manifest variances   lavaanSyntax <- c(lavaanSyntax,                     paste0(                       \"Y1_T\", tp+1, \" ~~ 0* Y1_T\", tp+1, \"\\n\",                       \"Y2_T\", tp+1, \" ~~ 0* Y2_T\", tp+1, \"\\n\"                     )   )      # manifest means   lavaanSyntax <- c(lavaanSyntax,                     paste0(                       \"Y1_T\", tp+1, \" ~ mMean1 * 1\\n\",                       \"Y2_T\", tp+1, \" ~ mMean2 * 1\\n\"                     )   ) } lavaanSyntax <- paste0(lavaanSyntax, collapse = \"\") cat(lavaanSyntax) #> eta1_T0 =~ 1 * Y1_T0 #> eta2_T0 =~ 1 * Y2_T0 #> Y1_T0 ~~ 0*Y1_T0 #> Y2_T0 ~~ 0*Y2_T0 #> eta1_T0 ~~ v0_11 * eta1_T0 + v0_12 * eta2_T0 #> eta2_T0 ~~ v0_22 * eta2_T0 #> eta1_T0 ~ 1 #> eta2_T0 ~ 1 #> Y1_T0~mMean1*1 #> Y2_T0~mMean2*1 #> eta1_T1 ~ a1_11 * eta1_T0 + a1_12 * eta2_T0 #> eta2_T1 ~ a1_21 * eta1_T0 + a1_22 * eta2_T0 #> eta1_T1 ~~ v1_11 * eta1_T1 + v1_12 * eta2_T1 #> eta2_T1 ~~ v1_22 * eta2_T1 #> eta1_T1 =~ 1 * Y1_T1 #> eta2_T1 =~ 1 * Y2_T1 #> Y1_T1 ~~ 0* Y1_T1 #> Y2_T1 ~~ 0* Y2_T1 #> Y1_T1 ~ mMean1 * 1 #> Y2_T1 ~ mMean2 * 1 #> eta1_T2 ~ a1_11 * eta1_T1 + a1_12 * eta2_T1 #> eta2_T2 ~ a1_21 * eta1_T1 + a1_22 * eta2_T1 #> eta1_T2 ~~ v1_11 * eta1_T2 + v1_12 * eta2_T2 #> eta2_T2 ~~ v1_22 * eta2_T2 #> eta1_T2 =~ 1 * Y1_T2 #> eta2_T2 =~ 1 * Y2_T2 #> Y1_T2 ~~ 0* Y1_T2 #> Y2_T2 ~~ 0* Y2_T2 #> Y1_T2 ~ mMean1 * 1 #> Y2_T2 ~ mMean2 * 1 #> eta1_T3 ~ a2_11 * eta1_T2 + a2_12 * eta2_T2 #> eta2_T3 ~ a2_21 * eta1_T2 + a2_22 * eta2_T2 #> eta1_T3 ~~ v2_11 * eta1_T3 + v2_12 * eta2_T3 #> eta2_T3 ~~ v2_22 * eta2_T3 #> eta1_T3 =~ 1 * Y1_T3 #> eta2_T3 =~ 1 * Y2_T3 #> Y1_T3 ~~ 0* Y1_T3 #> Y2_T3 ~~ 0* Y2_T3 #> Y1_T3 ~ mMean1 * 1 #> Y2_T3 ~ mMean2 * 1 #> eta1_T4 ~ a2_11 * eta1_T3 + a2_12 * eta2_T3 #> eta2_T4 ~ a2_21 * eta1_T3 + a2_22 * eta2_T3 #> eta1_T4 ~~ v2_11 * eta1_T4 + v2_12 * eta2_T4 #> eta2_T4 ~~ v2_22 * eta2_T4 #> eta1_T4 =~ 1 * Y1_T4 #> eta2_T4 =~ 1 * Y2_T4 #> Y1_T4 ~~ 0* Y1_T4 #> Y2_T4 ~~ 0* Y2_T4 #> Y1_T4 ~ mMean1 * 1 #> Y2_T4 ~ mMean2 * 1 lavaanFit <- sem(model = lavaanSyntax, data = AnomAuth,                  orthogonal.y = TRUE,                   orthogonal.x = TRUE,                  missing = \"ml\",                  do.fit = FALSE) transformations <- \" // Define all parameters which we want to use: parameters: a1_11, a1_12, a1_21, a1_22, a2_11, a2_12, a2_21, a2_22,  ctA_11, ctA_12, ctA_21, ctA_22,  v1_11, v1_12, v1_22, v2_11, v2_12, v2_22,  ctV_11, ctV_12, ctV_22  // Define the starting values for the continuous time parameters: start: ctA_11 = -1, ctA_12 = 0, ctA_21 = 0, ctA_22 = -1,  ctV_11 = .1, ctV_12 = 0, ctV_22 = .1  // transformations: arma::mat drift(2,2); arma::mat ARCL1(2,2); arma::mat ARCL2(2,2); arma::mat driftHash(4,4); drift(0,0) = ctA_11; drift(1,0) = ctA_21; drift(0,1) = ctA_12; drift(1,1) = ctA_22; ARCL1 = expmat(drift); ARCL2 = expmat(drift*2.0);  driftHash = kron(drift, arma::eye(2,2)) + kron(arma::eye(2,2), drift);  arma::mat diffusion(2,2); arma::mat discreteDiff1(2,2); arma::mat discreteDiff2(2,2); diffusion(0,0) = ctV_11; diffusion(1,0) = ctV_12; diffusion(0,1) = ctV_12; diffusion(1,1) = ctV_22; discreteDiff1 = arma::reshape(arma::inv(driftHash) *    (expmat(driftHash) - arma::eye(arma::size(expmat(driftHash))))*   arma::vectorise(diffusion),2,2); discreteDiff2 = arma::reshape(arma::inv(driftHash) *    (expmat(driftHash*2.0) - arma::eye(arma::size(expmat(driftHash*2.0))))*   arma::vectorise(diffusion),2,2);  // extract parameters  a1_11 = ARCL1(0,0); a1_12 = ARCL1(0,1); a1_21 = ARCL1(1,0); a1_22 = ARCL1(1,1);  a2_11 = ARCL2(0,0); a2_12 = ARCL2(0,1); a2_21 = ARCL2(1,0); a2_22 = ARCL2(1,1);  v1_11 = log(discreteDiff1(0,0)); // we take the log because of the internal  // transformation in lessSEM v1_12 = discreteDiff1(0,1); v1_22 = log(discreteDiff1(1,1)); // we take the log because of the internal  // transformation in lessSEM  v2_11 = log(discreteDiff2(0,0)); // we take the log because of the internal  // transformation in lessSEM v2_12 = discreteDiff2(0,1); v2_22 = log(discreteDiff2(1,1)); // we take the log because of the internal  // transformation in lessSEM \" lessSEMFit <- bfgs(lavaanModel = lavaanFit,                    # Our model modification must make use of the modifyModel - function:                    modifyModel = modifyModel(transformations = transformations) ) AnomAuthmodel <- ctModel(LAMBDA = matrix(c(1, 0, 0, 1), nrow = 2, ncol = 2),                           Tpoints = 5, n.latent = 2, n.manifest = 2, MANIFESTVAR=diag(0, 2), TRAITVAR = NULL)  AnomAuthfit <- ctFit(AnomAuth, AnomAuthmodel) matrix(coef(lessSEMFit)@estimates[,c(\"ctA_11\", \"ctA_21\", \"ctA_12\", \"ctA_22\")],2,2) #>             [,1]       [,2] #> [1,] -0.44814616  0.2320881 #> [2,]  0.04260196 -0.1171318 AnomAuthfit$mxobj$DRIFT$values #>             [,1]       [,2] #> [1,] -0.44728184  0.2324980 #> [2,]  0.04329283 -0.1174662 matrix(coef(lessSEMFit)@estimates[,c(\"ctV_11\", \"ctV_12\", \"ctV_12\", \"ctV_22\")],2,2) #>              [,1]         [,2] #> [1,]  0.473528285 -0.003850882 #> [2,] -0.003850882  0.154513444 AnomAuthfit$mxobj$DIFFUSIONchol$result%*%t(AnomAuthfit$mxobj$DIFFUSIONchol$result) #>              [,1]         [,2] #> [1,]  0.473241884 -0.004610149 #> [2,] -0.004610149  0.154509547 lassoFit <- lasso(lavaanModel = lavaanFit,                    regularized = \"ctA_21\",                   nLambdas = 30,                   method = \"glmnet\",                   control = controlGlmnet(),                   # Our model modification must make use of the modifyModel - function:                   modifyModel = modifyModel(transformations = transformations) ) plot(lassoFit)"},{"path":"/articles/Parameter-transformations.html","id":"how-it-is-implemented","dir":"Articles","previous_headings":"","what":"How it is implemented","title":"Parameter-transformations","text":"basic idea behind transformations follows: Assume SEM parameters given \\(\\pmb\\theta\\). 2-log-likelihood model given \\(f(\\pmb\\theta)\\). using transformations, redefine \\(\\pmb\\theta\\) function parameters, say \\(\\pmb\\gamma\\). , \\(\\pmb\\theta =\\pmb g(\\pmb\\gamma)\\), \\(\\pmb g\\) function returning vector. result, can re-write fitting function \\(f(\\pmb\\theta) = f(\\pmb g(\\pmb\\gamma))\\). Instead optimizing \\(\\pmb\\theta\\), now optimize \\(\\pmb\\gamma\\). Within lessSEM, gradients \\(f(\\pmb\\theta)\\) respect \\(\\pmb\\theta\\) implemented closed form results considerably faster run time. get gradients \\(f(\\pmb\\theta) = f(\\pmb g(\\pmb\\gamma))\\) respect \\(\\pmb\\gamma\\), lessSEM makes use chain rule. gradients transformation \\(\\pmb g(\\pmb\\gamma)\\) approximated numerically. similar procedure Arnold et al. (submission) continuous time SEM, derivative matrix exponential approximated numerically, elements derived closed form solutions.","code":""},{"path":"/articles/Parameter-transformations.html","id":"bibliography","dir":"Articles","previous_headings":"","what":"Bibliography","title":"Parameter-transformations","text":"Arnold, M., Cancér, P. F., Estrada, E., & Voelkle, M. C. (submission). Score-Guided Recursive Partitioning Continuous-Time Structural Equation Models. Belzak, W. C. M., & Bauer, D. J. (2020). Improving assessment measurement invariance: Using regularization select anchor items identify differential item functioning. Psychological Methods, 25(6), 673–690. https://doi.org/10.1037/met0000253 Driver, C. C., Oud, J. H. L., & Voelkle, M. C. (2017). Continuous time structural equation modelling R package ctsem. Journal Statistical Software, 77(5), 1–36. https://doi.org/10.18637/jss.v077.i05 Fisher, Z. F., Kim, Y., Fredrickson, B. L., & Pipiras, V. (2022). Penalized Estimation Forecasting Multiple Subject Intensive Longitudinal Data. Psychometrika, 87(2), 1–29. https://doi.org/10.1007/s11336-021-09825-7 Huang, P.-H. (2018). penalized likelihood method multi-group structural equation modelling. British Journal Mathematical Statistical Psychology, 71(3), 499–522. https://doi.org/10.1111/bmsp.12130 Hunter, M. D. (2018). State space modeling open source, modular, structural equation modeling environment. Structural Equation Modeling, 25(2), 307-324. doi: 10.1080/10705511.2017.1369354 Jacobucci, R., & Grimm, K. J. (2018). Regularized Estimation Multivariate Latent Change Score Models. E. Ferrer, S. M. Boker, & K. J. Grimm (Eds.), Longitudinal Multivariate Psychology (1st ed., pp. 109–125). Routledge. https://doi.org/10.4324/9781315160542-6 Jacobucci, R., Grimm, K. J., Brandmaier, . M., Serang, S., Kievit, R. ., & Scharf, F. (2019). regsem: Regularized structural equation modeling. https://CRAN.R-project.org/package=regsem Liang, X., Yang, Y., & Huang, J. (2018). Evaluation Structural Relationships Autoregressive Cross-Lagged Models Longitudinal Approximate Invariance:Bayesian Analysis. Structural Equation Modeling: Multidisciplinary Journal, 25(4), 558–572. https://doi.org/10.1080/10705511.2017.1410706 Muthen, B., & Asparouhov, T. (2013). BSEM Measurement Invariance Analysis. Mplus Web Notes: . 17. Neale, M. C., Hunter, M. D., Pritikin, J. N., Zahery, M., Brick, T. R., Kirkpatrick, R. M., Estabrook, R., Bates, T. C., Maes, H. H., & Boker, S. M. (2016). OpenMx 2.0: Extended structural equation statistical modeling. Psychometrika, 81(2), 535–549. https://doi.org/10.1007/s11336-014-9435-8 Ou, L., Hunter, M., D., & Chow, S.-M. (2019). Whats dynr: package linear nonlinear dynamic modeling r. R Journal, 11(1), 91–111. https://doi.org/10.32614/RJ-2019-012 Orzek, J. H., & Voelkle, M. C. (review). Regularized continuous time structural equation models: network perspective. Pritikin, J. N., Hunter, M. D., & Boker, S. M. (2015). Modular open-source software Item Factor Analysis. Educational Psychological Measurement, 75(3), 458-474 Tibshirani, R., Saunders, M., Rosset, S., Zhu, J., & Knight, K. (2005). Sparsity smoothness via fused lasso. Journal Royal Statistical Society: Series B (Statistical Methodology), 67(1), 91–108. https://doi.org/10.1111/j.1467-9868.2005.00490.x Voelkle, M. C., Oud, J. H. L., Davidov, E., & Schmidt, P. (2012). sem approach continuous time modeling panel data: Relating authoritarianism anomia. Psychological Methods, 17(2), 176–192. https://doi.org/10.1037/a0027543","code":""},{"path":"/articles/SCAD-and-MCP.html","id":"scad","dir":"Articles","previous_headings":"","what":"SCAD","title":"SCAD-and-MCP","text":"scad penalty given : \\[p(x) = \\begin{cases} \\lambda |x| & \\text{} |x| \\leq \\theta\\\\ \\frac{-x^2 + 2\\theta\\lambda |x| - \\lambda^2}{2(\\theta -1)} & \\text{} \\lambda < |x| < \\lambda\\theta\\\\ (\\theta + 1) \\lambda^2/2 & \\text{} |x| \\geq \\theta\\lambda\\\\ \\end{cases}\\] \\(\\theta > 2\\) \\(\\lambda \\geq 0\\). proximal operator searching solution function \\(\\hat x = \\arg\\min_x \\frac 12 (x-u)^2 + \\frac 1t p(x)\\). idea Gong et al. (2013) minimze function regions mentioned compare minima find global minimum.","code":""},{"path":"/articles/SCAD-and-MCP.html","id":"assume-x-leq-lambda","dir":"Articles","previous_headings":"SCAD","what":"Assume: \\(|x| \\leq \\lambda\\)","title":"SCAD-and-MCP","text":"Assuming solution region \\(|x| \\leq \\lambda\\), scad identical lasso. follows: \\[\\hat x = \\text{sign} (u)\\max(0,|u|-\\lambda / t)\\] also take border \\(|x| \\leq \\lambda\\) account, follows: \\(x \\geq 0\\): \\(\\hat x = \\min(\\lambda, \\text{sign} (u)\\max(0,|u|-\\lambda / t))\\) \\(x \\leq 0\\): \\(\\hat x = \\max(-\\lambda, \\text{sign} (u)\\max(0,|u|-\\lambda / t))\\) Combined: \\[\\hat x = \\text{sign}(u)\\max(\\lambda, \\max(0,|u|-\\lambda / t))\\]","code":""},{"path":"/articles/SCAD-and-MCP.html","id":"assume-lambda-x-leq-lambdatheta","dir":"Articles","previous_headings":"SCAD","what":"Assume: \\(\\lambda < |x| \\leq \\lambda\\theta\\)","title":"SCAD-and-MCP","text":"Assuming solution region \\(\\lambda < |x| \\leq \\lambda\\theta\\), critical section absolute value function (\\(x=0\\)) avoided. Therefore, derivative respect \\(x\\) defined can set zero. solution given : \\[ \\hat x = \\begin{cases} \\frac{u}{v} - \\frac{\\theta\\lambda}{t(\\theta-1)v} & \\text{} \\lambda < x <= \\lambda\\theta\\\\ \\frac{u}{v} + \\frac{\\theta\\lambda}{t(\\theta-1)v} & \\text{}  -\\theta > x > -\\lambda\\theta \\\\ \\end{cases} \\] \\(v = (1-\\frac{1}{t(\\theta-1)})\\). Also accounting borders gives: \\[ \\hat x = \\begin{cases} \\min(\\lambda\\theta, \\max(\\lambda, \\frac{u}{v} - \\frac{\\theta\\lambda}{t(\\theta-1)v}) & \\text{} x \\geq 0\\\\ \\max(-\\lambda\\theta, \\min(-\\lambda, \\frac{u}{v} + \\frac{\\theta\\lambda}{t(\\theta-1)v}) & \\text{} x \\leq 0\\\\ \\end{cases} \\] Derivation: penalty given \\(\\frac{-x^2 + 2\\theta\\lambda |x| - \\lambda^2}{2(\\theta -1)}\\). Differentiation respect \\(x\\) gives: \\[ \\begin{aligned} & \\frac{(-2x + 2\\theta\\lambda \\text{sign}(x))*2(\\theta -1)}{(2(\\theta -1))^2} \\\\ &= \\frac{-x + \\theta\\lambda \\text{sign}(x)}{(\\theta -1)} \\end{aligned} \\] (Note: indicated , \\(x \\neq 0\\) \\(\\lambda < |x| \\leq \\lambda\\theta\\). \\(\\partial |x| = \\text{sign}(x)*1\\).) Now combine differentiation penalty differentiation \\(\\frac 12 (x-u)^2\\) set 0: \\[x-u + \\frac 1t \\frac{-x + \\theta\\lambda \\text{sign}(x)}{(\\theta -1)} := 0\\] get equations solution.","code":""},{"path":"/articles/SCAD-and-MCP.html","id":"assume-x-geq-thetalambda","dir":"Articles","previous_headings":"SCAD","what":"Assume: \\(|x| \\geq \\theta\\lambda\\)","title":"SCAD-and-MCP","text":"\\(x \\neq 0\\), differentiation respect \\(x\\) defined. penalty given \\((\\theta + 1) \\lambda^2/2\\). Differentiating respect \\(x\\) gives \\(0\\) solution. follows: \\[x-u + \\frac 1t 0 := 0 \\Rightarrow \\hat x = u\\] Respecting borders: \\[\\hat x = \\text{sign}(u) \\min(\\theta\\lambda, |u|)\\]","code":""},{"path":"/articles/SCAD-and-MCP.html","id":"combining-the-solutions","dir":"Articles","previous_headings":"SCAD","what":"combining the solutions","title":"SCAD-and-MCP","text":"now minima section penalty function. find global minimum, compute \\(\\frac 12 (x-u)^2 + \\frac 1t p(x)\\) proposed solution select one results smallest value.","code":""},{"path":"/articles/SCAD-and-MCP.html","id":"mcp","dir":"Articles","previous_headings":"","what":"MCP","title":"SCAD-and-MCP","text":"MCP defined \\[ p(x) = \\begin{cases} \\lambda |x| - x^2/(2\\theta) & \\text{} |x| \\leq \\theta\\lambda\\\\ \\theta\\lambda^2/2 & \\text{} |x| > \\lambda\\theta \\end{cases}; \\theta > 0 \\]","code":""},{"path":[]},{"path":"/articles/SCAD-and-MCP.html","id":"assume-that-the-solution-is-given-by-x-geq-0-","dir":"Articles","previous_headings":"MCP > Assume: \\(|x| \\leq \\theta\\lambda\\)","what":"Assume that the solution is given by \\(x \\geq 0\\).","title":"SCAD-and-MCP","text":"\\(\\frac{\\partial}{\\partial x}p(x) = \\lambda - \\frac x\\theta\\). follows minimum \\(f(x) = \\frac 12 (x-u)^2 + \\frac 1t p(x)\\) given \\[ \\begin{aligned} x-u + \\frac 1t (\\lambda - \\frac x\\theta) &:= 0\\\\ \\Rightarrow x = \\frac u v - \\frac{1}{tv}\\lambda \\end{aligned} \\] \\(v = 1-\\frac{1}{\\theta t}\\) Respecting borders: \\[x = \\max(0,\\min(\\frac u v - \\frac{1}{tv}\\lambda, \\theta\\lambda))\\]","code":""},{"path":"/articles/SCAD-and-MCP.html","id":"assume-that-the-solution-is-given-by-x-leq-0-","dir":"Articles","previous_headings":"MCP > Assume: \\(|x| \\leq \\theta\\lambda\\)","what":"Assume that the solution is given by \\(x \\leq 0\\).","title":"SCAD-and-MCP","text":"\\(\\frac{\\partial}{\\partial x}p(x) = -\\lambda - \\frac x\\theta\\). follows minimum \\(f(x) = \\frac 12 (x-u)^2 + \\frac 1t p(x)\\) given \\[ \\begin{aligned} x-u + \\frac 1t (-\\lambda - \\frac x\\theta) &:= 0\\\\ \\Rightarrow x = \\frac u v + \\frac{1}{tv}\\lambda \\end{aligned} \\] \\(v = 1-\\frac{1}{\\theta t}\\) Respecting borders: \\[x = \\min(0,\\max(\\frac u v + \\frac{1}{tv}\\lambda, -\\theta\\lambda))\\]","code":""},{"path":"/articles/SCAD-and-MCP.html","id":"assume-x-geq-thetalambda-1","dir":"Articles","previous_headings":"MCP","what":"Assume \\(|x| \\geq \\theta\\lambda\\)","title":"SCAD-and-MCP","text":"case, differentiation respect \\(x\\) well defined. get \\(\\frac{\\partial}{\\partial x}p(x) = 0\\) \\(x = u\\) minimum function. Respecting borders: \\[x = \\text{sign}(u)\\max(\\theta\\lambda, |u|)\\] Finally, going compare proposed minima select one actually minimizes function.","code":""},{"path":"/articles/SCAD-and-MCP.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"SCAD-and-MCP","text":"Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). general iterative shrinkage thresholding algorithm non-convex regularized optimization problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/articles/The-Structural-Equation-Model.html","id":"from-lavaan-to-lesssem","dir":"Articles","previous_headings":"","what":"From lavaan to lessSEM","title":"The-Structural-Equation-Model","text":"translate model lavaan lessSEM, use lessSEM:::.SEMFromLavaan function. Importantly, function exported lessSEM. , must use three colons shown access function! lessSEM:::.SEMFromLavaan function comes additional arguments fine tune initialization model. whichPars: whichPars arguments, can change parameters used mySEM created . default, use estimates (whichPars = \"est\") lavaan model, also use starting values (whichPars = \"start\") supply custom parameter values fit: fit = TRUE, lessSEM fit model compare fitting function value lavaanModel. supplied parameters “est”, set fit = FALSE addMeans: mean structure added? currenlty recomended set TRUE activeSet: allows using part data set. can useful cross-validation. dataSet: allows passing different data set mySEM. can useful cross-validation. cases, recommend setting model shown , none additional arguments used.","code":"library(lessSEM)  # won't work: mySEM <- .SEMFromLavaan(lavaanModel = lavaanModel)  # will work: mySEM <- lessSEM:::.SEMFromLavaan(lavaanModel = lavaanModel) show(mySEM) #> Internal C++ model representation of lessSEM #> Parameters: #>    ind60=~x2    ind60=~x3            a            b            c  dem60~ind60  #>    2.1796566    1.8182100    1.1907820    1.1745407    1.2509789    1.4713302  #>  dem65~ind60  dem65~dem60       y1~~y5       y2~~y4       y2~~y6       y3~~y7  #>    0.6004746    0.8650430    0.5825389    1.4402477    2.1829448    0.7115901  #>       y4~~y8       y6~~y8       x1~~x1       x2~~x2       x3~~x3       y1~~y1  #>    0.3627964    1.3717741    0.0813878    0.1204271    0.4666596    1.8546417  #>       y2~~y2       y3~~y3       y4~~y4       y5~~y5       y6~~y6       y7~~y7  #>    7.5813926    4.9556766    3.2245521    2.3130404    4.9681408    3.5600367  #>       y8~~y8 ind60~~ind60 dem60~~dem60 dem65~~dem65         x1~1         x2~1  #>    3.3076854    0.4485989    3.8753039    0.1644633    5.0543838    4.7921946  #>         x3~1         y1~1         y2~1         y3~1         y4~1         y5~1  #>    3.5576898    5.4646667    4.2564429    6.5631103    4.4525330    5.1362519  #>         y6~1         y7~1         y8~1  #>    2.9780741    6.1962639    4.0433897  #>  #> Objective value: 3097.6361581071"},{"path":"/articles/The-Structural-Equation-Model.html","id":"working-with-the-rcpp_semcpp-class","dir":"Articles","previous_headings":"","what":"Working with the Rcpp_SEMCpp class","title":"The-Structural-Equation-Model","text":"mySEM object implemented C++ make everything run faster. underlying class Rcpp_SEMCpp created using wonderful Rcpp RcppArmadillo packages. can access elements using dollar-operator: Note , identical regsem, model implemented RAM notation (McArdle & McDonald, 1984). familiar notation, Fox (2006) provides short introduction. However, won’t need know details time . Instead, focus get set parameters, fit model, get gradients, etc.","code":"class(mySEM) #> [1] \"Rcpp_SEMCpp\" #> attr(,\"package\") #> [1] \"lessSEM\" mySEM$A #>            [,1]     [,2]     [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] #>  [1,] 0.0000000 0.000000 0.000000    0    0    0    0    0    0     0     0 #>  [2,] 1.4713302 0.000000 0.000000    0    0    0    0    0    0     0     0 #>  [3,] 0.6004746 0.865043 0.000000    0    0    0    0    0    0     0     0 #>  [4,] 1.0000000 0.000000 0.000000    0    0    0    0    0    0     0     0 #>  [5,] 2.1796566 0.000000 0.000000    0    0    0    0    0    0     0     0 #>  [6,] 1.8182100 0.000000 0.000000    0    0    0    0    0    0     0     0 #>  [7,] 0.0000000 1.000000 0.000000    0    0    0    0    0    0     0     0 #>  [8,] 0.0000000 1.190782 0.000000    0    0    0    0    0    0     0     0 #>  [9,] 0.0000000 1.174541 0.000000    0    0    0    0    0    0     0     0 #> [10,] 0.0000000 1.250979 0.000000    0    0    0    0    0    0     0     0 #> [11,] 0.0000000 0.000000 1.000000    0    0    0    0    0    0     0     0 #> [12,] 0.0000000 0.000000 1.190782    0    0    0    0    0    0     0     0 #> [13,] 0.0000000 0.000000 1.174541    0    0    0    0    0    0     0     0 #> [14,] 0.0000000 0.000000 1.250979    0    0    0    0    0    0     0     0 #>       [,12] [,13] [,14] #>  [1,]     0     0     0 #>  [2,]     0     0     0 #>  [3,]     0     0     0 #>  [4,]     0     0     0 #>  [5,]     0     0     0 #>  [6,]     0     0     0 #>  [7,]     0     0     0 #>  [8,]     0     0     0 #>  [9,]     0     0     0 #> [10,]     0     0     0 #> [11,]     0     0     0 #> [12,]     0     0     0 #> [13,]     0     0     0 #> [14,]     0     0     0"},{"path":"/articles/The-Structural-Equation-Model.html","id":"accessing-the-parameters","dir":"Articles","previous_headings":"Working with the Rcpp_SEMCpp class","what":"Accessing the Parameters","title":"The-Structural-Equation-Model","text":"parameters model can accessed lessSEM:::.getParameters function: naming identical lavaanModel. default, parameters returned transformed format. requires explanation: lessSEM assume negative variances outside parameter space. , negative variances allowed (different lavaan!). ensure variances positive, use transformation: Say interested variance ind60~~ind60. Internally, parameter called x1~~x1 parameter rawValue transformed value (called just value). can access values : parameters variances, rawValue identical value. variances, rawValue can real value. value computed \\(e^{\\text{rawValue}}\\); ensures value always positive. can access raw values follows: Note raw value ind60~~ind60 negative transformed value positive.","code":"(myParameters <- lessSEM:::.getParameters(mySEM)) #>    ind60=~x2    ind60=~x3            a            b            c  dem60~ind60  #>    2.1796566    1.8182100    1.1907820    1.1745407    1.2509789    1.4713302  #>  dem65~ind60  dem65~dem60       y1~~y5       y2~~y4       y2~~y6       y3~~y7  #>    0.6004746    0.8650430    0.5825389    1.4402477    2.1829448    0.7115901  #>       y4~~y8       y6~~y8       x1~~x1       x2~~x2       x3~~x3       y1~~y1  #>    0.3627964    1.3717741    0.0813878    0.1204271    0.4666596    1.8546417  #>       y2~~y2       y3~~y3       y4~~y4       y5~~y5       y6~~y6       y7~~y7  #>    7.5813926    4.9556766    3.2245521    2.3130404    4.9681408    3.5600367  #>       y8~~y8 ind60~~ind60 dem60~~dem60 dem65~~dem65         x1~1         x2~1  #>    3.3076854    0.4485989    3.8753039    0.1644633    5.0543838    4.7921946  #>         x3~1         y1~1         y2~1         y3~1         y4~1         y5~1  #>    3.5576898    5.4646667    4.2564429    6.5631103    4.4525330    5.1362519  #>         y6~1         y7~1         y8~1  #>    2.9780741    6.1962639    4.0433897 mySEM$getParameters() #>           label     value   rawValue location isTransformation #> 1     ind60=~x2 2.1796566  2.1796566  Amatrix            FALSE #> 2     ind60=~x3 1.8182100  1.8182100  Amatrix            FALSE #> 3             a 1.1907820  1.1907820  Amatrix            FALSE #> 4             b 1.1745407  1.1745407  Amatrix            FALSE #> 5             c 1.2509789  1.2509789  Amatrix            FALSE #> 6   dem60~ind60 1.4713302  1.4713302  Amatrix            FALSE #> 7   dem65~ind60 0.6004746  0.6004746  Amatrix            FALSE #> 8   dem65~dem60 0.8650430  0.8650430  Amatrix            FALSE #> 9        y1~~y5 0.5825389  0.5825389  Smatrix            FALSE #> 10       y2~~y4 1.4402477  1.4402477  Smatrix            FALSE #> 11       y2~~y6 2.1829448  2.1829448  Smatrix            FALSE #> 12       y3~~y7 0.7115901  0.7115901  Smatrix            FALSE #> 13       y4~~y8 0.3627964  0.3627964  Smatrix            FALSE #> 14       y6~~y8 1.3717741  1.3717741  Smatrix            FALSE #> 15       x1~~x1 0.0813878 -2.5085299  Smatrix            FALSE #> 16       x2~~x2 0.1204271 -2.1167106  Smatrix            FALSE #> 17       x3~~x3 0.4666596 -0.7621551  Smatrix            FALSE #> 18       y1~~y1 1.8546417  0.6176915  Smatrix            FALSE #> 19       y2~~y2 7.5813926  2.0256969  Smatrix            FALSE #> 20       y3~~y3 4.9556766  1.6005337  Smatrix            FALSE #> 21       y4~~y4 3.2245521  1.1707941  Smatrix            FALSE #> 22       y5~~y5 2.3130404  0.8385629  Smatrix            FALSE #> 23       y6~~y6 4.9681408  1.6030457  Smatrix            FALSE #> 24       y7~~y7 3.5600367  1.2697708  Smatrix            FALSE #> 25       y8~~y8 3.3076854  1.1962487  Smatrix            FALSE #> 26 ind60~~ind60 0.4485989 -0.8016262  Smatrix            FALSE #> 27 dem60~~dem60 3.8753039  1.3546241  Smatrix            FALSE #> 28 dem65~~dem65 0.1644633 -1.8050678  Smatrix            FALSE #> 29         x1~1 5.0543838  5.0543838  Mvector            FALSE #> 30         x2~1 4.7921946  4.7921946  Mvector            FALSE #> 31         x3~1 3.5576898  3.5576898  Mvector            FALSE #> 32         y1~1 5.4646667  5.4646667  Mvector            FALSE #> 33         y2~1 4.2564429  4.2564429  Mvector            FALSE #> 34         y3~1 6.5631103  6.5631103  Mvector            FALSE #> 35         y4~1 4.4525330  4.4525330  Mvector            FALSE #> 36         y5~1 5.1362519  5.1362519  Mvector            FALSE #> 37         y6~1 2.9780741  2.9780741  Mvector            FALSE #> 38         y7~1 6.1962639  6.1962639  Mvector            FALSE #> 39         y8~1 4.0433897  4.0433897  Mvector            FALSE lessSEM:::.getParameters(mySEM, raw = TRUE) #>    ind60=~x2    ind60=~x3            a            b            c  dem60~ind60  #>    2.1796566    1.8182100    1.1907820    1.1745407    1.2509789    1.4713302  #>  dem65~ind60  dem65~dem60       y1~~y5       y2~~y4       y2~~y6       y3~~y7  #>    0.6004746    0.8650430    0.5825389    1.4402477    2.1829448    0.7115901  #>       y4~~y8       y6~~y8       x1~~x1       x2~~x2       x3~~x3       y1~~y1  #>    0.3627964    1.3717741   -2.5085299   -2.1167106   -0.7621551    0.6176915  #>       y2~~y2       y3~~y3       y4~~y4       y5~~y5       y6~~y6       y7~~y7  #>    2.0256969    1.6005337    1.1707941    0.8385629    1.6030457    1.2697708  #>       y8~~y8 ind60~~ind60 dem60~~dem60 dem65~~dem65         x1~1         x2~1  #>    1.1962487   -0.8016262    1.3546241   -1.8050678    5.0543838    4.7921946  #>         x3~1         y1~1         y2~1         y3~1         y4~1         y5~1  #>    3.5576898    5.4646667    4.2564429    6.5631103    4.4525330    5.1362519  #>         y6~1         y7~1         y8~1  #>    2.9780741    6.1962639    4.0433897"},{"path":"/articles/The-Structural-Equation-Model.html","id":"changing-the-parameters","dir":"Articles","previous_headings":"Working with the Rcpp_SEMCpp class","what":"Changing the Parameters","title":"The-Structural-Equation-Model","text":"able change parameters essential fitting model. lessSEM, facilitated lessSEM:::.setParameters function: Note specify parameters myParameters given raw format. , already used transformed parameters, set raw = FALSE. Using raw parameters instead look follows: Let’s check parameters: Note now value 1.","code":"# first, let's change one of the parameters: myParameters[\"a\"] <- 1  # now, let's change the parameters of the model mySEM <- lessSEM:::.setParameters(SEM = mySEM, # the model                                   labels = names(myParameters), # names of the parameters                                   values = myParameters, # values of the parameters                                    raw = FALSE) myParameters <- lessSEM:::.getParameters(mySEM, raw = TRUE) # first, let's change one of the parameters: myParameters[\"a\"] <- 1  # now, let's change the parameters of the model mySEM <- lessSEM:::.setParameters(SEM = mySEM, # the model                                   labels = names(myParameters), # names of the parameters                                   values = myParameters, # values of the parameters                                   raw = TRUE) lessSEM:::.getParameters(mySEM) #>    ind60=~x2    ind60=~x3            a            b            c  dem60~ind60  #>    2.1796566    1.8182100    1.0000000    1.1745407    1.2509789    1.4713302  #>  dem65~ind60  dem65~dem60       y1~~y5       y2~~y4       y2~~y6       y3~~y7  #>    0.6004746    0.8650430    0.5825389    1.4402477    2.1829448    0.7115901  #>       y4~~y8       y6~~y8       x1~~x1       x2~~x2       x3~~x3       y1~~y1  #>    0.3627964    1.3717741    0.0813878    0.1204271    0.4666596    1.8546417  #>       y2~~y2       y3~~y3       y4~~y4       y5~~y5       y6~~y6       y7~~y7  #>    7.5813926    4.9556766    3.2245521    2.3130404    4.9681408    3.5600367  #>       y8~~y8 ind60~~ind60 dem60~~dem60 dem65~~dem65         x1~1         x2~1  #>    3.3076854    0.4485989    3.8753039    0.1644633    5.0543838    4.7921946  #>         x3~1         y1~1         y2~1         y3~1         y4~1         y5~1  #>    3.5576898    5.4646667    4.2564429    6.5631103    4.4525330    5.1362519  #>         y6~1         y7~1         y8~1  #>    2.9780741    6.1962639    4.0433897"},{"path":"/articles/The-Structural-Equation-Model.html","id":"fitting-the-model","dir":"Articles","previous_headings":"","what":"Fitting the model","title":"The-Structural-Equation-Model","text":"compute -2-log-likelihood model, use lessSEM:::.fit function: -2-log-likelihood can accessed :","code":"mySEM <- lessSEM:::.fit(SEM = mySEM) mySEM$objectiveValue #> [1] 3100.741"},{"path":"/articles/The-Structural-Equation-Model.html","id":"computing-the-gradients","dir":"Articles","previous_headings":"","what":"Computing the gradients","title":"The-Structural-Equation-Model","text":"compute gradients, use lessSEM:::.getGradients function. Gradients can computed transformed parameters raw parameters","code":"lessSEM:::.getGradients(mySEM, raw = FALSE) #>     ind60=~x2     ind60=~x3             a             b             c  #>   0.361097622   0.105564095 -32.837359814   2.453232158  17.076222049  #>   dem60~ind60   dem65~ind60   dem65~dem60        y1~~y5        y2~~y4  #>  -0.450648533  -1.078272542  -5.357920036   0.004650747  -0.157923486  #>        y2~~y6        y3~~y7        y4~~y8        y6~~y8        x1~~x1  #>  -0.567076177   0.161163293   0.266869495  -0.271533827   0.230054158  #>        x2~~x2        x3~~x3        y1~~y1        y2~~y2        y3~~y3  #>   0.306685898  -0.093753843  -0.015516535  -0.343353554   0.099359383  #>        y4~~y4        y5~~y5        y6~~y6        y7~~y7        y8~~y8  #>   0.137753880   0.131454473  -0.330083693   0.073331567   0.148964628  #>  ind60~~ind60  dem60~~dem60  dem65~~dem65          x1~1          x2~1  #>  -0.103960291  -0.252921392  -1.955349241   0.000000000   0.000000000  #>          x3~1          y1~1          y2~1          y3~1          y4~1  #>   0.000000000   0.000000000   0.000000000   0.000000000   0.000000000  #>          y5~1          y6~1          y7~1          y8~1  #>   0.000000000   0.000000000   0.000000000   0.000000000 lessSEM:::.getGradients(mySEM, raw = TRUE) #>     ind60=~x2     ind60=~x3             a             b             c  #>   0.361097622   0.105564095 -32.837359814   2.453232158  17.076222049  #>   dem60~ind60   dem65~ind60   dem65~dem60        y1~~y5        y2~~y4  #>  -0.450648533  -1.078272542  -5.357920036   0.004650747  -0.157923486  #>        y2~~y6        y3~~y7        y4~~y8        y6~~y8        x1~~x1  #>  -0.567076177   0.161163293   0.266869495  -0.271533827   0.018723602  #>        x2~~x2        x3~~x3        y1~~y1        y2~~y2        y3~~y3  #>   0.036933297  -0.043751134  -0.028777612  -2.603098086   0.492392971  #>        y4~~y4        y5~~y5        y6~~y6        y7~~y7        y8~~y8  #>   0.444194569   0.304059508  -1.639902248   0.261063066   0.492728130  #>  ind60~~ind60  dem60~~dem60  dem65~~dem65          x1~1          x2~1  #>  -0.046636468  -0.980147244  -0.321583201   0.000000000   0.000000000  #>          x3~1          y1~1          y2~1          y3~1          y4~1  #>   0.000000000   0.000000000   0.000000000   0.000000000   0.000000000  #>          y5~1          y6~1          y7~1          y8~1  #>   0.000000000   0.000000000   0.000000000   0.000000000"},{"path":"/articles/The-Structural-Equation-Model.html","id":"computing-the-hessian","dir":"Articles","previous_headings":"","what":"Computing the Hessian","title":"The-Structural-Equation-Model","text":"compute Hessian, use lessSEM:::.getHessian function. Hessian can computed transformed parameters raw parameters","code":"lessSEM:::.getHessian(mySEM, raw = FALSE) lessSEM:::.getHessian(mySEM, raw = TRUE)"},{"path":"/articles/The-Structural-Equation-Model.html","id":"computing-the-scores","dir":"Articles","previous_headings":"","what":"Computing the Scores","title":"The-Structural-Equation-Model","text":"compute scores (derivative -2-log-likelihood person), use lessSEM:::.getScores function. scores can computed transformed parameters raw parameters","code":"lessSEM:::.getScores(mySEM, raw = FALSE) lessSEM:::.getScores(mySEM, raw = TRUE)"},{"path":"/articles/The-Structural-Equation-Model.html","id":"using-lesssem-with-general-purpose-optimizers","dir":"Articles","previous_headings":"","what":"Using lessSEM with general purpose optimizers","title":"The-Structural-Equation-Model","text":"important part whole SEM implementation mentioned can use flexibly different optimizers. instance, may want try BFGS optimizer optim. Important: highly recommend use raw parameters optimization. Using non-raw parameters can cause errors unnecessary headaches! Let’s look optim function: Note function requires par argument - parameter estimates - fn argument - fitting function - also allows gradients passed function using gr argument. build functions based lessSEM:::.fit lessSEM:::.getGradients functions shown , however convenience wrappers already implemented lessSEM. fitting function called lessSEM:::.fitFunction gradient function called lessSEM:::.gradientFunction. expect vector parameters, SEM, argument specifying parameters raw format. can use optim follows: Note parameter now back maximum likelihood estimate . However, parameters still raw format. get transformed parameters, let’s take one step: Compare parameter estimates lavaan: Finally, can compute standard errors: Let’s compare lavaan :","code":"args(optim) #> function (par, fn, gr = NULL, ..., method = c(\"Nelder-Mead\",  #>     \"BFGS\", \"CG\", \"L-BFGS-B\", \"SANN\", \"Brent\"), lower = -Inf,  #>     upper = Inf, control = list(), hessian = FALSE)  #> NULL # let's get the starting values: par <- lessSEM:::.getParameters(mySEM, raw = TRUE) # important: Use raw = TRUE!  print(par) #>    ind60=~x2    ind60=~x3            a            b            c  dem60~ind60  #>    2.1796566    1.8182100    1.0000000    1.1745407    1.2509789    1.4713302  #>  dem65~ind60  dem65~dem60       y1~~y5       y2~~y4       y2~~y6       y3~~y7  #>    0.6004746    0.8650430    0.5825389    1.4402477    2.1829448    0.7115901  #>       y4~~y8       y6~~y8       x1~~x1       x2~~x2       x3~~x3       y1~~y1  #>    0.3627964    1.3717741   -2.5085299   -2.1167106   -0.7621551    0.6176915  #>       y2~~y2       y3~~y3       y4~~y4       y5~~y5       y6~~y6       y7~~y7  #>    2.0256969    1.6005337    1.1707941    0.8385629    1.6030457    1.2697708  #>       y8~~y8 ind60~~ind60 dem60~~dem60 dem65~~dem65         x1~1         x2~1  #>    1.1962487   -0.8016262    1.3546241   -1.8050678    5.0543838    4.7921946  #>         x3~1         y1~1         y2~1         y3~1         y4~1         y5~1  #>    3.5576898    5.4646667    4.2564429    6.5631103    4.4525330    5.1362519  #>         y6~1         y7~1         y8~1  #>    2.9780741    6.1962639    4.0433897  opt <- optim(par = par,               fn = lessSEM:::.fitFunction, # use the fitting function wrapper              gr = lessSEM:::.gradientFunction, # use the gradient function wrapper              SEM = mySEM, # use the SEM we created above              raw = TRUE, # make sure to tell the functions that we are using raw parameters              method = \"BFGS\" # use the BFGS optimizer ) print(opt$par) #>    ind60=~x2    ind60=~x3            a            b            c  dem60~ind60  #>    2.1791276    1.8180458    1.1909397    1.1740909    1.2511328    1.4725867  #>  dem65~ind60  dem65~dem60       y1~~y5       y2~~y4       y2~~y6       y3~~y7  #>    0.6007137    0.8649836    0.5817910    1.4336940    2.1828828    0.7229781  #>       y4~~y8       y6~~y8       x1~~x1       x2~~x2       x3~~x3       y1~~y1  #>    0.3605874    1.3774602   -2.5093044   -2.1126718   -0.7633056    0.6178333  #>       y2~~y2       y3~~y3       y4~~y4       y5~~y5       y6~~y6       y7~~y7  #>    2.0246995    1.6022529    1.1690474    0.8385775    1.6039528    1.2711405  #>       y8~~y8 ind60~~ind60 dem60~~dem60 dem65~~dem65         x1~1         x2~1  #>    1.1971146   -0.8013687    1.3542390   -1.8057174    5.0543838    4.7921946  #>         x3~1         y1~1         y2~1         y3~1         y4~1         y5~1  #>    3.5576898    5.4646667    4.2564429    6.5631103    4.4525330    5.1362519  #>         y6~1         y7~1         y8~1  #>    2.9780741    6.1962639    4.0433897 mySEM <- lessSEM:::.setParameters(SEM = mySEM, # the model                                   labels = names(opt$par), # names of the parameters                                   values = opt$par, # values of the parameters                                   raw = TRUE) print(lessSEM:::.getParameters(mySEM, raw = FALSE)) #>    ind60=~x2    ind60=~x3            a            b            c  dem60~ind60  #>   2.17912764   1.81804575   1.19093968   1.17409087   1.25113276   1.47258673  #>  dem65~ind60  dem65~dem60       y1~~y5       y2~~y4       y2~~y6       y3~~y7  #>   0.60071368   0.86498364   0.58179103   1.43369401   2.18288278   0.72297808  #>       y4~~y8       y6~~y8       x1~~x1       x2~~x2       x3~~x3       y1~~y1  #>   0.36058737   1.37746019   0.08132479   0.12091448   0.46612308   1.85490471  #>       y2~~y2       y3~~y3       y4~~y4       y5~~y5       y6~~y6       y7~~y7  #>   7.57383439   4.96420359   3.21892497   2.31307420   4.97264951   3.56491599  #>       y8~~y8 ind60~~ind60 dem60~~dem60 dem65~~dem65         x1~1         x2~1  #>   3.31055101   0.44871438   3.87381195   0.16435650   5.05438384   4.79219463  #>         x3~1         y1~1         y2~1         y3~1         y4~1         y5~1  #>   3.55768979   5.46466667   4.25644288   6.56311025   4.45253304   5.13625192  #>         y6~1         y7~1         y8~1  #>   2.97807408   6.19626389   4.04338968 coef(lavaanModel) #>    ind60=~x2    ind60=~x3            a            b            c            a  #>        2.180        1.818        1.191        1.175        1.251        1.191  #>            b            c  dem60~ind60  dem65~ind60  dem65~dem60       y1~~y5  #>        1.175        1.251        1.471        0.600        0.865        0.583  #>       y2~~y4       y2~~y6       y3~~y7       y4~~y8       y6~~y8       x1~~x1  #>        1.440        2.183        0.712        0.363        1.372        0.081  #>       x2~~x2       x3~~x3       y1~~y1       y2~~y2       y3~~y3       y4~~y4  #>        0.120        0.467        1.855        7.581        4.956        3.225  #>       y5~~y5       y6~~y6       y7~~y7       y8~~y8 ind60~~ind60 dem60~~dem60  #>        2.313        4.968        3.560        3.308        0.449        3.875  #> dem65~~dem65  #>        0.164 lessSEM:::.standardErrors(SEM = mySEM, raw = FALSE) #>    ind60=~x2    ind60=~x3            a            b            c  dem60~ind60  #>   0.13885220   0.15204330   0.14166120   0.11987057   0.12295637   0.39139696  #>  dem65~ind60  dem65~dem60       y1~~y5       y2~~y4       y2~~y6       y3~~y7  #>   0.23828913   0.07567860   0.36462028   0.68977248   0.73096921   0.62119518  #>       y4~~y8       y6~~y8       x1~~x1       x2~~x2       x3~~x3       y1~~y1  #>   0.46062833   0.57969391   0.01968652   0.06991196   0.08897395   0.45717113  #>       y2~~y2       y3~~y3       y4~~y4       y5~~y5       y6~~y6       y7~~y7  #>   1.34332169   0.96373267   0.74092220   0.48364101   0.89600780   0.73922564  #>       y8~~y8 ind60~~ind60 dem60~~dem60 dem65~~dem65         x1~1         x2~1  #>   0.71425332   0.08675480   0.88802933   0.23331748   0.08406657   0.17326967  #>         x3~1         y1~1         y2~1         y3~1         y4~1         y5~1  #>   0.16121433   0.29892606   0.43891242   0.39404806   0.37957637   0.30446534  #>         y6~1         y7~1         y8~1  #>   0.39247640   0.36442149   0.37545879 parameterEstimates(lavaanModel)[,1:6] #>      lhs op   rhs label   est    se #> 1  ind60 =~    x1       1.000 0.000 #> 2  ind60 =~    x2       2.180 0.138 #> 3  ind60 =~    x3       1.818 0.152 #> 4  dem60 =~    y1       1.000 0.000 #> 5  dem60 =~    y2     a 1.191 0.139 #> 6  dem60 =~    y3     b 1.175 0.120 #> 7  dem60 =~    y4     c 1.251 0.117 #> 8  dem65 =~    y5       1.000 0.000 #> 9  dem65 =~    y6     a 1.191 0.139 #> 10 dem65 =~    y7     b 1.175 0.120 #> 11 dem65 =~    y8     c 1.251 0.117 #> 12 dem60  ~ ind60       1.471 0.392 #> 13 dem65  ~ ind60       0.600 0.226 #> 14 dem65  ~ dem60       0.865 0.075 #> 15    y1 ~~    y5       0.583 0.356 #> 16    y2 ~~    y4       1.440 0.689 #> 17    y2 ~~    y6       2.183 0.737 #> 18    y3 ~~    y7       0.712 0.611 #> 19    y4 ~~    y8       0.363 0.444 #> 20    y6 ~~    y8       1.372 0.577 #> 21    x1 ~~    x1       0.081 0.019 #> 22    x2 ~~    x2       0.120 0.070 #> 23    x3 ~~    x3       0.467 0.090 #> 24    y1 ~~    y1       1.855 0.433 #> 25    y2 ~~    y2       7.581 1.366 #> 26    y3 ~~    y3       4.956 0.956 #> 27    y4 ~~    y4       3.225 0.723 #> 28    y5 ~~    y5       2.313 0.479 #> 29    y6 ~~    y6       4.968 0.921 #> 30    y7 ~~    y7       3.560 0.710 #> 31    y8 ~~    y8       3.308 0.704 #> 32 ind60 ~~ ind60       0.449 0.087 #> 33 dem60 ~~ dem60       3.875 0.866 #> 34 dem65 ~~ dem65       0.164 0.227"},{"path":"/articles/The-Structural-Equation-Model.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"The-Structural-Equation-Model","text":"Fox, J. (2006). Teacher’s corner: Structural equation modeling sem package R. Structural Equation Modeling: Multidisciplinary Journal, 13(3), 465–486. https://doi.org/10.1207/s15328007sem1303_7 McArdle, J. J., & McDonald, R. P. (1984). algebraic properties Reticular Action Model moment structures. British Journal Mathematical Statistical Psychology, 37(2), 234–251. https://doi.org/10.1111/j.2044-8317.1984.tb00802.x","code":""},{"path":"/articles/lessSEM.html","id":"regularized-structural-equation-modeling","dir":"Articles","previous_headings":"","what":"Regularized Structural Equation Modeling","title":"lessSEM","text":"Regularized structural equation modeling proposed Jacobucci et al. (2016) Huang et al. (2017). objective reduce overfitting small samples allow flexibility. general idea push parameters towards zero. end, penalty function \\(p(\\pmb\\theta)\\) added vanilla objective function. lessSEM, objective function given full information maximum likelihood function \\(F_{\\text{ML}}(\\pmb\\theta)\\). new objective function defined : \\[F_{\\text{REGSEM},\\lambda}(\\pmb\\theta) = F_{\\text{ML}}(\\pmb\\theta)+ \\lambda N p(\\pmb\\theta)\\] Think function tug--war: \\(F_{\\text{ML}}(\\pmb\\theta)\\) wants parameters close ordinary maximum likelihood estimates \\(p(\\pmb\\theta)\\) wants regularized parameters close zero \\(\\lambda\\) allows us fine tune two forces mentioned gets influence final parameter estimates \\(N\\) sample size. Scaling \\(N\\) done stay consistent results returned regsem lslx. many different penalty functions used. lessSEM, implemented following functions: \\[ \\begin{array}{l|llll}     \\text{penalty} & \\text{function} & \\text{optimizer} & \\text{reference}\\\\     \\hline     \\text{ridge} & p( x_j) = \\lambda x_j^2 & \\text{glmnet, ista} & \\text{(Hoerl \\& Kennard, 1970)}\\\\     \\text{lasso} & p( x_j) = \\lambda| x_j| & \\text{glmnet, ista} & \\text{(Tibshirani, 1996)}\\\\     \\text{adaptiveLasso} & p( x_j) = \\frac{1}{w_j}\\lambda| x_j| & \\text{glmnet, ista} & \\text{(Zou, 2006)}\\\\     \\text{elasticNet} & p( x_j) = \\alpha\\lambda| x_j| + (1-\\alpha)\\lambda x_j^2 & \\text{glmnet, ista} & \\text{(Zou \\& Hastie, 2005)}\\\\     \\text{cappedL1} & p( x_j) = \\lambda \\min(| x_j|, \\theta); \\theta > 0 &\\text{glmnet, ista}& \\text{(Zhang, 2010)}\\\\     \\text{lsp} & p( x_j) = \\lambda \\log(1 + |x_j|/\\theta); \\theta > 0 &\\text{glmnet, ista}& \\text{(Candès et al., 2008)} \\\\     \\text{scad} & p( x_j) = \\begin{cases}         \\lambda |x_j| & \\text{} |x_j| \\leq \\lambda\\\\         \\frac{-x_j^2 + 2\\theta\\lambda |x_j| - \\lambda^2}{2(\\theta -1)} & \\text{} \\lambda < |x_j| \\leq \\lambda\\theta \\\\         (\\theta + 1) \\lambda^2/2 & \\text{} |x_j| \\geq \\theta\\lambda\\\\     \\end{cases}; \\theta > 2 &\\text{glmnet, ista}& \\text{(Fan \\& Li, 2001)} \\\\     \\text{mcp} & p( x_j) =     \\begin{cases}         \\lambda |x_j| - x_j^2/(2\\theta) & \\text{} |x_j| \\leq \\theta\\lambda\\\\         \\theta\\lambda^2/2 & \\text{} |x_j| > \\lambda\\theta     \\end{cases}; \\theta > 0 &\\text{glmnet, ista}& \\text{(Zhang, 2010)} \\end{array} \\]","code":""},{"path":"/articles/lessSEM.html","id":"objectives","dir":"Articles","previous_headings":"","what":"Objectives","title":"lessSEM","text":"objectives lessSEM provide … flexible framework regularizing SEM. optimizers packages can handle non-differentiable penalty functions.","code":""},{"path":"/articles/lessSEM.html","id":"regularizing-sem","dir":"Articles","previous_headings":"","what":"Regularizing SEM","title":"lessSEM","text":"lessSEM heavily inspired regsem package. also builds lavaan set model.","code":""},{"path":"/articles/lessSEM.html","id":"setting-up-a-model","dir":"Articles","previous_headings":"Regularizing SEM","what":"Setting up a model","title":"lessSEM","text":"First, start lavaan: Next, decide parameters regularized. Let’s go l5-l7. lessSEM, always use parameter labels specify parameters regularized! Finally, set regularized model. end, must first decide penalty function want use. want shrink parameters without setting zero, can use ridge regularization. Otherwise, must use penalty functions mentioned . lessSEM, dedicated function penalties. names functions identical “penalty” column table . instance, let’s look lasso penalty: Plot paths see going : Note parameters pulled towards zero \\(\\lambda\\) increases. Note also specify specific values \\(\\lambda\\) lasso function . Instead, specified many \\(\\lambda\\)s want (nLambdas=50). use lasso adaptive lasso, lessSEM can automatically compute \\(\\lambda\\) necessary set parameters zero. currently supported penalties. plots returned lessSEM either ggplot2 elements (case single tuning parameter), created plotly (case 2 tuning parameters). can change plot post-hoc: coef function gives access parameter estimates: interested estimates, use Now, let’s assume also want try scad penalty. case, replace lasso() function scad() function: scad penalty two tuning parmeters \\(\\lambda\\) \\(\\theta\\). naming follows used Gong et al. (2013). can plot results , however requires plotly package currently supported Rmarkdown. parameter estimates can accessed coef() function:","code":"library(lavaan) library(lessSEM) set.seed(4321) # let's simulate data for a simple  # cfa with 7 observed variables data <- lessSEM::simulateExampleData(N = 50,                                       loadings = c(rep(1,4),                                                   rep(0,3)) ) head(data) #>              y1         y2         y3         y4          y5         y6 #> [1,] -0.1737175 -0.1970204  1.1888412  1.8520403  0.16257957  1.8825526 #> [2,] -1.5179940  0.9029781 -0.1726986 -0.3596920 -0.02092956 -0.5798953 #> [3,]  0.6136418  0.2578986 -0.1359237  0.7703602  0.23502463  0.2001872 #> [4,] -0.5920933  0.2157830  1.6784758  1.8568433 -0.60458482  0.2219578 #> [5,]  0.0763996 -1.1442382 -2.8122156  0.4899892  0.03453494  2.0457604 #> [6,]  2.2504896  2.9742206  0.4353705  1.2338364  0.04693253 -0.6438847 #>              y7 #> [1,]  1.1383999 #> [2,]  0.9020861 #> [3,]  0.7986506 #> [4,]  0.4736751 #> [5,] -2.6721417 #> [6,] -1.1386235  # we assume a single factor structure lavaanSyntax <- \"       f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 + l6*y6 + l7*y7        f ~~ 1*f       \" # estimate the model with lavaan lavaanModel <- cfa(lavaanSyntax,                     data = data) regularized <- c(\"l5\", \"l6\", \"l7\") # tip: we can use paste0 to make this easier: regularized <- paste0(\"l\", 5:7) fitLasso <- lasso(lavaanModel = lavaanModel,                    regularized = regularized,                   # please use much larger nLambdas in practice (e.g., 100)!                   nLambdas = 5) plot(fitLasso) plot(fitLasso) +    ggplot2::theme_bw() coef(fitLasso) #>                                                                                #>   Tuning         ||--||  Estimates                                             #>  ------- ------- ||--|| ---------- ---------- ---------- ---------- ---------- #>   lambda   alpha ||--||         l2         l3         l4         l5         l6 #>  ======= ======= ||--|| ========== ========== ========== ========== ========== #>   0.1034  1.0000 ||--||     0.7523     0.7536     0.5742          .          . #>   0.0776  1.0000 ||--||     0.7477     0.7480     0.5720    -0.0104          . #>   0.0517  1.0000 ||--||     0.7399     0.7396     0.5688    -0.0266          . #>   0.0259  1.0000 ||--||     0.7301     0.7332     0.5677    -0.0418          . #>   0.0000  1.0000 ||--||     0.7239     0.7319     0.5688    -0.0562     0.0166 #>                                                                               #>                                                                               #>  ---------- ---------- ---------- ---------- ---------- ---------- ---------- #>          l7     y1~~y1     y2~~y2     y3~~y3     y4~~y4     y5~~y5     y6~~y6 #>  ========== ========== ========== ========== ========== ========== ========== #>           .     0.8812     1.1477     1.9273     1.0804     0.5710     0.9628 #>           .     0.8742     1.1523     1.9331     1.0818     0.5705     0.9628 #>     -0.0090     0.8631     1.1602     1.9417     1.0838     0.5697     0.9628 #>     -0.0478     0.8528     1.1706     1.9482     1.0841     0.5689     0.9628 #>     -0.0894     0.8491     1.1779     1.9496     1.0830     0.5685     0.9626 #>             #>             #>  ---------- #>      y7~~y7 #>  ========== #>      1.5320 #>      1.5320 #>      1.5312 #>      1.5282 #>      1.5255 estimates(fitLasso) #>             l2        l3        l4          l5         l6           l7 #> [1,] 0.7522904 0.7536140 0.5742228  0.00000000 0.00000000  0.000000000 #> [2,] 0.7476862 0.7480112 0.5720039 -0.01042497 0.00000000  0.000000000 #> [3,] 0.7398991 0.7395932 0.5688300 -0.02660810 0.00000000 -0.008974807 #> [4,] 0.7301019 0.7331646 0.5677000 -0.04181445 0.00000000 -0.047849804 #> [5,] 0.7239003 0.7318701 0.5688230 -0.05624391 0.01658325 -0.089365627 #>         y1~~y1   y2~~y2   y3~~y3   y4~~y4    y5~~y5    y6~~y6   y7~~y7 #> [1,] 0.8812416 1.147737 1.927350 1.080353 0.5710041 0.9628056 1.531997 #> [2,] 0.8741617 1.152318 1.933141 1.081837 0.5704936 0.9628058 1.532015 #> [3,] 0.8630783 1.160213 1.941722 1.083766 0.5696926 0.9628057 1.531243 #> [4,] 0.8528254 1.170580 1.948163 1.084065 0.5689390 0.9628055 1.528245 #> [5,] 0.8491201 1.177878 1.949609 1.082966 0.5684702 0.9625805 1.525473 fitScad <- scad(lavaanModel = lavaanModel,                  regularized = regularized,                 lambdas = seq(0,1,length.out = 4),                 thetas = seq(2.1, 5,length.out = 2)) plot(fitScad) coef(fitScad) #>                                                                                #>   Tuning         ||--||  Estimates                                             #>  ------- ------- ||--|| ---------- ---------- ---------- ---------- ---------- #>   lambda   theta ||--||         l2         l3         l4         l5         l6 #>  ======= ======= ||--|| ========== ========== ========== ========== ========== #>   0.0000  2.1000 ||--||     0.7240     0.7320     0.5689    -0.0562     0.0166 #>   0.3333  2.1000 ||--||     0.7523     0.7535     0.5742          .          . #>   0.6667  2.1000 ||--||     0.7522     0.7536     0.5742          .          . #>   1.0000  2.1000 ||--||     0.7522     0.7536     0.5742          .          . #>   0.0000  5.0000 ||--||     0.7242     0.7323     0.5690    -0.0562     0.0166 #>   0.3333  5.0000 ||--||     0.7522     0.7535     0.5740          .          . #>   0.6667  5.0000 ||--||     0.7522     0.7535     0.5742          .          . #>   1.0000  5.0000 ||--||     0.7522     0.7535     0.5742          .          . #>                                                                               #>                                                                               #>  ---------- ---------- ---------- ---------- ---------- ---------- ---------- #>          l7     y1~~y1     y2~~y2     y3~~y3     y4~~y4     y5~~y5     y6~~y6 #>  ========== ========== ========== ========== ========== ========== ========== #>     -0.0894     0.8492     1.1778     1.9495     1.0829     0.5684     0.9626 #>           .     0.8812     1.1478     1.9274     1.0804     0.5710     0.9628 #>           .     0.8812     1.1478     1.9274     1.0804     0.5710     0.9628 #>           .     0.8812     1.1478     1.9274     1.0804     0.5710     0.9628 #>     -0.0894     0.8495     1.1776     1.9493     1.0829     0.5684     0.9626 #>           .     0.8811     1.1479     1.9275     1.0805     0.5711     0.9628 #>           .     0.8811     1.1479     1.9274     1.0804     0.5710     0.9628 #>           .     0.8811     1.1478     1.9275     1.0804     0.5710     0.9628 #>             #>             #>  ---------- #>      y7~~y7 #>  ========== #>      1.5255 #>      1.5320 #>      1.5320 #>      1.5320 #>      1.5255 #>      1.5320 #>      1.5320 #>      1.5320"},{"path":"/articles/lessSEM.html","id":"selecting-a-model","dir":"Articles","previous_headings":"Regularizing SEM","what":"Selecting a model","title":"lessSEM","text":"select model report final parameter estimates, can use AIC BIC. two ways use information criteria. First, can compute select model : easier way use coef() function : Alternatively, can extract just estimates :","code":"AICs <- AIC(fitLasso) head(AICs) #>       lambda alpha objectiveValue regObjectiveValue     m2LL  regM2LL #> 1 0.10340887     1       1071.078          1071.078 1071.078 1071.078 #> 2 0.07755666     1       1071.033          1071.074 1071.033 1071.074 #> 3 0.05170444     1       1070.956          1071.048 1070.956 1071.048 #> 4 0.02585222     1       1070.851          1070.967 1070.851 1070.967 #> 5 0.00000000     1       1070.810          1070.810 1070.810 1070.810 #>   nonZeroParameters convergence      AIC #> 1                10        TRUE 1091.078 #> 2                11        TRUE 1093.033 #> 3                12        TRUE 1094.956 #> 4                12        TRUE 1094.851 #> 5                13        TRUE 1096.810  fitLasso@parameters[which.min(AICs$AIC),] #>      lambda alpha        l2       l3        l4 l5 l6 l7    y1~~y1   y2~~y2 #> 1 0.1034089     1 0.7522904 0.753614 0.5742228  0  0  0 0.8812416 1.147737 #>    y3~~y3   y4~~y4    y5~~y5    y6~~y6   y7~~y7 #> 1 1.92735 1.080353 0.5710041 0.9628056 1.531997 coef(fitLasso, criterion = \"AIC\") #>                                                                                #>   Tuning         ||--||  Estimates                                             #>  ------- ------- ||--|| ---------- ---------- ---------- ---------- ---------- #>   lambda   alpha ||--||         l2         l3         l4         l5         l6 #>  ======= ======= ||--|| ========== ========== ========== ========== ========== #>   0.1034  1.0000 ||--||     0.7523     0.7536     0.5742          .          . #>                                                                               #>                                                                               #>  ---------- ---------- ---------- ---------- ---------- ---------- ---------- #>          l7     y1~~y1     y2~~y2     y3~~y3     y4~~y4     y5~~y5     y6~~y6 #>  ========== ========== ========== ========== ========== ========== ========== #>           .     0.8812     1.1477     1.9273     1.0804     0.5710     0.9628 #>             #>             #>  ---------- #>      y7~~y7 #>  ========== #>      1.5320 estimates(fitLasso, criterion = \"AIC\") #>             l2       l3        l4 l5 l6 l7    y1~~y1   y2~~y2  y3~~y3   y4~~y4 #> [1,] 0.7522904 0.753614 0.5742228  0  0  0 0.8812416 1.147737 1.92735 1.080353 #>         y5~~y5    y6~~y6   y7~~y7 #> [1,] 0.5710041 0.9628056 1.531997"},{"path":"/articles/lessSEM.html","id":"cross-validation","dir":"Articles","previous_headings":"Regularizing SEM > Selecting a model","what":"Cross-Validation","title":"lessSEM","text":"good alternative information criteria use cross-validation. lessSEM, dedicated cross-validation function penalties discussed . Let’s look lsp() penalty time. Now, non-cross-validated lsp, use use cross-validated version lsp, simply use cv prefix. function called cvLsp(): best model can now accessed ","code":"fitLsp <- lsp(lavaanModel = lavaanModel,                regularized = regularized,               lambdas = seq(0,1,.1),               thetas = seq(.1,2,length.out = 4)) fitCvLsp <- cvLsp(lavaanModel = lavaanModel,                    regularized = regularized,                   lambdas = seq(0,1,.1),                   thetas = seq(.1,2,length.out = 4)) coef(fitCvLsp) #>                                                                                #>   Tuning         ||--||  Estimates                                             #>  ------- ------- ||--|| ---------- ---------- ---------- ---------- ---------- #>   lambda   theta ||--||         l2         l3         l4         l5         l6 #>  ======= ======= ||--|| ========== ========== ========== ========== ========== #>   1.0000  0.1000 ||--||     0.7523     0.7536     0.5742          .          . #>                                                                               #>                                                                               #>  ---------- ---------- ---------- ---------- ---------- ---------- ---------- #>          l7     y1~~y1     y2~~y2     y3~~y3     y4~~y4     y5~~y5     y6~~y6 #>  ========== ========== ========== ========== ========== ========== ========== #>           .     0.8813     1.1477     1.9273     1.0804     0.5710     0.9628 #>             #>             #>  ---------- #>      y7~~y7 #>  ========== #>      1.5320"},{"path":"/articles/lessSEM.html","id":"missing-data","dir":"Articles","previous_headings":"Regularizing SEM","what":"Missing Data","title":"lessSEM","text":"psychological data sets missing data. lessSEM, use full information maximum likelihood function account missingness. lessSEM expects already use full information maximum likelihood method lavaan. Note added argument missing = 'ml' lavaan model. tells lavaan use full information maximum likelihood function. Next, pass model penalty functions lessSEM. lessSEM automatically switch full information maximum likelihood function well: check lessSEM actually use full information maximum likelihood, can compare 2log-likelihood lavaan lessSEM penalty used (\\(\\lambda = 0\\)): Compare :","code":"# let's simulate data for a simple  # cfa with 7 observed variables # and 10 % missing data data <- lessSEM::simulateExampleData(N = 100,                                       loadings = c(rep(1,4),                                                   rep(0,3)),                                      percentMissing = 10 ) head(data) #>               y1         y2         y3          y4         y5        y6 #> [1,]  0.60367543 -0.3206755 -0.5712115  0.36626658  0.6138552 0.8207451 #> [2,]  0.37497661  2.0100766 -1.5925242 -0.02983920  0.2409065 1.1250778 #> [3,]          NA  0.8134143  1.7803075  3.27710938 -0.3651732        NA #> [4,] -0.04379503  0.1369219 -1.9424719  0.40304282 -0.6435542 1.5412868 #> [5,] -0.32969221         NA -1.6536493 -2.20991516  1.2462449 0.6725163 #> [6,]  0.61738032  0.9116425  0.9196841  0.03340633  0.5553805 0.1209500 #>              y7 #> [1,]  0.6346473 #> [2,]  0.8865902 #> [3,] -0.8283463 #> [4,]  0.0635044 #> [5,]         NA #> [6,]  2.0956358  # we assume a single factor structure lavaanSyntax <- \"       f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 + l6*y6 + l7*y7        f ~~ 1*f       \" # estimate the model with lavaan lavaanModel <- cfa(lavaanSyntax,                     data = data,                    missing = \"ml\") # important: use fiml for missing data fitLasso <- lasso(lavaanModel = lavaanModel,                    regularized = regularized,                   nLambdas = 10) fitLasso <- lasso(lavaanModel = lavaanModel,                    regularized = regularized,                   lambdas = 0) fitLasso@fits$m2LL #> [1] 2034.104 -2*logLik(lavaanModel) #> 'log Lik.' 2034.104 (df=20)"},{"path":"/articles/lessSEM.html","id":"using-multiple-cores","dir":"Articles","previous_headings":"","what":"Using multiple cores","title":"lessSEM","text":"default, lessSEM use one computer core. However, model many parameters, parallel computations can faster. Multi-Core support therefore provided using RcppParallel package (Allaire et. al, 2023). make use multiple cores, number cores must specified control argument (see ). , makes sense check many cores computer : Note using cores can block computer resources left tasks R. use 2 cores, can set nCores = 2 follows: Note multi-core support provided SEM. Using optimizers implemented lessSEM models SEM (e.g., lessLM package) automatically allow multi-core execution.","code":"library(RcppParallel) # Print the number of threads (we call them cores for simplicity, but technically they are threads) RcppParallel::defaultNumThreads() #> [1] 4 fitLasso <- lasso(lavaanModel = lavaanModel,                    regularized = regularized,                   nLambdas = 10,                   control = controlGlmnet(nCores = 2))"},{"path":"/articles/lessSEM.html","id":"changing-the-optimizer","dir":"Articles","previous_headings":"","what":"Changing the optimizer","title":"lessSEM","text":"lessSEM comes two specialized optimization procedures: ista glmnet. Currently, default glmnet penalties. Ista require computation Hessian matrix. However, comes price: ista optimization tends call fit gradient function lot glment. recommend first test glmnet optimizer switch ista glmnet results errors due Hessian matrix. Switching ista done follows:","code":"fitLasso <- lasso(lavaanModel = lavaanModel,                    regularized = regularized,                   nLambdas = 10,                   method = \"ista\", # change the method                   control = controlIsta() # change the control argument                   )"},{"path":"/articles/lessSEM.html","id":"parameter-transformations","dir":"Articles","previous_headings":"","what":"Parameter transformations","title":"lessSEM","text":"lessSEM allows parameter transformations. explained detail vignette Parameter-transformations (see vignette(\"Parameter-transformations\", package = \"lessSEM\")). provide short example, let’s look political democracy data set: Note model estimated , loadings latent variables constrained equality time. also relax assumption allowing time point specific loadings: Deciding approaches can difficult may parameters equality time holds, others violate assumption. , transformations can used regularize differences parameters. end, define transformations: Next, pass transformations variable penalty function: check measurement invariance can assumed, can select best model using information criteria: details provided vignette(\"Parameter-transformations\", package = \"lessSEM\").","code":"# example from ?lavaan::sem library(lavaan) modelSyntax <- '    # latent variable definitions      ind60 =~ x1 + x2 + x3      dem60 =~ y1 + a*y2 + b*y3 + c*y4      dem65 =~ y5 + a*y6 + b*y7 + c*y8    # regressions     dem60 ~ ind60     dem65 ~ ind60 + dem60    # residual correlations     y1 ~~ y5     y2 ~~ y4      y3 ~~ y7     y4 ~~ y8     y6 ~~ y8 '  lavaanFit <- sem(model = modelSyntax,                  data = PoliticalDemocracy) library(lavaan) modelSyntax <- '    # latent variable definitions      ind60 =~ x1 + x2 + x3      dem60 =~ y1 + a1*y2 + b1*y3 + c1*y4      dem65 =~ y5 + a2*y6 + b2*y7 + c2*y8    # regressions     dem60 ~ ind60     dem65 ~ ind60 + dem60    # residual correlations     y1 ~~ y5     y2 ~~ y4      y3 ~~ y7     y4 ~~ y8     y6 ~~ y8 '  lavaanFit <- sem(model = modelSyntax,                  data = PoliticalDemocracy) transformations <- \" // IMPORTANT: Our transformations always have to start with the follwing line: parameters: a1, a2, b1, b2, c1, c2, delta_a2, delta_b2, delta_c2  // In the line above, we defined the names of the parameters which we // want to use in our transformations. EACH AND EVERY PARAMETER USED IN // THE FOLLOWING MUST BE STATED ABOVE. The line must always start with // the keyword 'parameters' followed by a colon. The parameters must be // separated by commata. // Comments are added with double-backslash  // Now we can state our transformations:  a2 = a1 + delta_a2; // statements must end with semicolon b2 = b1 + delta_b2; c2 = c1 + delta_c2; \" lassoFit <- lasso(lavaanModel = lavaanFit,                    regularized = c(\"delta_a2\", \"delta_b2\", \"delta_c2\"),# we want to regularize                    # the differences between the parameters                   nLambdas = 100,                   # Our model modification must make use of the modifyModel - function:                   modifyModel = modifyModel(transformations = transformations) ) coef(lassoFit, criterion = \"BIC\") #>                                                                                #>   Tuning         ||--||  Estimates                                             #>  ------- ------- ||--|| ---------- ---------- ---------- ---------- ---------- #>   lambda   alpha ||--||  ind60=~x2  ind60=~x3         a1         b1         c1 #>  ======= ======= ||--|| ========== ========== ========== ========== ========== #>   0.2216  1.0000 ||--||     2.1825     1.8189     1.2110     1.1679     1.2340 #>                                                                       #>                                                                       #>  ----------- ----------- ----------- ---------- ---------- ---------- #>  dem60~ind60 dem65~ind60 dem65~dem60     y1~~y5     y2~~y4     y3~~y7 #>  =========== =========== =========== ========== ========== ========== #>       1.4534      0.5935      0.8659     0.5552     1.5947     0.7807 #>                                                                               #>                                                                               #>  ---------- ---------- ---------- ---------- ---------- ---------- ---------- #>      y4~~y8     y6~~y8     x1~~x1     x2~~x2     x3~~x3     y1~~y1     y2~~y2 #>  ========== ========== ========== ========== ========== ========== ========== #>      0.6537     1.5350     0.0820     0.1177     0.4675     1.7929     7.3843 #>                                                                                 #>                                                                                 #>  ---------- ---------- ---------- ---------- ---------- ---------- ------------ #>      y3~~y3     y4~~y4     y5~~y5     y6~~y6     y7~~y7     y8~~y8 ind60~~ind60 #>  ========== ========== ========== ========== ========== ========== ============ #>      5.0175     3.4074     2.2857     4.8977     3.5510     3.4511       0.4480 #>                                                                               #>                                                             ||--||  Transform #>  ------------ ------------ ---------- ---------- ---------- ||--|| ---------- #>  dem60~~dem60 dem65~~dem65   delta_a2   delta_b2   delta_c2 ||--||         a2 #>  ============ ============ ========== ========== ========== ||--|| ========== #>        3.9408       0.2034          .          .          . ||--||     1.2110 #>                        #>                        #>  ---------- ---------- #>          b2         c2 #>  ========== ========== #>      1.1679     1.2340"},{"path":"/articles/lessSEM.html","id":"experimental-features","dir":"Articles","previous_headings":"","what":"Experimental Features","title":"lessSEM","text":"following features relatively new may still experience bugs. Please aware using features.","code":""},{"path":"/articles/lessSEM.html","id":"from-lesssem-to-lavaan","dir":"Articles","previous_headings":"Experimental Features","what":"From lessSEM to lavaan","title":"lessSEM","text":"lessSEM supports exporting specific models lavaan. can useful plotting final model. result can plotted , instance, semPlot:","code":"lavaanModel <- lessSEM2Lavaan(regularizedSEM = rsem,                                criterion = \"BIC\") library(semPlot) semPaths(lavaanModel,          what = \"est\",          fade = FALSE)"},{"path":"/articles/lessSEM.html","id":"multi-group-models-and-definition-variables","dir":"Articles","previous_headings":"Experimental Features","what":"Multi-Group Models and Definition Variables","title":"lessSEM","text":"lessSEM supports multi-group SEM , degree, definition variables. Regularized multi-group SEM proposed Huang (2018) implemented lslx (Huang, 2020). , differences groups regularized. detailed introduction can found vignette(topic = \"Definition-Variables--Multi-Group-SEM\", package = \"lessSEM\"). Therein also explained multi-group SEM can used implement definition variables (e.g., latent growth curve models).","code":""},{"path":"/articles/lessSEM.html","id":"mixed-penalties","dir":"Articles","previous_headings":"Experimental Features","what":"Mixed Penalties","title":"lessSEM","text":"lessSEM allows defining different penalties different parts model. feature new experimental. Please keep mind using procedure. detailed introduction can found vignette(topic = \"Mixed-Penalties\", package = \"lessSEM\"). provide short example, regularize loadings regression parameters Political Democracy data set different penalties. following script adapted ?lavaan::sem. best model according BIC can extracted :","code":"model <- '    # latent variable definitions      ind60 =~ x1 + x2 + x3 + c2*y2 + c3*y3 + c4*y4      dem60 =~ y1 + y2 + y3 + y4      dem65 =~ y5 + y6 + y7 + c*y8    # regressions     dem60 ~ r1*ind60     dem65 ~ r2*ind60 + r3*dem60 '  lavaanModel <- sem(model,                    data = PoliticalDemocracy)  # Let's add a lasso penalty on the cross-loadings c2 - c4 and  # scad penalty on the regressions r1-r3 mp <- lavaanModel |>   mixedPenalty() |>   addLasso(regularized = c(\"c2\", \"c3\", \"c4\"),             lambdas = seq(0,1,.1)) |>   addLasso(regularized = c(\"r1\", \"r2\", \"r3\"),             lambdas = seq(0,1,.2)) |>   fit() coef(fitMp, criterion = \"BIC\")"},{"path":"/articles/lessSEM.html","id":"more-information","dir":"Articles","previous_headings":"","what":"More information","title":"lessSEM","text":"provide information documentation individual functions. instance, see ?lessSEM::lasso details lasso penalty. interested general purpose interface, look ?lessEM::gpLasso, ?lesssEM::gpMcp, etc. get details implementing lessSEM optimizers package, look vignettes vignette('General-Purpose-Optimization') vignette('-optimizer-interface') lessLM package.","code":""},{"path":[]},{"path":"/articles/lessSEM.html","id":"r---packages-software","dir":"Articles","previous_headings":"References","what":"R - Packages / Software","title":"lessSEM","text":"lavaan Rosseel, Y. (2012). lavaan: R Package Structural Equation Modeling. Journal Statistical Software, 48(2), 1–36. https://doi.org/10.18637/jss.v048.i02 regsem: Jacobucci, R. (2017). regsem: Regularized Structural Equation Modeling. ArXiv:1703.08489 [Stat]. https://arxiv.org/abs/1703.08489 lslx: Huang, P.-H. (2020). lslx: Semi-confirmatory structural equation modeling via penalized likelihood. Journal Statistical Software, 93(7). https://doi.org/10.18637/jss.v093.i07 fasta: Another implementation fista algorithm (Beck & Teboulle, 2009) ensmallen: Curtin, R. R., Edel, M., Prabhu, R. G., Basak, S., Lou, Z., & Sanderson, C. (2021). ensmallen library ﬂexible numerical optimization. Journal Machine Learning Research, 22, 1–6. RcppParallel Allaire J, Francois R, Ushey K, Vandenbrouck G, Geelnard M, Intel (2023). RcppParallel: Parallel Programming Tools ‘Rcpp’. R package version 5.1.6, https://CRAN.R-project.org/package=RcppParallel.","code":""},{"path":"/articles/lessSEM.html","id":"regularized-structural-equation-modeling-1","dir":"Articles","previous_headings":"References","what":"Regularized Structural Equation Modeling","title":"lessSEM","text":"Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Huang, P.-H. (2018). penalized likelihood method multi-group structural equation modelling. British Journal Mathematical Statistical Psychology, 71(3), 499–522. https://doi.org/10.1111/bmsp.12130 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793","code":""},{"path":"/articles/lessSEM.html","id":"penalty-functions","dir":"Articles","previous_headings":"References","what":"Penalty Functions","title":"lessSEM","text":"Candès, E. J., Wakin, M. B., & Boyd, S. P. (2008). Enhancing Sparsity Reweighted l1 Minimization. Journal Fourier Analysis Applications, 14(5–6), 877–905. https://doi.org/10.1007/s00041-008-9045-x Fan, J., & Li, R. (2001). Variable selection via nonconcave penalized likelihood oracle properties. Journal American Statistical Association, 96(456), 1348–1360. https://doi.org/10.1198/016214501753382273 Hoerl, . E., & Kennard, R. W. (1970). Ridge Regression: Biased Estimation Nonorthogonal Problems. Technometrics, 12(1), 55–67. https://doi.org/10.1080/00401706.1970.10488634 Tibshirani, R. (1996). Regression shrinkage selection via lasso. Journal Royal Statistical Society. Series B (Methodological), 58(1), 267–288. Zhang, C.-H. (2010). Nearly unbiased variable selection minimax concave penalty. Annals Statistics, 38(2), 894–942. https://doi.org/10.1214/09-AOS729 Zhang, T. (2010). Analysis Multi-stage Convex Relaxation Sparse Regularization. Journal Machine Learning Research, 11, 1081–1107. Zou, H. (2006). adaptive lasso oracle properties. Journal American Statistical Association, 101(476), 1418–1429. https://doi.org/10.1198/016214506000000735 Zou, H., & Hastie, T. (2005). Regularization variable selection via elastic net. Journal Royal Statistical Society: Series B, 67(2), 301–320. https://doi.org/10.1111/j.1467-9868.2005.00503.x","code":""},{"path":[]},{"path":"/articles/lessSEM.html","id":"glmnet","dir":"Articles","previous_headings":"References > Optimizer","what":"GLMNET","title":"lessSEM","text":"Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization paths generalized linear models via coordinate descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421","code":""},{"path":"/articles/lessSEM.html","id":"variants-of-ista","dir":"Articles","previous_headings":"References > Optimizer","what":"Variants of ISTA","title":"lessSEM","text":"Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). general iterative shrinkage thresholding algorithm non-convex regularized optimization problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/articles/lessSEM.html","id":"license-note","dir":"Articles","previous_headings":"","what":"LICENSE NOTE","title":"lessSEM","text":"SOFTWARE PROVIDED ‘’, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"/articles/log-likelihood-gradients.html","id":"ableitung-der-log-likelihood","dir":"Articles","previous_headings":"","what":"Ableitung der Log-Likelihood","title":"log-likelihood-gradients","text":"\\[L(\\pmb\\theta) = \\underbrace{k\\ln(2\\pi)}_{1} + \\underbrace{\\ln(|\\pmb\\Sigma(\\pmb\\theta)|)}_{2} +  \\underbrace{(\\pmb x - \\pmb \\mu(\\pmb\\theta))^T\\pmb\\Sigma(\\pmb\\theta)^{-1}(\\pmb x - \\pmb \\mu(\\pmb\\theta))}_{3}\\] Wir wollen nach \\(\\pmb \\theta\\) ableiten.","code":""},{"path":"/articles/log-likelihood-gradients.html","id":"element-1","dir":"Articles","previous_headings":"Ableitung der Log-Likelihood","what":"Element 1","title":"log-likelihood-gradients","text":"Es gilt \\(\\frac{\\partial}{\\partial \\theta_j} k\\ln(2\\pi)= 0\\)","code":""},{"path":"/articles/log-likelihood-gradients.html","id":"element-2","dir":"Articles","previous_headings":"Ableitung der Log-Likelihood","what":"Element 2","title":"log-likelihood-gradients","text":"Es gilt: \\[\\frac{\\partial}{\\partial \\theta_j}\\ln(|\\pmb\\Sigma(\\pmb\\theta)|) = \\frac{1}{|\\pmb\\Sigma(\\pmb\\theta)|}\\frac{\\partial}{\\partial \\theta_j}|\\pmb\\Sigma(\\pmb\\theta)|\\] Jacobis Formel: \\[\\frac{\\partial}{\\partial \\theta_j}|\\pmb\\Sigma(\\pmb\\theta)| = |\\pmb\\Sigma(\\pmb\\theta)|tr(\\pmb\\Sigma(\\pmb\\theta)^{-1}\\frac{\\partial}{\\partial \\theta_j}\\pmb\\Sigma(\\pmb\\theta))\\] und somit: \\[\\frac{\\partial}{\\partial \\theta_j}\\ln(|\\pmb\\Sigma(\\pmb\\theta)|) = \\frac{1}{|\\pmb\\Sigma(\\pmb\\theta)|}|\\pmb\\Sigma(\\pmb\\theta)|tr(\\pmb\\Sigma(\\pmb\\theta)^{-1}\\frac{\\partial}{\\partial \\theta_j}\\pmb\\Sigma(\\pmb\\theta)) = tr(\\pmb\\Sigma(\\pmb\\theta)^{-1}\\frac{\\partial}{\\partial \\theta_j}\\pmb\\Sigma(\\pmb\\theta))\\] Wir brauchen also die Ableitung der modell-implizierten Kovarianzmatrix nach den Parametern: \\(\\frac{\\partial}{\\partial \\theta_j}\\pmb\\Sigma(\\pmb\\theta)\\). Dabei gilt: \\(\\pmb\\Sigma(\\pmb\\theta) = \\pmb F (\\pmb - \\pmb )^{-1} \\pmb S ((\\pmb - \\pmb )^{-1})^T \\pmb F^T\\).","code":""},{"path":"/articles/log-likelihood-gradients.html","id":"fall-1-der-parameter-theta_j-ist-in-pmb-s-","dir":"Articles","previous_headings":"Ableitung der Log-Likelihood > Element 2","what":"Fall 1: Der Parameter \\(\\theta_j\\) ist in \\(\\pmb S\\).","title":"log-likelihood-gradients","text":"Dann gilt: Außer \\(\\pmb S\\) kann alles andere als Konstante behandelt werden. Es folgt: \\[\\frac{\\partial}{\\partial \\theta_j}\\pmb\\Sigma(\\pmb\\theta) = \\pmb F (\\pmb - \\pmb )^{-1} \\frac{\\partial}{\\partial \\theta_j}\\pmb S ((\\pmb - \\pmb )^{-1})^T \\pmb F^T\\] wobei \\(\\frac{\\partial}{\\partial \\theta_j}\\pmb S\\) eine sparse Matrix mit einsen den Stellen ist, denen \\(\\theta_j\\) vorkommt. Zusammenfassung: \\[\\frac{\\partial}{\\partial \\theta_j}\\ln(|\\pmb\\Sigma(\\pmb\\theta)|) = tr(\\pmb\\Sigma(\\pmb\\theta)^{-1}\\pmb F (\\pmb - \\pmb )^{-1} \\frac{\\partial}{\\partial \\theta_j}\\pmb S ((\\pmb - \\pmb )^{-1})^T \\pmb F^T)\\] Achtung: Wenn die Person Missings hat, kann man die Matrix \\(\\pmb F\\) anpassen, dass die entsprechenden Zeilen und Spalten herausfallen.","code":""},{"path":"/articles/log-likelihood-gradients.html","id":"fall-2-der-parameter-theta_j-ist-in-pmb-a-","dir":"Articles","previous_headings":"Ableitung der Log-Likelihood > Element 2","what":"Fall 2: Der Parameter \\(\\theta_j\\) ist in \\(\\pmb A\\).","title":"log-likelihood-gradients","text":"Dann gilt: Außer \\(\\pmb \\) kann alles andere als Konstante behandelt werden. Zudem gilt: \\(\\frac{\\partial}{\\partial a_i}\\pmb ^{-1} = \\pmb ^{-1}\\frac{\\partial \\pmb }{\\partial a_i} \\pmb ^{-1}\\) (https://math.stackexchange.com/questions/4074265/derivative-involving-inverse-matrix?noredirect=1&lq=1). Es folgt: \\[\\frac{\\partial}{\\partial \\theta_j}\\pmb\\Sigma(\\pmb\\theta) = \\pmb F[(\\pmb - \\pmb )^{-1} \\frac{\\partial\\pmb }{\\partial \\theta_j}(\\pmb - \\pmb )^{-1}][\\pmb S ((\\pmb - \\pmb )^{-1})^T \\pmb F^T] + \\pmb F(\\pmb - \\pmb )^{-1} \\pmb S[(\\pmb - \\pmb )^{-1} \\frac{\\partial\\pmb }{\\partial \\theta_j}(\\pmb - \\pmb )^{-1}]^T\\pmb F^T\\] Zusammenfassung: \\[\\frac{\\partial}{\\partial \\theta_j}\\ln(|\\pmb\\Sigma(\\pmb\\theta)|) = tr(\\pmb\\Sigma(\\pmb\\theta)^{-1}[\\pmb F[(\\pmb - \\pmb )^{-1} \\frac{\\partial\\pmb }{\\partial \\theta_j}(\\pmb - \\pmb )^{-1}][\\pmb S ((\\pmb - \\pmb )^{-1})^T \\pmb F^T] + \\pmb F(\\pmb - \\pmb )^{-1} \\pmb S[(\\pmb - \\pmb )^{-1} \\frac{\\partial\\pmb }{\\partial \\theta_j}(\\pmb - \\pmb )^{-1}]^T\\pmb F^T])\\]","code":""},{"path":"/articles/log-likelihood-gradients.html","id":"fall-3-der-parameter-theta_j-ist-in-pmb-m-wobei-pmb-m-die-mittelwertstruktur-des-sem-ist-","dir":"Articles","previous_headings":"Ableitung der Log-Likelihood > Element 2","what":"Fall 3: Der Parameter \\(\\theta_j\\) ist in \\(\\pmb m\\), wobei \\(\\pmb m\\) die Mittelwertstruktur des SEM ist.","title":"log-likelihood-gradients","text":"Dann gilt: Die Ableitung ist \\(0\\). Hinweis: Element 2 ist unabhängig vom Datensatz!","code":""},{"path":"/articles/log-likelihood-gradients.html","id":"element-3","dir":"Articles","previous_headings":"Ableitung der Log-Likelihood","what":"Element 3","title":"log-likelihood-gradients","text":"\\[\\frac{\\partial}{\\partial \\theta_j}(\\pmb x - \\pmb \\mu(\\pmb\\theta))^T\\pmb\\Sigma(\\pmb\\theta)^{-1}(\\pmb x - \\pmb \\mu(\\pmb\\theta))\\] Es gilt: \\[\\begin{aligned} &\\frac{\\partial}{\\partial \\theta_j}(\\pmb x - \\pmb \\mu(\\pmb\\theta))^T\\pmb\\Sigma(\\pmb\\theta)^{-1}(\\pmb x - \\pmb \\mu(\\pmb\\theta))\\\\ =& [\\frac{\\partial}{\\partial \\theta_j}(\\pmb x - \\pmb \\mu(\\pmb\\theta))^T]\\pmb\\Sigma(\\pmb\\theta)^{-1}(\\pmb x - \\pmb \\mu(\\pmb\\theta)) + (\\pmb x - \\pmb \\mu(\\pmb\\theta))^T\\frac{\\partial}{\\partial \\theta_j}[\\pmb\\Sigma(\\pmb\\theta)^{-1}(\\pmb x - \\pmb \\mu(\\pmb\\theta))] \\\\ =& [\\frac{\\partial}{\\partial \\theta_j}(\\pmb x - \\pmb \\mu(\\pmb\\theta))^T]\\pmb\\Sigma(\\pmb\\theta)^{-1}(\\pmb x - \\pmb \\mu(\\pmb\\theta)) + (\\pmb x - \\pmb \\mu(\\pmb\\theta))^T[\\frac{\\partial}{\\partial \\theta_j}\\pmb\\Sigma(\\pmb\\theta)^{-1}](\\pmb x - \\pmb \\mu(\\pmb\\theta)) + (\\pmb x - \\pmb \\mu(\\pmb\\theta))^T\\pmb\\Sigma(\\pmb\\theta)^{-1}\\frac{\\partial}{\\partial \\theta_j}[(\\pmb x - \\pmb \\mu(\\pmb\\theta))] \\end{aligned}\\] mit \\(\\pmb\\mu (\\pmb\\theta) = \\pmb F(\\pmb - \\pmb )^{-1}\\pmb m\\) wobei \\(\\pmb m\\) die Mittelwertstruktur des SEMs ist.","code":""},{"path":"/articles/log-likelihood-gradients.html","id":"fall-1-der-parameter-theta_j-ist-in-pmb-s--1","dir":"Articles","previous_headings":"Ableitung der Log-Likelihood > Element 3","what":"Fall 1: Der Parameter \\(\\theta_j\\) ist in \\(\\pmb S\\).","title":"log-likelihood-gradients","text":"Dann gilt: Außer \\(\\pmb S\\) kann alles andere als Konstante behandelt werden. Es folgt: \\([\\frac{\\partial}{\\partial \\theta_j}(\\pmb x - \\pmb \\mu(\\pmb\\theta))^T] = 0\\) und somit \\[\\begin{aligned} &[\\frac{\\partial}{\\partial \\theta_j}(\\pmb x - \\pmb \\mu(\\pmb\\theta))^T]\\pmb\\Sigma(\\pmb\\theta)^{-1}(\\pmb x - \\pmb \\mu(\\pmb\\theta)) + (\\pmb x - \\pmb \\mu(\\pmb\\theta))^T[\\frac{\\partial}{\\partial \\theta_j}\\pmb\\Sigma(\\pmb\\theta)^{-1}](\\pmb x - \\pmb \\mu(\\pmb\\theta)) + (\\pmb x - \\pmb \\mu(\\pmb\\theta))^T\\pmb\\Sigma(\\pmb\\theta)^{-1}\\frac{\\partial}{\\partial \\theta_j}[(\\pmb x - \\pmb \\mu(\\pmb\\theta))] \\\\ =&(\\pmb x - \\pmb \\mu(\\pmb\\theta))^T[\\frac{\\partial}{\\partial \\theta_j}\\pmb\\Sigma(\\pmb\\theta)^{-1}](\\pmb x - \\pmb \\mu(\\pmb\\theta)) \\end{aligned}\\] Es gilt (https://math.stackexchange.com/questions/4074265/derivative-involving-inverse-matrix?noredirect=1&lq=1): \\[\\frac{\\partial}{\\partial \\theta_j} \\pmb \\Sigma(\\pmb\\theta)^{-1} = -\\pmb \\Sigma(\\pmb\\theta)^{-1}\\frac{\\partial}{\\partial \\theta_j}\\pmb \\Sigma(\\pmb\\theta)\\Sigma(\\pmb\\theta)^{-1}\\] und somit: \\[\\begin{aligned} &\\frac{\\partial}{\\partial \\theta_j}(\\pmb x - \\pmb \\mu(\\pmb\\theta))^T\\pmb\\Sigma(\\pmb\\theta)^{-1}(\\pmb x - \\pmb \\mu(\\pmb\\theta))\\\\ =&(\\pmb x - \\pmb \\mu(\\pmb\\theta))^T[\\frac{\\partial}{\\partial \\theta_j}\\pmb\\Sigma(\\pmb\\theta)^{-1}](\\pmb x - \\pmb \\mu(\\pmb\\theta))\\\\ =& (\\pmb x - \\pmb \\mu(\\pmb\\theta))^T[-\\pmb \\Sigma(\\pmb\\theta)^{-1}\\frac{\\partial}{\\partial \\theta_j}\\pmb \\Sigma(\\pmb\\theta)\\Sigma(\\pmb\\theta)^{-1}](\\pmb x - \\pmb \\mu(\\pmb\\theta))\\\\ =& (\\pmb x - \\pmb \\mu(\\pmb\\theta))^T[-\\pmb \\Sigma(\\pmb\\theta)^{-1}\\pmb F (\\pmb - \\pmb )^{-1} \\frac{\\partial}{\\partial \\theta_j}\\pmb S ((\\pmb - \\pmb )^{-1})^T \\pmb F^T\\pmb\\Sigma(\\pmb\\theta)^{-1}](\\pmb x - \\pmb \\mu(\\pmb\\theta))\\\\ \\end{aligned}\\] Hinweis: Der letzte Schritt wurde bei Element 2 besprochen.","code":""},{"path":"/articles/log-likelihood-gradients.html","id":"fall-2-der-parameter-theta_j-ist-in-pmb-a--1","dir":"Articles","previous_headings":"Ableitung der Log-Likelihood > Element 3","what":"Fall 2: Der Parameter \\(\\theta_j\\) ist in \\(\\pmb A\\).","title":"log-likelihood-gradients","text":"\\(\\pmb \\) findet sich auch der Mittelwertstruktur wieder. Hier gilt \\[\\begin{aligned} &\\frac{\\partial}{\\partial \\theta_j}(\\pmb x - \\pmb \\mu(\\pmb\\theta))^T\\pmb\\Sigma(\\pmb\\theta)^{-1}(\\pmb x - \\pmb \\mu(\\pmb\\theta))\\\\ =& [\\frac{\\partial}{\\partial \\theta_j}(\\pmb x - \\pmb \\mu(\\pmb\\theta))^T]\\pmb\\Sigma(\\pmb\\theta)^{-1}(\\pmb x - \\pmb \\mu(\\pmb\\theta)) + (\\pmb x - \\pmb \\mu(\\pmb\\theta))^T[\\frac{\\partial}{\\partial \\theta_j}\\pmb\\Sigma(\\pmb\\theta)^{-1}](\\pmb x - \\pmb \\mu(\\pmb\\theta)) + (\\pmb x - \\pmb \\mu(\\pmb\\theta))^T\\pmb\\Sigma(\\pmb\\theta)^{-1}\\frac{\\partial}{\\partial \\theta_j}[(\\pmb x - \\pmb \\mu(\\pmb\\theta))] \\end{aligned}\\] mit \\([\\frac{\\partial}{\\partial \\theta_j}(\\pmb x - \\pmb \\mu(\\pmb\\theta))] = [- \\frac{\\partial}{\\partial \\theta_j}\\pmb \\mu(\\pmb\\theta))] = -\\frac{\\partial}{\\partial \\theta_j}\\pmb F(\\pmb - \\pmb )^{-1}\\pmb m = -\\pmb F(\\pmb - \\pmb )^{-1}\\frac{\\partial (\\pmb - \\pmb )}{\\partial \\theta_j}(\\pmb - \\pmb )^{-1}\\pmb m\\) Es folgt: \\[\\begin{aligned} &\\frac{\\partial}{\\partial \\theta_j}(\\pmb x - \\pmb \\mu(\\pmb\\theta))^T\\pmb\\Sigma(\\pmb\\theta)^{-1}(\\pmb x - \\pmb \\mu(\\pmb\\theta))\\\\ =& 2*[-\\pmb F(\\pmb - \\pmb )^{-1}\\frac{\\partial (\\pmb - \\pmb )}{\\partial \\theta_j}(\\pmb - \\pmb )^{-1}\\pmb m]^T\\pmb\\Sigma(\\pmb\\theta)^{-1}(\\pmb x - \\pmb \\mu(\\pmb\\theta)) + (\\pmb x - \\pmb \\mu(\\pmb\\theta))^T[\\frac{\\partial}{\\partial \\theta_j}\\pmb\\Sigma(\\pmb\\theta)^{-1}](\\pmb x - \\pmb \\mu(\\pmb\\theta))\\\\ =& 2*[-\\pmb F(\\pmb - \\pmb )^{-1}\\frac{\\partial (\\pmb - \\pmb )}{\\partial \\theta_j}(\\pmb - \\pmb )^{-1}\\pmb m]^T\\pmb\\Sigma(\\pmb\\theta)^{-1}(\\pmb x - \\pmb \\mu(\\pmb\\theta)) \\\\ &+ (\\pmb x - \\pmb \\mu(\\pmb\\theta))^T[-\\pmb \\Sigma(\\pmb\\theta)^{-1}[\\pmb F[(\\pmb - \\pmb )^{-1} \\frac{\\partial\\pmb }{\\partial \\theta_j}(\\pmb - \\pmb )^{-1}][\\pmb S ((\\pmb - \\pmb )^{-1})^T \\pmb F^T] \\\\ &+ \\pmb F(\\pmb - \\pmb )^{-1} \\pmb S[(\\pmb - \\pmb )^{-1} \\frac{\\partial\\pmb }{\\partial \\theta_j}(\\pmb - \\pmb )^{-1}]^T\\pmb F^T]\\pmb \\Sigma(\\pmb\\theta)^{-1}](\\pmb x - \\pmb \\mu(\\pmb\\theta))\\\\ \\end{aligned}\\] Hinweis: Der letzte Schritt wurde bei Element 3 besprochen.","code":""},{"path":"/articles/log-likelihood-gradients.html","id":"fall-3-der-parameter-theta_j-ist-in-pmb-m-","dir":"Articles","previous_headings":"Ableitung der Log-Likelihood > Element 3","what":"Fall 3: Der Parameter \\(\\theta_j\\) ist in \\(\\pmb m\\).","title":"log-likelihood-gradients","text":"Dann gilt: Außer \\(\\pmb\\mu (\\pmb\\theta) = \\pmb F(\\pmb - \\pmb )^{-1}\\pmb m\\) kann alles andere als Konstante behandelt werden. \\[\\begin{aligned} &\\frac{\\partial}{\\partial \\theta_j}(\\pmb x - \\pmb \\mu(\\pmb\\theta))^T\\pmb\\Sigma(\\pmb\\theta)^{-1}(\\pmb x - \\pmb \\mu(\\pmb\\theta))\\\\ =& [\\frac{\\partial}{\\partial \\theta_j}(\\pmb x - \\pmb \\mu(\\pmb\\theta))^T]\\pmb\\Sigma(\\pmb\\theta)^{-1}(\\pmb x - \\pmb \\mu(\\pmb\\theta)) + (\\pmb x - \\pmb \\mu(\\pmb\\theta))^T\\frac{\\partial}{\\partial \\theta_j}[\\pmb\\Sigma(\\pmb\\theta)^{-1}(\\pmb x - \\pmb \\mu(\\pmb\\theta))] \\\\ =& [\\frac{\\partial}{\\partial \\theta_j}(\\pmb x - \\pmb \\mu(\\pmb\\theta))^T]\\pmb\\Sigma(\\pmb\\theta)^{-1}(\\pmb x - \\pmb \\mu(\\pmb\\theta)) + (\\pmb x - \\pmb \\mu(\\pmb\\theta))^T\\pmb\\Sigma(\\pmb\\theta)^{-1}\\frac{\\partial}{\\partial \\theta_j}[(\\pmb x - \\pmb \\mu(\\pmb\\theta))] \\\\ =& (-\\pmb F(\\pmb - \\pmb )^{-1}\\pmb e)^T\\pmb\\Sigma(\\pmb\\theta)^{-1}(\\pmb x - \\pmb \\mu(\\pmb\\theta)) + (\\pmb x - \\pmb \\mu(\\pmb\\theta))^T\\pmb\\Sigma(\\pmb\\theta)^{-1}(-\\pmb F(\\pmb - \\pmb )^{-1}\\pmb e)\\\\ =& 2*(- \\pmb F(\\pmb - \\pmb )^{-1}\\pmb e)^T\\pmb\\Sigma(\\pmb\\theta)^{-1}(\\pmb x - \\pmb \\mu(\\pmb\\theta)) \\end{aligned}\\] wobei \\(\\pmb e = \\begin{bmatrix} 0 & 0 & ... & 1 & ... &0\\end{bmatrix}^T\\) ein Vektor ist, der eine eins der Stelle hat, der \\(\\theta_j\\) \\(\\pmb m\\) sitzt.","code":""},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Jannik H. Orzek. Author, maintainer, copyright holder.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Orzek J (2023). lessSEM: Non-Smooth Regularization Structural Equation Models. R package version 1.4, https://github.com/jhorzek/lessSEM. Orzek J, Arnold M, Voelkle M (2023). “Striving Sparsity: Exact Approximate Solutions Regularized Structural Equation Models.” Structural Equation Modeling: Multidisciplinary Journal. doi:10.1080/10705511.2023.2189070. Jacobucci R, Grimm K, McArdle J (2016). “Regularized structural equation modeling.” Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555-566. doi:10.1080/10705511.2016.1154793. Huang P, Chen H, Weng L (2017). “penalized likelihood method structural equation modeling.” Psychometrika, 82(2), 329-354. doi:10.1007/s11336-017-9566-9. Huang P (2018). “penalized likelihood method multi-group structural equation modelling.” British Journal Mathematical Statistical Psychology, 71(3), 499-522. doi:10.1111/bmsp.12130.","code":"@Manual{,   title = {lessSEM: Non-Smooth Regularization for Structural Equation Models},   author = {Jannik H. Orzek},   year = {2023},   url = {https://github.com/jhorzek/lessSEM},   note = {R package version 1.4}, } @Article{,   title = {Striving for Sparsity: On Exact and Approximate Solutions in Regularized Structural Equation Models},   author = {Jannik H. Orzek and Manuel Arnold and Manuel C. Voelkle},   journal = {Structural Equation Modeling: A Multidisciplinary Journal},   year = {2023},   doi = {10.1080/10705511.2023.2189070}, } @Article{,   title = {Regularized structural equation modeling},   author = {Ross Jacobucci and Kevin J. Grimm and John J. McArdle},   journal = {Structural Equation Modeling: A Multidisciplinary Journal},   year = {2016},   volume = {23},   number = {4},   pages = {555-566},   doi = {10.1080/10705511.2016.1154793}, } @Article{,   title = {A penalized likelihood method for structural equation modeling},   author = {Po-Hsien Huang and Hung Chen and Li-Jen Weng},   journal = {Psychometrika},   year = {2017},   volume = {82},   number = {2},   pages = {329-354},   doi = {10.1007/s11336-017-9566-9}, } @Article{,   title = {A penalized likelihood method for multi-group structural equation modelling},   author = {Po-Hsien Huang},   journal = {British Journal of Mathematical and Statistical Psychology},   year = {2018},   volume = {71},   number = {3},   pages = {499-522},   doi = {10.1111/bmsp.12130}, }"},{"path":"/index.html","id":"lesssem-","dir":"","previous_headings":"","what":"Non-Smooth Regularization for Structural Equation Models","title":"Non-Smooth Regularization for Structural Equation Models","text":"lessSEM (lessSEM estimates sparse SEM) R package regularized structural equation modeling (regularized SEM) non-smooth penalty functions (e.g., lasso) building lavaan. lessSEM heavily inspired regsem package lslx packages similar functionality. use lessSEM, please also cite regsem lslx! objectives lessSEM provide … flexible framework regularizing SEM. optimizers packages can handle non-differentiable penalty functions. following penalty functions currently implemented lessSEM:  column “penalty” refers name function call lessSEM package (e.g., lasso called lasso() function). best model can selected AIC BIC. want use cross-validation, use cvLasso, cvAdaptiveLasso, etc. instead (see, e.g., ?lessSEM::cvLasso).","code":""},{"path":"/index.html","id":"regsem-lslx-and-lesssem","dir":"","previous_headings":"","what":"regsem, lslx, and lessSEM","title":"Non-Smooth Regularization for Structural Equation Models","text":"packages regsem, lslx, lessSEM can used regularize basic SEM. fact, outlined , lessSEM heavily inspired regsem lslx. However, packages differ targets: objective lessSEM replace mature packages regsem lslx. Instead, objective provide method developers flexible framework regularized SEM. following shows incomplete comparison features implemented three packages: Warning Dev. refers features supported, still development may bugs. Use caution!","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Non-Smooth Regularization for Structural Equation Models","text":"want install lessSEM CRAN, use following commands R: newest version package can installed GitHub. However, project uses submodules, devtools::install_github download entire R package. download development version, following command needs run git: Navigate folder git copied project install package lessSEM.Rproj file. want download main branch, use Note lessSEM project multiple branches. main branch match version currently available CRAN. development branch newer features yet available CRAN. branch passed current tests test suite, may ready CRAN yet (e.g., objectives road map met). gh-pages used create documentation website. Finally, branches used ongoing development considered unstable.","code":"install.packages(\"lessSEM\") git clone --branch development --recurse-submodules https://github.com/jhorzek/lessSEM.git git clone --recurse-submodules https://github.com/jhorzek/lessSEM.git"},{"path":"/index.html","id":"introduction","dir":"","previous_headings":"","what":"Introduction","title":"Non-Smooth Regularization for Structural Equation Models","text":"Please visit lessSEM website latest documentation. also find short introduction regularized SEM vignette('lessSEM', package = 'lessSEM')documentation individual functions (e.g., see ?lessSEM::scad). Finally, find templates selection models can used lessSEM (e.g., cross-lagged panel model) package lessTemplates.","code":""},{"path":"/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"Non-Smooth Regularization for Structural Equation Models","text":"want regularize loadings, regressions, variances, covariances, can also use one helper functions extract respective parameter labels lavaan pass lessSEM:","code":"library(lessSEM) library(lavaan)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \"       f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +             l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +             l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15       f ~~ 1*f       \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Optional: Plot the model # if(!require(\"semPlot\")) install.packages(\"semPlot\") # semPlot::semPaths(lavaanModel,  #                   what = \"est\", #                   fade = FALSE)  lsem <- lasso(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   # names of the regularized parameters:   regularized = c(\"l6\", \"l7\", \"l8\", \"l9\", \"l10\",                   \"l11\", \"l12\", \"l13\", \"l14\", \"l15\"),   # in case of lasso and adaptive lasso, we can specify the number of lambda   # values to use. lessSEM will automatically find lambda_max and fit   # models for nLambda values between 0 and lambda_max. For the other   # penalty functions, lambdas must be specified explicitly   nLambdas = 50)  # use the plot-function to plot the regularized parameters: plot(lsem)  # use the coef-function to show the estimates coef(lsem)  # the best parameters can be extracted with: coef(lsem, criterion = \"AIC\") coef(lsem, criterion = \"BIC\")  # if you just want the estimates, use estimates(): estimates(lsem, criterion = \"AIC\")  # elements of lsem can be accessed with the @ operator: lsem@parameters[1,]  # AIC and BIC for all tuning parameter configurations: AIC(lsem) BIC(lsem)  # cross-validation cv <- cvLasso(lavaanModel = lavaanModel,               regularized = c(\"l6\", \"l7\", \"l8\", \"l9\", \"l10\",                               \"l11\", \"l12\", \"l13\", \"l14\", \"l15\"),               lambdas = seq(0,1,.1),               standardize = TRUE)  # get best model according to cross-validation: coef(cv)  #### Advanced #### # Switching the optimizer: # Use the \"method\" argument to switch the optimizer. The control argument # must also be changed to the corresponding function: lsemIsta <- lasso(   lavaanModel = lavaanModel,   regularized = paste0(\"l\", 6:15),   nLambdas = 50,   method = \"ista\",   control = controlIsta(     # Here, we can also specify that we want to use multiple cores:     nCores = 2))  # Note: The results are basically identical: lsemIsta@parameters - lsem@parameters loadings(lavaanModel) #>  [1] \"l1\"  \"l2\"  \"l3\"  \"l4\"  \"l5\"  \"l6\"  \"l7\"  \"l8\"  \"l9\"  \"l10\" \"l11\" \"l12\" #> [13] \"l13\" \"l14\" \"l15\" regressions(lavaanModel) #> character(0) variances(lavaanModel) #>  [1] \"y1~~y1\"   \"y2~~y2\"   \"y3~~y3\"   \"y4~~y4\"   \"y5~~y5\"   \"y6~~y6\"   #>  [7] \"y7~~y7\"   \"y8~~y8\"   \"y9~~y9\"   \"y10~~y10\" \"y11~~y11\" \"y12~~y12\" #> [13] \"y13~~y13\" \"y14~~y14\" \"y15~~y15\" covariances(lavaanModel) #> character(0)"},{"path":"/index.html","id":"transformations","dir":"","previous_headings":"","what":"Transformations","title":"Non-Smooth Regularization for Structural Equation Models","text":"lessSEM allows parameter transformations , instance, used test measurement invariance longitudinal models (e.g., Liang, 2018; Bauer et al., 2020). thorough introduction provided vignette('Parameter-transformations', package = 'lessSEM'). example, test measurement invariance PoliticalDemocracy data set. Finally, can extract best parameters: differences (delta_a2, delta_b2, delta_c2) zeroed, can assume measurement invariance.","code":"library(lessSEM) library(lavaan) # we will use the PoliticalDemocracy from lavaan (see ?lavaan::sem) model <- '    # latent variable definitions      ind60 =~ x1 + x2 + x3      # assuming different loadings for different time points:      dem60 =~ y1 + a1*y2 + b1*y3 + c1*y4      dem65 =~ y5 + a2*y6 + b2*y7 + c2*y8    # regressions     dem60 ~ ind60     dem65 ~ ind60 + dem60    # residual correlations     y1 ~~ y5     y2 ~~ y4 + y6     y3 ~~ y7     y4 ~~ y8     y6 ~~ y8 '  fit <- sem(model, data = PoliticalDemocracy)  # We will define a transformation which regularizes differences # between loadings over time:  transformations <- \" // which parameters do we want to use? parameters: a1, a2, b1, b2, c1, c2, delta_a2, delta_b2, delta_c2  // transformations: a2 = a1 + delta_a2; b2 = b1 + delta_b2; c2 = c1 + delta_c2; \"  # setting delta_a2, delta_b2, or delta_c2 to zero implies measurement invariance # for the respective parameters (a1, b1, c1) lassoFit <- lasso(lavaanModel = fit,                    # we want to regularize the differences between the parameters                   regularized = c(\"delta_a2\", \"delta_b2\", \"delta_c2\"),                   nLambdas = 100,                   # Our model modification must make use of the modifyModel - function:                   modifyModel = modifyModel(transformations = transformations) ) coef(lassoFit, criterion = \"BIC\")"},{"path":"/index.html","id":"experimental-features","dir":"","previous_headings":"","what":"Experimental Features","title":"Non-Smooth Regularization for Structural Equation Models","text":"following features relatively new may still experience bugs. Please aware using features.","code":""},{"path":"/index.html","id":"from-lesssem-to-lavaan","dir":"","previous_headings":"","what":"From lessSEM to lavaan","title":"Non-Smooth Regularization for Structural Equation Models","text":"lessSEM supports exporting specific models lavaan. can useful plotting final model. result can plotted , instance, semPlot:","code":"lavaanModel <- lessSEM2Lavaan(regularizedSEM = lsem,                                criterion = \"BIC\") library(semPlot) semPaths(lavaanModel,          what = \"est\",          fade = FALSE)"},{"path":"/index.html","id":"multi-group-models-and-definition-variables","dir":"","previous_headings":"","what":"Multi-Group Models and Definition Variables","title":"Non-Smooth Regularization for Structural Equation Models","text":"lessSEM supports multi-group SEM , degree, definition variables. Regularized multi-group SEM proposed Huang (2018) implemented lslx (Huang, 2020). , differences groups regularized. detailed introduction can found vignette(topic = \"Definition-Variables--Multi-Group-SEM\", package = \"lessSEM\"). Therein also explained multi-group SEM can used implement definition variables (e.g., latent growth curve models).","code":""},{"path":"/index.html","id":"mixed-penalties","dir":"","previous_headings":"","what":"Mixed Penalties","title":"Non-Smooth Regularization for Structural Equation Models","text":"lessSEM allows defining different penalties different parts model. feature new experimental. Please keep mind using procedure. detailed introduction can found vignette(topic = \"Mixed-Penalties\", package = \"lessSEM\"). provide short example, regularize loadings regression parameters Political Democracy data set different penalties. following script adapted ?lavaan::sem. best model according BIC can extracted :","code":"model <- '    # latent variable definitions      ind60 =~ x1 + x2 + x3 + c2*y2 + c3*y3 + c4*y4      dem60 =~ y1 + y2 + y3 + y4      dem65 =~ y5 + y6 + y7 + c*y8    # regressions     dem60 ~ r1*ind60     dem65 ~ r2*ind60 + r3*dem60 '  lavaanModel <- sem(model,                    data = PoliticalDemocracy)  # Let's add a lasso penalty on the cross-loadings c2 - c4 and  # scad penalty on the regressions r1-r3 fitMp <- lavaanModel |>   mixedPenalty() |>   addLasso(regularized = c(\"c2\", \"c3\", \"c4\"),             lambdas = seq(0,1,.1)) |>   addScad(regularized = c(\"r1\", \"r2\", \"r3\"),            lambdas = seq(0,1,.2),           thetas = 3.7) |>   fit() coef(fitMp, criterion = \"BIC\")"},{"path":"/index.html","id":"optimizers","dir":"","previous_headings":"","what":"Optimizers","title":"Non-Smooth Regularization for Structural Equation Models","text":"Currently, lessSEM following optimizers: (variants ) iterative shrinkage thresholding (e.g., Beck & Teboulle, 2009; Gong et al., 2013; Parikh & Boyd, 2013); optimization cappedL1, lsp, scad, mcp based Gong et al. (2013) glmnet (Friedman et al., 2010; Yuan et al., 2012; Huang, 2020) optimizers implemented based regCtsem package. importantly, optimizers lessSEM available packages. four ways implement documented vignette(\"General-Purpose-Optimization\", package = \"lessSEM\"). short, : using R interface: general purpose implementations functions called prefix “gp” (gpLasso, gpScad, …). information examples can found documentation functions (e.g., ?lessSEM::gpLasso, ?lessSEM::gpAdaptiveLasso, ?lessSEM::gpElasticNet). interface similar optim optimizers R. using Rcpp, can pass C++ function pointers general purpose optimizers gpLassoCpp, gpScadCpp, … (e.g., ?lessSEM::gpLassoCpp) optimizers implemented C++ header-files lessSEM. Thus, can accessed packages using C++. interface similar ensmallen library. implemented simple example elastic net regularization linear regressions lessLM package. can also find details general design optimizer interface vignette(\"-optimizer-interface\", package = \"lessSEM\"). optimizers implemented separate C++ header library lesstimate can used submodule R packages.","code":""},{"path":[]},{"path":"/index.html","id":"r---packages--software","dir":"","previous_headings":"","what":"R - Packages / Software","title":"Non-Smooth Regularization for Structural Equation Models","text":"lavaan Rosseel, Y. (2012). lavaan: R Package Structural Equation Modeling. Journal Statistical Software, 48(2), 1-36. https://doi.org/10.18637/jss.v048.i02 regsem: Jacobucci, R. (2017). regsem: Regularized Structural Equation Modeling. ArXiv:1703.08489 [Stat]. https://arxiv.org/abs/1703.08489 lslx: Huang, P.-H. (2020). lslx: Semi-confirmatory structural equation modeling via penalized likelihood. Journal Statistical Software, 93(7). https://doi.org/10.18637/jss.v093.i07 fasta: Another implementation fista algorithm (Beck & Teboulle, 2009). ensmallen: Curtin, R. R., Edel, M., Prabhu, R. G., Basak, S., Lou, Z., & Sanderson, C. (2021). ensmallen library ﬂexible numerical optimization. Journal Machine Learning Research, 22, 1–6. regCtsem: Orzek, J. H., & Voelkle, M. C. (press). Regularized continuous time structural equation models: network perspective. Psychological Methods.","code":""},{"path":"/index.html","id":"regularized-structural-equation-modeling","dir":"","previous_headings":"","what":"Regularized Structural Equation Modeling","title":"Non-Smooth Regularization for Structural Equation Models","text":"Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Huang, P.-H. (2018). penalized likelihood method multi-group structural equation modelling. British Journal Mathematical Statistical Psychology, 71(3), 499–522. https://doi.org/10.1111/bmsp.12130 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793","code":""},{"path":"/index.html","id":"penalty-functions","dir":"","previous_headings":"","what":"Penalty Functions","title":"Non-Smooth Regularization for Structural Equation Models","text":"Candès, E. J., Wakin, M. B., & Boyd, S. P. (2008). Enhancing Sparsity Reweighted l1 Minimization. Journal Fourier Analysis Applications, 14(5–6), 877–905. https://doi.org/10.1007/s00041-008-9045-x Fan, J., & Li, R. (2001). Variable selection via nonconcave penalized likelihood oracle properties. Journal American Statistical Association, 96(456), 1348–1360. https://doi.org/10.1198/016214501753382273 Hoerl, . E., & Kennard, R. W. (1970). Ridge Regression: Biased Estimation Nonorthogonal Problems. Technometrics, 12(1), 55–67. https://doi.org/10.1080/00401706.1970.10488634 Tibshirani, R. (1996). Regression shrinkage selection via lasso. Journal Royal Statistical Society. Series B (Methodological), 58(1), 267–288. Zhang, C.-H. (2010). Nearly unbiased variable selection minimax concave penalty. Annals Statistics, 38(2), 894–942. https://doi.org/10.1214/09-AOS729 Zhang, T. (2010). Analysis Multi-stage Convex Relaxation Sparse Regularization. Journal Machine Learning Research, 11, 1081–1107. Zou, H. (2006). adaptive lasso oracle properties. Journal American Statistical Association, 101(476), 1418–1429. https://doi.org/10.1198/016214506000000735 Zou, H., & Hastie, T. (2005). Regularization variable selection via elastic net. Journal Royal Statistical Society: Series B, 67(2), 301–320. https://doi.org/10.1111/j.1467-9868.2005.00503.x","code":""},{"path":[]},{"path":"/index.html","id":"glmnet","dir":"","previous_headings":"Optimizer","what":"GLMNET","title":"Non-Smooth Regularization for Structural Equation Models","text":"Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization paths generalized linear models via coordinate descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421","code":""},{"path":"/index.html","id":"variants-of-ista","dir":"","previous_headings":"Optimizer","what":"Variants of ISTA","title":"Non-Smooth Regularization for Structural Equation Models","text":"Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). general iterative shrinkage thresholding algorithm non-convex regularized optimization problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/index.html","id":"miscellaneous","dir":"","previous_headings":"","what":"Miscellaneous","title":"Non-Smooth Regularization for Structural Equation Models","text":"Liang, X., Yang, Y., & Huang, J. (2018). Evaluation structural relationships autoregressive cross-lagged models longitudinal approximate invariance: Bayesian analysis. Structural Equation Modeling: Multidisciplinary Journal, 25(4), 558–572. https://doi.org/10.1080/10705511.2017.1410706 Bauer, D. J., Belzak, W. C. M., & Cole, V. T. (2020). Simplifying Assessment Measurement Invariance Multiple Background Variables: Using Regularized Moderated Nonlinear Factor Analysis Detect Differential Item Functioning. Structural Equation Modeling: Multidisciplinary Journal, 27(1), 43–55. https://doi.org/10.1080/10705511.2019.1642754","code":""},{"path":"/index.html","id":"license-note","dir":"","previous_headings":"","what":"LICENSE NOTE","title":"Non-Smooth Regularization for Structural Equation Models","text":"SOFTWARE PROVIDED ‘’, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"/reference/AIC-Rcpp_SEMCpp-method.html","id":null,"dir":"Reference","previous_headings":"","what":"AIC — AIC,Rcpp_SEMCpp-method","title":"AIC — AIC,Rcpp_SEMCpp-method","text":"AIC","code":""},{"path":"/reference/AIC-Rcpp_SEMCpp-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"AIC — AIC,Rcpp_SEMCpp-method","text":"","code":"# S4 method for Rcpp_SEMCpp AIC(object, ..., k = 2)"},{"path":"/reference/AIC-Rcpp_SEMCpp-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"AIC — AIC,Rcpp_SEMCpp-method","text":"object object class Rcpp_SEMCpp ... used k multiplier number parameters","code":""},{"path":"/reference/AIC-Rcpp_SEMCpp-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"AIC — AIC,Rcpp_SEMCpp-method","text":"AIC values","code":""},{"path":"/reference/AIC-Rcpp_mgSEM-method.html","id":null,"dir":"Reference","previous_headings":"","what":"AIC — AIC,Rcpp_mgSEM-method","title":"AIC — AIC,Rcpp_mgSEM-method","text":"AIC","code":""},{"path":"/reference/AIC-Rcpp_mgSEM-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"AIC — AIC,Rcpp_mgSEM-method","text":"","code":"# S4 method for Rcpp_mgSEM AIC(object, ..., k = 2)"},{"path":"/reference/AIC-Rcpp_mgSEM-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"AIC — AIC,Rcpp_mgSEM-method","text":"object object class Rcpp_mgSEM ... used k multiplier number parameters","code":""},{"path":"/reference/AIC-Rcpp_mgSEM-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"AIC — AIC,Rcpp_mgSEM-method","text":"AIC values","code":""},{"path":"/reference/AIC-gpRegularized-method.html","id":null,"dir":"Reference","previous_headings":"","what":"AIC — AIC,gpRegularized-method","title":"AIC — AIC,gpRegularized-method","text":"returns AIC","code":""},{"path":"/reference/AIC-gpRegularized-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"AIC — AIC,gpRegularized-method","text":"","code":"# S4 method for gpRegularized AIC(object, ..., k = 2)"},{"path":"/reference/AIC-gpRegularized-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"AIC — AIC,gpRegularized-method","text":"object object class gpRegularized ... used k multiplier number parameters","code":""},{"path":"/reference/AIC-gpRegularized-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"AIC — AIC,gpRegularized-method","text":"data frame fit values, appended AIC","code":""},{"path":"/reference/AIC-regularizedSEM-method.html","id":null,"dir":"Reference","previous_headings":"","what":"AIC — AIC,regularizedSEM-method","title":"AIC — AIC,regularizedSEM-method","text":"returns AIC","code":""},{"path":"/reference/AIC-regularizedSEM-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"AIC — AIC,regularizedSEM-method","text":"","code":"# S4 method for regularizedSEM AIC(object, ..., k = 2)"},{"path":"/reference/AIC-regularizedSEM-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"AIC — AIC,regularizedSEM-method","text":"object object class regularizedSEM ... used k multiplier number parameters","code":""},{"path":"/reference/AIC-regularizedSEM-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"AIC — AIC,regularizedSEM-method","text":"AIC values","code":""},{"path":"/reference/AIC-regularizedSEMMixedPenalty-method.html","id":null,"dir":"Reference","previous_headings":"","what":"AIC — AIC,regularizedSEMMixedPenalty-method","title":"AIC — AIC,regularizedSEMMixedPenalty-method","text":"returns AIC","code":""},{"path":"/reference/AIC-regularizedSEMMixedPenalty-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"AIC — AIC,regularizedSEMMixedPenalty-method","text":"","code":"# S4 method for regularizedSEMMixedPenalty AIC(object, ..., k = 2)"},{"path":"/reference/AIC-regularizedSEMMixedPenalty-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"AIC — AIC,regularizedSEMMixedPenalty-method","text":"object object class regularizedSEMMixedPenalty ... used k multiplier number parameters","code":""},{"path":"/reference/AIC-regularizedSEMMixedPenalty-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"AIC — AIC,regularizedSEMMixedPenalty-method","text":"AIC values","code":""},{"path":"/reference/AIC-regularizedSEMWithCustomPenalty-method.html","id":null,"dir":"Reference","previous_headings":"","what":"AIC — AIC,regularizedSEMWithCustomPenalty-method","title":"AIC — AIC,regularizedSEMWithCustomPenalty-method","text":"returns AIC. Expects penalizedParameterLabels zeroThreshold","code":""},{"path":"/reference/AIC-regularizedSEMWithCustomPenalty-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"AIC — AIC,regularizedSEMWithCustomPenalty-method","text":"","code":"# S4 method for regularizedSEMWithCustomPenalty AIC(object, ..., k = 2)"},{"path":"/reference/AIC-regularizedSEMWithCustomPenalty-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"AIC — AIC,regularizedSEMWithCustomPenalty-method","text":"object object class regularizedSEMWithCustomPenalty ... Expects penalizedParameterLabels zeroThreshold. penalizedParameterLabels: vector labels penalized parameters. zeroThreshold: penalized parameters threshold counted zeroed. k multiplier number parameters","code":""},{"path":"/reference/AIC-regularizedSEMWithCustomPenalty-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"AIC — AIC,regularizedSEMWithCustomPenalty-method","text":"AIC values","code":""},{"path":"/reference/BIC-Rcpp_SEMCpp-method.html","id":null,"dir":"Reference","previous_headings":"","what":"BIC — BIC,Rcpp_SEMCpp-method","title":"BIC — BIC,Rcpp_SEMCpp-method","text":"BIC","code":""},{"path":"/reference/BIC-Rcpp_SEMCpp-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"BIC — BIC,Rcpp_SEMCpp-method","text":"","code":"# S4 method for Rcpp_SEMCpp BIC(object, ...)"},{"path":"/reference/BIC-Rcpp_SEMCpp-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"BIC — BIC,Rcpp_SEMCpp-method","text":"object object class Rcpp_SEMCpp ... used","code":""},{"path":"/reference/BIC-Rcpp_SEMCpp-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"BIC — BIC,Rcpp_SEMCpp-method","text":"BIC values","code":""},{"path":"/reference/BIC-Rcpp_mgSEM-method.html","id":null,"dir":"Reference","previous_headings":"","what":"BIC — BIC,Rcpp_mgSEM-method","title":"BIC — BIC,Rcpp_mgSEM-method","text":"BIC","code":""},{"path":"/reference/BIC-Rcpp_mgSEM-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"BIC — BIC,Rcpp_mgSEM-method","text":"","code":"# S4 method for Rcpp_mgSEM BIC(object, ...)"},{"path":"/reference/BIC-Rcpp_mgSEM-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"BIC — BIC,Rcpp_mgSEM-method","text":"object object class Rcpp_mgSEM ... used","code":""},{"path":"/reference/BIC-Rcpp_mgSEM-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"BIC — BIC,Rcpp_mgSEM-method","text":"BIC values","code":""},{"path":"/reference/BIC-gpRegularized-method.html","id":null,"dir":"Reference","previous_headings":"","what":"BIC — BIC,gpRegularized-method","title":"BIC — BIC,gpRegularized-method","text":"returns BIC","code":""},{"path":"/reference/BIC-gpRegularized-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"BIC — BIC,gpRegularized-method","text":"","code":"# S4 method for gpRegularized BIC(object, ...)"},{"path":"/reference/BIC-gpRegularized-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"BIC — BIC,gpRegularized-method","text":"object object class gpRegularized ... used","code":""},{"path":"/reference/BIC-gpRegularized-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"BIC — BIC,gpRegularized-method","text":"data frame fit values, appended BIC","code":""},{"path":"/reference/BIC-regularizedSEM-method.html","id":null,"dir":"Reference","previous_headings":"","what":"BIC — BIC,regularizedSEM-method","title":"BIC — BIC,regularizedSEM-method","text":"returns BIC","code":""},{"path":"/reference/BIC-regularizedSEM-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"BIC — BIC,regularizedSEM-method","text":"","code":"# S4 method for regularizedSEM BIC(object, ...)"},{"path":"/reference/BIC-regularizedSEM-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"BIC — BIC,regularizedSEM-method","text":"object object class regularizedSEM ... used","code":""},{"path":"/reference/BIC-regularizedSEM-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"BIC — BIC,regularizedSEM-method","text":"BIC values","code":""},{"path":"/reference/BIC-regularizedSEMMixedPenalty-method.html","id":null,"dir":"Reference","previous_headings":"","what":"BIC — BIC,regularizedSEMMixedPenalty-method","title":"BIC — BIC,regularizedSEMMixedPenalty-method","text":"returns BIC","code":""},{"path":"/reference/BIC-regularizedSEMMixedPenalty-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"BIC — BIC,regularizedSEMMixedPenalty-method","text":"","code":"# S4 method for regularizedSEMMixedPenalty BIC(object, ...)"},{"path":"/reference/BIC-regularizedSEMMixedPenalty-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"BIC — BIC,regularizedSEMMixedPenalty-method","text":"object object class regularizedSEMMixedPenalty ... used","code":""},{"path":"/reference/BIC-regularizedSEMMixedPenalty-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"BIC — BIC,regularizedSEMMixedPenalty-method","text":"BIC values","code":""},{"path":"/reference/BIC-regularizedSEMWithCustomPenalty-method.html","id":null,"dir":"Reference","previous_headings":"","what":"BIC — BIC,regularizedSEMWithCustomPenalty-method","title":"BIC — BIC,regularizedSEMWithCustomPenalty-method","text":"returns BIC","code":""},{"path":"/reference/BIC-regularizedSEMWithCustomPenalty-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"BIC — BIC,regularizedSEMWithCustomPenalty-method","text":"","code":"# S4 method for regularizedSEMWithCustomPenalty BIC(object, ...)"},{"path":"/reference/BIC-regularizedSEMWithCustomPenalty-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"BIC — BIC,regularizedSEMWithCustomPenalty-method","text":"object object class regularizedSEMWithCustomPenalty ... Expects penalizedParameterLabels zeroThreshold. penalizedParameterLabels: vector labels penalized parameters. zeroThreshold: penalized parameters threshold counted zeroed.","code":""},{"path":"/reference/BIC-regularizedSEMWithCustomPenalty-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"BIC — BIC,regularizedSEMWithCustomPenalty-method","text":"BIC values","code":""},{"path":"/reference/Rcpp_SEMCpp-class.html","id":null,"dir":"Reference","previous_headings":"","what":"internal representation of SEM in C++ — Rcpp_SEMCpp-class","title":"internal representation of SEM in C++ — Rcpp_SEMCpp-class","text":"internal representation SEM C++","code":""},{"path":"/reference/Rcpp_bfgsEnetMgSEM-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::bfgsEnetMgSEM — Rcpp_bfgsEnetMgSEM-class","title":"Wrapper for C++ module. See ?lessSEM::bfgsEnetMgSEM — Rcpp_bfgsEnetMgSEM-class","text":"Wrapper C++ module. See ?lessSEM::bfgsEnetMgSEM","code":""},{"path":"/reference/Rcpp_bfgsEnetSEM-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::bfgsEnetSEM — Rcpp_bfgsEnetSEM-class","title":"Wrapper for C++ module. See ?lessSEM::bfgsEnetSEM — Rcpp_bfgsEnetSEM-class","text":"Wrapper C++ module. See ?lessSEM::bfgsEnetSEM","code":""},{"path":"/reference/Rcpp_glmnetCappedL1MgSEM-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::glmnetCappedL1MgSEM — Rcpp_glmnetCappedL1MgSEM-class","title":"Wrapper for C++ module. See ?lessSEM::glmnetCappedL1MgSEM — Rcpp_glmnetCappedL1MgSEM-class","text":"Wrapper C++ module. See ?lessSEM::glmnetCappedL1MgSEM","code":""},{"path":"/reference/Rcpp_glmnetCappedL1SEM-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM:::glmnetCappedL1SEM — Rcpp_glmnetCappedL1SEM-class","title":"Wrapper for C++ module. See ?lessSEM:::glmnetCappedL1SEM — Rcpp_glmnetCappedL1SEM-class","text":"Wrapper C++ module. See ?lessSEM:::glmnetCappedL1SEM","code":""},{"path":"/reference/Rcpp_glmnetEnetGeneralPurpose-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::glmnetEnetGeneralPurpose — Rcpp_glmnetEnetGeneralPurpose-class","title":"Wrapper for C++ module. See ?lessSEM::glmnetEnetGeneralPurpose — Rcpp_glmnetEnetGeneralPurpose-class","text":"Wrapper C++ module. See ?lessSEM::glmnetEnetGeneralPurpose","code":""},{"path":"/reference/Rcpp_glmnetEnetGeneralPurposeCpp-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::glmnetEnetGeneralPurposeCpp — Rcpp_glmnetEnetGeneralPurposeCpp-class","title":"Wrapper for C++ module. See ?lessSEM::glmnetEnetGeneralPurposeCpp — Rcpp_glmnetEnetGeneralPurposeCpp-class","text":"Wrapper C++ module. See ?lessSEM::glmnetEnetGeneralPurposeCpp","code":""},{"path":"/reference/Rcpp_glmnetEnetMgSEM-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::glmnetEnetMgSEM — Rcpp_glmnetEnetMgSEM-class","title":"Wrapper for C++ module. See ?lessSEM::glmnetEnetMgSEM — Rcpp_glmnetEnetMgSEM-class","text":"Wrapper C++ module. See ?lessSEM::glmnetEnetMgSEM","code":""},{"path":"/reference/Rcpp_glmnetEnetSEM-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::glmnetEnetSEM — Rcpp_glmnetEnetSEM-class","title":"Wrapper for C++ module. See ?lessSEM::glmnetEnetSEM — Rcpp_glmnetEnetSEM-class","text":"Wrapper C++ module. See ?lessSEM::glmnetEnetSEM","code":""},{"path":"/reference/Rcpp_glmnetLspMgSEM-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::glmnetLspMgSEM — Rcpp_glmnetLspMgSEM-class","title":"Wrapper for C++ module. See ?lessSEM::glmnetLspMgSEM — Rcpp_glmnetLspMgSEM-class","text":"Wrapper C++ module. See ?lessSEM::glmnetLspMgSEM","code":""},{"path":"/reference/Rcpp_glmnetLspSEM-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM:::glmnetLspSEM — Rcpp_glmnetLspSEM-class","title":"Wrapper for C++ module. See ?lessSEM:::glmnetLspSEM — Rcpp_glmnetLspSEM-class","text":"Wrapper C++ module. See ?lessSEM:::glmnetLspSEM","code":""},{"path":"/reference/Rcpp_glmnetMcpMgSEM-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::glmnetMcpMgSEM — Rcpp_glmnetMcpMgSEM-class","title":"Wrapper for C++ module. See ?lessSEM::glmnetMcpMgSEM — Rcpp_glmnetMcpMgSEM-class","text":"Wrapper C++ module. See ?lessSEM::glmnetMcpMgSEM","code":""},{"path":"/reference/Rcpp_glmnetMcpSEM-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM:::glmnetMcpSEM — Rcpp_glmnetMcpSEM-class","title":"Wrapper for C++ module. See ?lessSEM:::glmnetMcpSEM — Rcpp_glmnetMcpSEM-class","text":"Wrapper C++ module. See ?lessSEM:::glmnetMcpSEM","code":""},{"path":"/reference/Rcpp_glmnetScadMgSEM-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::glmnetScadMgSEM — Rcpp_glmnetScadMgSEM-class","title":"Wrapper for C++ module. See ?lessSEM::glmnetScadMgSEM — Rcpp_glmnetScadMgSEM-class","text":"Wrapper C++ module. See ?lessSEM::glmnetScadMgSEM","code":""},{"path":"/reference/Rcpp_glmnetScadSEM-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM:::glmnetScadSEM — Rcpp_glmnetScadSEM-class","title":"Wrapper for C++ module. See ?lessSEM:::glmnetScadSEM — Rcpp_glmnetScadSEM-class","text":"Wrapper C++ module. See ?lessSEM:::glmnetScadSEM","code":""},{"path":"/reference/Rcpp_istaCappedL1GeneralPurpose-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::istaCappedL1GeneralPurpose — Rcpp_istaCappedL1GeneralPurpose-class","title":"Wrapper for C++ module. See ?lessSEM::istaCappedL1GeneralPurpose — Rcpp_istaCappedL1GeneralPurpose-class","text":"Wrapper C++ module. See ?lessSEM::istaCappedL1GeneralPurpose","code":""},{"path":"/reference/Rcpp_istaCappedL1GeneralPurposeCpp-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::istaCappedL1GeneralPurposeCpp — Rcpp_istaCappedL1GeneralPurposeCpp-class","title":"Wrapper for C++ module. See ?lessSEM::istaCappedL1GeneralPurposeCpp — Rcpp_istaCappedL1GeneralPurposeCpp-class","text":"Wrapper C++ module. See ?lessSEM::istaCappedL1GeneralPurposeCpp","code":""},{"path":"/reference/Rcpp_istaCappedL1SEM-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::istaCappedL1SEM — Rcpp_istaCappedL1SEM-class","title":"Wrapper for C++ module. See ?lessSEM::istaCappedL1SEM — Rcpp_istaCappedL1SEM-class","text":"Wrapper C++ module. See ?lessSEM::istaCappedL1SEM","code":""},{"path":"/reference/Rcpp_istaCappedL1mgSEM-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::istaCappedL1MgSEM — Rcpp_istaCappedL1mgSEM-class","title":"Wrapper for C++ module. See ?lessSEM::istaCappedL1MgSEM — Rcpp_istaCappedL1mgSEM-class","text":"Wrapper C++ module. See ?lessSEM::istaCappedL1MgSEM","code":""},{"path":"/reference/Rcpp_istaEnetGeneralPurpose-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::istaEnetGeneralPurpose — Rcpp_istaEnetGeneralPurpose-class","title":"Wrapper for C++ module. See ?lessSEM::istaEnetGeneralPurpose — Rcpp_istaEnetGeneralPurpose-class","text":"Wrapper C++ module. See ?lessSEM::istaEnetGeneralPurpose","code":""},{"path":"/reference/Rcpp_istaEnetGeneralPurposeCpp-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::istaEnetGeneralPurposeCpp — Rcpp_istaEnetGeneralPurposeCpp-class","title":"Wrapper for C++ module. See ?lessSEM::istaEnetGeneralPurposeCpp — Rcpp_istaEnetGeneralPurposeCpp-class","text":"Wrapper C++ module. See ?lessSEM::istaEnetGeneralPurposeCpp","code":""},{"path":"/reference/Rcpp_istaEnetMgSEM-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::istaEnetMgSEM — Rcpp_istaEnetMgSEM-class","title":"Wrapper for C++ module. See ?lessSEM::istaEnetMgSEM — Rcpp_istaEnetMgSEM-class","text":"Wrapper C++ module. See ?lessSEM::istaEnetMgSEM Wrapper C++ module. See ?lessSEM::istaEnetMgSEM","code":""},{"path":"/reference/Rcpp_istaEnetSEM-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::istaEnetSEM — Rcpp_istaEnetSEM-class","title":"Wrapper for C++ module. See ?lessSEM::istaEnetSEM — Rcpp_istaEnetSEM-class","text":"Wrapper C++ module. See ?lessSEM::istaEnetSEM Wrapper C++ module. See ?lessSEM::istaEnetSEM","code":""},{"path":"/reference/Rcpp_istaLSPMgSEM-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::istaLSPMgSEM — Rcpp_istaLSPMgSEM-class","title":"Wrapper for C++ module. See ?lessSEM::istaLSPMgSEM — Rcpp_istaLSPMgSEM-class","text":"Wrapper C++ module. See ?lessSEM::istaLSPMgSEM","code":""},{"path":"/reference/Rcpp_istaLSPSEM-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::istaLSPSEM — Rcpp_istaLSPSEM-class","title":"Wrapper for C++ module. See ?lessSEM::istaLSPSEM — Rcpp_istaLSPSEM-class","text":"Wrapper C++ module. See ?lessSEM::istaLSPSEM","code":""},{"path":"/reference/Rcpp_istaLspGeneralPurpose-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::istaLspGeneralPurpose — Rcpp_istaLspGeneralPurpose-class","title":"Wrapper for C++ module. See ?lessSEM::istaLspGeneralPurpose — Rcpp_istaLspGeneralPurpose-class","text":"Wrapper C++ module. See ?lessSEM::istaLspGeneralPurpose","code":""},{"path":"/reference/Rcpp_istaLspGeneralPurposeCpp-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::istaLspGeneralPurposeCpp — Rcpp_istaLspGeneralPurposeCpp-class","title":"Wrapper for C++ module. See ?lessSEM::istaLspGeneralPurposeCpp — Rcpp_istaLspGeneralPurposeCpp-class","text":"Wrapper C++ module. See ?lessSEM::istaLspGeneralPurposeCpp","code":""},{"path":"/reference/Rcpp_istaMcpGeneralPurpose-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::istaMcpGeneralPurpose — Rcpp_istaMcpGeneralPurpose-class","title":"Wrapper for C++ module. See ?lessSEM::istaMcpGeneralPurpose — Rcpp_istaMcpGeneralPurpose-class","text":"Wrapper C++ module. See ?lessSEM::istaMcpGeneralPurpose","code":""},{"path":"/reference/Rcpp_istaMcpGeneralPurposeCpp-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::istaMcpGeneralPurposeCpp — Rcpp_istaMcpGeneralPurposeCpp-class","title":"Wrapper for C++ module. See ?lessSEM::istaMcpGeneralPurposeCpp — Rcpp_istaMcpGeneralPurposeCpp-class","text":"Wrapper C++ module. See ?lessSEM::istaMcpGeneralPurposeCpp","code":""},{"path":"/reference/Rcpp_istaMcpMgSEM-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::istaMcpMgSEM — Rcpp_istaMcpMgSEM-class","title":"Wrapper for C++ module. See ?lessSEM::istaMcpMgSEM — Rcpp_istaMcpMgSEM-class","text":"Wrapper C++ module. See ?lessSEM::istaMcpMgSEM","code":""},{"path":"/reference/Rcpp_istaMcpSEM-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::istaMcpSEM — Rcpp_istaMcpSEM-class","title":"Wrapper for C++ module. See ?lessSEM::istaMcpSEM — Rcpp_istaMcpSEM-class","text":"Wrapper C++ module. See ?lessSEM::istaMcpSEM","code":""},{"path":"/reference/Rcpp_istaMixedPenaltySEM-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::istaMixedPenaltySEM — Rcpp_istaMixedPenaltySEM-class","title":"Wrapper for C++ module. See ?lessSEM::istaMixedPenaltySEM — Rcpp_istaMixedPenaltySEM-class","text":"Wrapper C++ module. See ?lessSEM::istaMixedPenaltySEM","code":""},{"path":"/reference/Rcpp_istaMixedPenaltymgSEM-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::istaMixedPenaltymgSEM — Rcpp_istaMixedPenaltymgSEM-class","title":"Wrapper for C++ module. See ?lessSEM::istaMixedPenaltymgSEM — Rcpp_istaMixedPenaltymgSEM-class","text":"Wrapper C++ module. See ?lessSEM::istaMixedPenaltymgSEM","code":""},{"path":"/reference/Rcpp_istaScadGeneralPurpose-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::istaScadGeneralPurpose — Rcpp_istaScadGeneralPurpose-class","title":"Wrapper for C++ module. See ?lessSEM::istaScadGeneralPurpose — Rcpp_istaScadGeneralPurpose-class","text":"Wrapper C++ module. See ?lessSEM::istaScadGeneralPurpose","code":""},{"path":"/reference/Rcpp_istaScadGeneralPurposeCpp-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::istaScadGeneralPurposeCpp — Rcpp_istaScadGeneralPurposeCpp-class","title":"Wrapper for C++ module. See ?lessSEM::istaScadGeneralPurposeCpp — Rcpp_istaScadGeneralPurposeCpp-class","text":"Wrapper C++ module. See ?lessSEM::istaScadGeneralPurposeCpp","code":""},{"path":"/reference/Rcpp_istaScadMgSEM-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::istaScadMgSEM — Rcpp_istaScadMgSEM-class","title":"Wrapper for C++ module. See ?lessSEM::istaScadMgSEM — Rcpp_istaScadMgSEM-class","text":"Wrapper C++ module. See ?lessSEM::istaScadMgSEM","code":""},{"path":"/reference/Rcpp_istaScadSEM-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for C++ module. See ?lessSEM::istaScadSEM — Rcpp_istaScadSEM-class","title":"Wrapper for C++ module. See ?lessSEM::istaScadSEM — Rcpp_istaScadSEM-class","text":"Wrapper C++ module. See ?lessSEM::istaScadSEM","code":""},{"path":"/reference/Rcpp_mgSEM-class.html","id":null,"dir":"Reference","previous_headings":"","what":"internal representation of SEM in C++ — Rcpp_mgSEM-class","title":"internal representation of SEM in C++ — Rcpp_mgSEM-class","text":"internal representation SEM C++","code":""},{"path":"/reference/SEMCpp.html","id":null,"dir":"Reference","previous_headings":"","what":"SEMCpp class — SEMCpp","title":"SEMCpp class — SEMCpp","text":"internal SEM representation","code":""},{"path":"/reference/SEMCpp.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"SEMCpp class — SEMCpp","text":"new Creates new SEMCpp. fill fills SEM elements Rcpp::List addTransformation adds transforamtions model implied Computes implied means covariance matrix fit Fits model. Returns objective value fitting function getParameters Returns data frame model parameters. getEstimator returns estimator used model (e.g., fiml) getParameterLabels Returns vector unique parameter labels used internally. getGradients Returns matrix scores. getScores Returns matrix scores. getHessian Returns hessian model. Expects labels parameters values parameters well boolean indicating raw. Finally, double (eps) controls precision approximation. computeTransformations compute transformations. setTransformationGradientStepSize change step size gradient computation transformations","code":""},{"path":"/reference/adaptiveLasso.html","id":null,"dir":"Reference","previous_headings":"","what":"adaptiveLasso — adaptiveLasso","title":"adaptiveLasso — adaptiveLasso","text":"Implements adaptive lasso regularization structural equation models. penalty function given : $$p( x_j) = p( x_j) = \\frac{1}{w_j}\\lambda| x_j|$$ Adaptive lasso regularization set parameters zero \\(\\lambda\\) large enough.","code":""},{"path":"/reference/adaptiveLasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"adaptiveLasso — adaptiveLasso","text":"","code":"adaptiveLasso(   lavaanModel,   regularized,   weights = NULL,   lambdas = NULL,   nLambdas = NULL,   reverse = TRUE,   curve = 1,   method = \"glmnet\",   modifyModel = lessSEM::modifyModel(),   control = lessSEM::controlGlmnet() )"},{"path":"/reference/adaptiveLasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"adaptiveLasso — adaptiveLasso","text":"lavaanModel model class lavaan regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object weights labeled vector weights parameters model. unsure parameters called, use getLavaanParameters(model) lavaan model object. set NULL, default weights used: inverse absolute values unregularized parameter estimates lambdas numeric vector: values tuning parameter lambda nLambdas alternative lambda: alpha = 1, lessSEM can automatically compute first lambda value sets regularized parameters zero. generate nLambda values 0 computed lambda. reverse set TRUE nLambdas used, lessSEM start largest lambda gradually decrease lambda. Otherwise, lessSEM start smallest lambda gradually increase . curve Allows unequally spaced lambda steps (e.g., .01,.02,.05,1,5,20). curve close 1 lambda values equally spaced, curve large lambda values concentrated close 0. See ?lessSEM::curveLambda information. method optimizer used? Currently implemented ista glmnet. ista, control argument can used switch related procedures (currently gist). modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. element generated controlIsta controlGlmnet functions. See ?controlIsta ?controlGlmnet details.","code":""},{"path":"/reference/adaptiveLasso.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"adaptiveLasso — adaptiveLasso","text":"Model class regularizedSEM","code":""},{"path":"/reference/adaptiveLasso.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"adaptiveLasso — adaptiveLasso","text":"Identical regsem, models specified using lavaan. Currently, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. Adaptive lasso regularization: Zou, H. (2006). adaptive lasso oracle properties. Journal American Statistical Association, 101(476), 1418–1429. https://doi.org/10.1198/016214506000000735 Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classification. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/adaptiveLasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"adaptiveLasso — adaptiveLasso","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +      l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +      l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Regularization:  lsem <- adaptiveLasso(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   # names of the regularized parameters:   regularized = paste0(\"l\", 6:15),   # in case of lasso and adaptive lasso, we can specify the number of lambda   # values to use. lessSEM will automatically find lambda_max and fit   # models for nLambda values between 0 and lambda_max. For the other   # penalty functions, lambdas must be specified explicitly   nLambdas = 50)  # use the plot-function to plot the regularized parameters: plot(lsem)  # the coefficients can be accessed with: coef(lsem) # if you are only interested in the estimates and not the tuning parameters, use coef(lsem)@estimates # or estimates(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters[1,]  # fit Measures: fitIndices(lsem)  # The best parameters can also be extracted with: coef(lsem, criterion = \"AIC\") # or estimates(lsem, criterion = \"AIC\")  #### Advanced ### # Switching the optimizer # # Use the \"method\" argument to switch the optimizer. The control argument # must also be changed to the corresponding function: lsemIsta <- adaptiveLasso(   lavaanModel = lavaanModel,   regularized = paste0(\"l\", 6:15),   nLambdas = 50,   method = \"ista\",   control = controlIsta())  # Note: The results are basically identical: lsemIsta@parameters - lsem@parameters"},{"path":"/reference/addCappedL1.html","id":null,"dir":"Reference","previous_headings":"","what":"addCappedL1 — addCappedL1","title":"addCappedL1 — addCappedL1","text":"Implements cappedL1 regularization structural equation models. penalty function given : $$p( x_j) = \\lambda \\min(| x_j|, \\theta)$$ \\(\\theta > 0\\). cappedL1 penalty identical lasso parameters \\(\\theta\\) identical constant parameters \\(\\theta\\). adding constant fitting function change minimum, larger parameters can stay unregularized smaller ones set zero.","code":""},{"path":"/reference/addCappedL1.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"addCappedL1 — addCappedL1","text":"","code":"addCappedL1(mixedPenalty, regularized, lambdas, thetas)"},{"path":"/reference/addCappedL1.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"addCappedL1 — addCappedL1","text":"mixedPenalty model class mixedPenalty created mixedPenalty function (see ?mixedPenalty) regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda thetas parameters whose absolute value threshold penalized constant (theta)","code":""},{"path":"/reference/addCappedL1.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"addCappedL1 — addCappedL1","text":"Model class mixedPenalty. Use fit() - function fit model","code":""},{"path":"/reference/addCappedL1.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"addCappedL1 — addCappedL1","text":"Identical regsem, models specified using lavaan. Currently, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. CappedL1 regularization: Zhang, T. (2010). Analysis Multi-stage Convex Relaxation Sparse Regularization. Journal Machine Learning Research, 11, 1081–1107. Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/addCappedL1.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"addCappedL1 — addCappedL1","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +       l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +       l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # We can add mixed penalties as follows:  regularized <- lavaanModel |>   # create template for regularized model with mixed penalty:   mixedPenalty() |>   # add penalty on loadings l6 - l10:   addCappedL1(regularized = paste0(\"l\", 11:15),            lambdas = seq(0,1,.1),           thetas = 2.3) |>   # fit the model:   fit()"},{"path":"/reference/addElasticNet.html","id":null,"dir":"Reference","previous_headings":"","what":"addElasticNet — addElasticNet","title":"addElasticNet — addElasticNet","text":"Adds elastic net penalty specified parameters. penalty function given : $$p( x_j) = \\alpha\\lambda|x_j| + (1-\\alpha)\\lambda x_j^2$$ Note elastic net combines ridge lasso regularization. \\(\\alpha = 0\\), elastic net reduces ridge regularization. \\(\\alpha = 1\\) reduces lasso regularization. , elastic net compromise shrinkage lasso ridge penalty.","code":""},{"path":"/reference/addElasticNet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"addElasticNet — addElasticNet","text":"","code":"addElasticNet(mixedPenalty, regularized, alphas, lambdas, weights = 1)"},{"path":"/reference/addElasticNet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"addElasticNet — addElasticNet","text":"mixedPenalty model class mixedPenalty created mixedPenalty function (see ?mixedPenalty) regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object alphas numeric vector: values tuning parameter alpha. Set 1 lasso zero ridge. Anything elastic net penalty. lambdas numeric vector: values tuning parameter lambda weights can used give different weights different parameters","code":""},{"path":"/reference/addElasticNet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"addElasticNet — addElasticNet","text":"Model class mixedPenalty. Use fit() - function fit model","code":""},{"path":"/reference/addElasticNet.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"addElasticNet — addElasticNet","text":"Identical regsem, models specified using lavaan. Currently, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. Elastic net regularization: Zou, H., & Hastie, T. (2005). Regularization variable selection via elastic net. Journal Royal Statistical Society: Series B, 67(2), 301–320. https://doi.org/10.1111/j.1467-9868.2005.00503.x Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classification. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/addElasticNet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"addElasticNet — addElasticNet","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +       l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +       l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # We can add mixed penalties as follows:  regularized <- lavaanModel |>   # create template for regularized model with mixed penalty:   mixedPenalty() |>   # add penalty on loadings l6 - l10:   addElasticNet(regularized = paste0(\"l\", 11:15),            lambdas = seq(0,1,.1),           alphas = .4) |>   # fit the model:   fit()"},{"path":"/reference/addLasso.html","id":null,"dir":"Reference","previous_headings":"","what":"addLasso — addLasso","title":"addLasso — addLasso","text":"Implements lasso regularization structural equation models. penalty function given : $$p( x_j) = \\lambda |x_j|$$ Lasso regularization set parameters zero \\(\\lambda\\) large enough","code":""},{"path":"/reference/addLasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"addLasso — addLasso","text":"","code":"addLasso(mixedPenalty, regularized, weights = 1, lambdas)"},{"path":"/reference/addLasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"addLasso — addLasso","text":"mixedPenalty model class mixedPenalty created mixedPenalty function (see ?mixedPenalty) regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object weights can used give different weights different parameters lambdas numeric vector: values tuning parameter lambda","code":""},{"path":"/reference/addLasso.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"addLasso — addLasso","text":"Model class mixedPenalty. Use fit() - function fit model","code":""},{"path":"/reference/addLasso.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"addLasso — addLasso","text":"Identical regsem, models specified using lavaan. Currently, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. Lasso regularization: Tibshirani, R. (1996). Regression shrinkage selection via lasso. Journal Royal Statistical Society. Series B (Methodological), 58(1), 267–288. Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classification. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/addLasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"addLasso — addLasso","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +       l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +       l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # We can add mixed penalties as follows:  regularized <- lavaanModel |>   # create template for regularized model with mixed penalty:   mixedPenalty() |>   # add penalty on loadings l6 - l10:   addLasso(regularized = paste0(\"l\", 11:15),            lambdas = seq(0,1,.1)) |>   # fit the model:   fit()"},{"path":"/reference/addLsp.html","id":null,"dir":"Reference","previous_headings":"","what":"addLsp — addLsp","title":"addLsp — addLsp","text":"Implements lsp regularization structural equation models. penalty function given : $$p( x_j) = \\lambda \\log(1 + |x_j|/\\theta)$$ \\(\\theta > 0\\).","code":""},{"path":"/reference/addLsp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"addLsp — addLsp","text":"","code":"addLsp(mixedPenalty, regularized, lambdas, thetas)"},{"path":"/reference/addLsp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"addLsp — addLsp","text":"mixedPenalty model class mixedPenalty created mixedPenalty function (see ?mixedPenalty) regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda thetas parameters whose absolute value threshold penalized constant (theta)","code":""},{"path":"/reference/addLsp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"addLsp — addLsp","text":"Model class mixedPenalty. Use fit() - function fit model","code":""},{"path":"/reference/addLsp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"addLsp — addLsp","text":"Identical regsem, models specified using lavaan. Currently, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. lsp regularization: Candès, E. J., Wakin, M. B., & Boyd, S. P. (2008). Enhancing Sparsity Reweighted l1 Minimization. Journal Fourier Analysis Applications, 14(5–6), 877–905. https://doi.org/10.1007/s00041-008-9045-x Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classification. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/addLsp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"addLsp — addLsp","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +       l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +       l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # We can add mixed penalties as follows:  regularized <- lavaanModel |>   # create template for regularized model with mixed penalty:   mixedPenalty() |>   # add penalty on loadings l6 - l10:   addLsp(regularized = paste0(\"l\", 11:15),            lambdas = seq(0,1,.1),           thetas = 2.3) |>   # fit the model:   fit()"},{"path":"/reference/addMcp.html","id":null,"dir":"Reference","previous_headings":"","what":"addMcp — addMcp","title":"addMcp — addMcp","text":"Implements mcp regularization structural equation models. penalty function given : $$p( x_j) = \\begin{cases} \\lambda |x_j| - x_j^2/(2\\theta) & \\text{} |x_j| \\leq \\theta\\lambda\\\\ \\theta\\lambda^2/2 & \\text{} |x_j| > \\lambda\\theta \\end{cases}$$ \\(\\theta > 0\\).","code":""},{"path":"/reference/addMcp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"addMcp — addMcp","text":"","code":"addMcp(mixedPenalty, regularized, lambdas, thetas)"},{"path":"/reference/addMcp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"addMcp — addMcp","text":"mixedPenalty model class mixedPenalty created mixedPenalty function (see ?mixedPenalty) regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda thetas parameters whose absolute value threshold penalized constant (theta)","code":""},{"path":"/reference/addMcp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"addMcp — addMcp","text":"Model class mixedPenalty. Use fit() - function fit model","code":""},{"path":"/reference/addMcp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"addMcp — addMcp","text":"Identical regsem, models specified using lavaan. Currently, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. mcp regularization: Zhang, C.-H. (2010). Nearly unbiased variable selection minimax concave penalty. Annals Statistics, 38(2), 894–942. https://doi.org/10.1214/09-AOS729 Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/addMcp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"addMcp — addMcp","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +       l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +       l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # We can add mixed penalties as follows:  regularized <- lavaanModel |>   # create template for regularized model with mixed penalty:   mixedPenalty() |>   # add penalty on loadings l6 - l10:   addMcp(regularized = paste0(\"l\", 11:15),            lambdas = seq(0,1,.1),           thetas = 2.3) |>   # fit the model:   fit()"},{"path":"/reference/addScad.html","id":null,"dir":"Reference","previous_headings":"","what":"addScad — addScad","title":"addScad — addScad","text":"Implements scad regularization structural equation models. penalty function given :  $$p( x_j) = \\begin{cases} \\lambda |x_j| & \\text{} |x_j| \\leq \\theta\\\\ \\frac{-x_j^2 + 2\\theta\\lambda |x_j| - \\lambda^2}{2(\\theta -1)} &  \\text{} \\lambda < |x_j| \\leq \\lambda\\theta \\\\ (\\theta + 1) \\lambda^2/2 & \\text{} |x_j| \\geq \\theta\\lambda\\\\ \\end{cases}$$ \\(\\theta > 2\\).","code":""},{"path":"/reference/addScad.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"addScad — addScad","text":"","code":"addScad(mixedPenalty, regularized, lambdas, thetas)"},{"path":"/reference/addScad.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"addScad — addScad","text":"mixedPenalty model class mixedPenalty created mixedPenalty function (see ?mixedPenalty) regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda thetas parameters whose absolute value threshold penalized constant (theta)","code":""},{"path":"/reference/addScad.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"addScad — addScad","text":"Model class mixedPenalty. Use fit() - function fit model","code":""},{"path":"/reference/addScad.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"addScad — addScad","text":"Identical regsem, models specified using lavaan. Currently, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. scad regularization: Fan, J., & Li, R. (2001). Variable selection via nonconcave penalized likelihood oracle properties. Journal American Statistical Association, 96(456), 1348–1360. https://doi.org/10.1198/016214501753382273 Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/addScad.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"addScad — addScad","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +       l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +       l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # We can add mixed penalties as follows:  regularized <- lavaanModel |>   # create template for regularized model with mixed penalty:   mixedPenalty() |>   # add penalty on loadings l6 - l10:   addScad(regularized = paste0(\"l\", 11:15),            lambdas = seq(0,1,.1),           thetas = 3.1) |>   # fit the model:   fit()"},{"path":"/reference/bfgs.html","id":null,"dir":"Reference","previous_headings":"","what":"bfgs — bfgs","title":"bfgs — bfgs","text":"function allows optimizing models built lavaan using BFGS optimizer implemented lessSEM. elements can accessed \"@\" operator (see examples). main purpose make transformations lavaan models accessible.","code":""},{"path":"/reference/bfgs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"bfgs — bfgs","text":"","code":"bfgs(   lavaanModel,   modifyModel = lessSEM::modifyModel(),   control = lessSEM::controlBFGS() )"},{"path":"/reference/bfgs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"bfgs — bfgs","text":"lavaanModel model class lavaan modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. See ?controlBFGS details.","code":""},{"path":"/reference/bfgs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"bfgs — bfgs","text":"Model class regularizedSEM","code":""},{"path":"/reference/bfgs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"bfgs — bfgs","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +      l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +      l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)   lsem <- bfgs(   # pass the fitted lavaan model   lavaanModel = lavaanModel)  # the coefficients can be accessed with: coef(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters"},{"path":"/reference/bfgsEnet.html","id":null,"dir":"Reference","previous_headings":"","what":"smoothly approximated elastic net — bfgsEnet","title":"smoothly approximated elastic net — bfgsEnet","text":"Object smoothly approximated elastic net optimization bfgs optimizer","code":""},{"path":"/reference/bfgsEnet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"smoothly approximated elastic net — bfgsEnet","text":"list fit results","code":""},{"path":"/reference/bfgsEnet.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"smoothly approximated elastic net — bfgsEnet","text":"new creates new object. Requires (1) vector weights parameter (2) list control elements setHessian changes Hessian model. Expects matrix optimize optimize model. Expects vector starting values, SEM type SEM_Cpp, lambda alpha value.","code":""},{"path":"/reference/bfgsEnetMgSEM.html","id":null,"dir":"Reference","previous_headings":"","what":"smoothly approximated elastic net — bfgsEnetMgSEM","title":"smoothly approximated elastic net — bfgsEnetMgSEM","text":"Object smoothly approximated elastic net optimization bfgs optimizer","code":""},{"path":"/reference/bfgsEnetMgSEM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"smoothly approximated elastic net — bfgsEnetMgSEM","text":"list fit results","code":""},{"path":"/reference/bfgsEnetMgSEM.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"smoothly approximated elastic net — bfgsEnetMgSEM","text":"new creates new object. Requires (1) vector weights parameter (2) list control elements setHessian changes Hessian model. Expects matrix optimize optimize model. Expects vector starting values, SEM type SEM_Cpp, lambda alpha value.","code":""},{"path":"/reference/bfgsEnetSEM.html","id":null,"dir":"Reference","previous_headings":"","what":"smoothly approximated elastic net — bfgsEnetSEM","title":"smoothly approximated elastic net — bfgsEnetSEM","text":"Object smoothly approximated elastic net optimization bfgs optimizer","code":""},{"path":"/reference/bfgsEnetSEM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"smoothly approximated elastic net — bfgsEnetSEM","text":"list fit results","code":""},{"path":"/reference/bfgsEnetSEM.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"smoothly approximated elastic net — bfgsEnetSEM","text":"new creates new object. Requires (1) vector weights parameter (2) list control elements setHessian changes Hessian model. Expects matrix optimize optimize model. Expects vector starting values, SEM type SEM_Cpp, lambda alpha value.","code":""},{"path":"/reference/callFitFunction.html","id":null,"dir":"Reference","previous_headings":"","what":"callFitFunction — callFitFunction","title":"callFitFunction — callFitFunction","text":"wrapper call user defined fit function","code":""},{"path":"/reference/callFitFunction.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"callFitFunction — callFitFunction","text":"","code":"callFitFunction(fitFunctionSEXP, parameters, userSuppliedElements)"},{"path":"/reference/callFitFunction.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"callFitFunction — callFitFunction","text":"fitFunctionSEXP pointer fit function parameters vector parameter values userSuppliedElements list additional elements","code":""},{"path":"/reference/callFitFunction.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"callFitFunction — callFitFunction","text":"fit value (double)","code":""},{"path":"/reference/cappedL1.html","id":null,"dir":"Reference","previous_headings":"","what":"cappedL1 — cappedL1","title":"cappedL1 — cappedL1","text":"Implements cappedL1 regularization structural equation models. penalty function given : $$p( x_j) = \\lambda \\min(| x_j|, \\theta)$$ \\(\\theta > 0\\). cappedL1 penalty identical lasso parameters \\(\\theta\\) identical constant parameters \\(\\theta\\). adding constant fitting function change minimum, larger parameters can stay unregularized smaller ones set zero.","code":""},{"path":"/reference/cappedL1.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"cappedL1 — cappedL1","text":"","code":"cappedL1(   lavaanModel,   regularized,   lambdas,   thetas,   modifyModel = lessSEM::modifyModel(),   method = \"glmnet\",   control = lessSEM::controlGlmnet() )"},{"path":"/reference/cappedL1.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"cappedL1 — cappedL1","text":"lavaanModel model class lavaan regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda thetas parameters whose absolute value threshold penalized constant (theta) modifyModel used modify lavaanModel. See ?modifyModel. method optimizer used? Currently implemented ista glmnet. ista, control argument can used switch related procedures control used control optimizer. element generated controlIsta (see ?controlIsta)","code":""},{"path":"/reference/cappedL1.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"cappedL1 — cappedL1","text":"Model class regularizedSEM","code":""},{"path":"/reference/cappedL1.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"cappedL1 — cappedL1","text":"Identical regsem, models specified using lavaan. Currently, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. CappedL1 regularization: Zhang, T. (2010). Analysis Multi-stage Convex Relaxation Sparse Regularization. Journal Machine Learning Research, 11, 1081–1107. Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classification. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/cappedL1.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"cappedL1 — cappedL1","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +      l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +      l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Regularization:  lsem <- cappedL1(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   # names of the regularized parameters:   regularized = paste0(\"l\", 6:15),   lambdas = seq(0,1,length.out = 20),   thetas = seq(0.01,2,length.out = 5))  # the coefficients can be accessed with: coef(lsem) # if you are only interested in the estimates and not the tuning parameters, use coef(lsem)@estimates # or estimates(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters[1,]  # fit Measures: fitIndices(lsem)  # The best parameters can also be extracted with: coef(lsem, criterion = \"AIC\") # or estimates(lsem, criterion = \"AIC\")  # optional: plotting the paths requires installation of plotly # plot(lsem)"},{"path":"/reference/coef-Rcpp_SEMCpp-method.html","id":null,"dir":"Reference","previous_headings":"","what":"coef — coef,Rcpp_SEMCpp-method","title":"coef — coef,Rcpp_SEMCpp-method","text":"coef","code":""},{"path":"/reference/coef-Rcpp_SEMCpp-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"coef — coef,Rcpp_SEMCpp-method","text":"","code":"# S4 method for Rcpp_SEMCpp coef(object, ...)"},{"path":"/reference/coef-Rcpp_SEMCpp-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"coef — coef,Rcpp_SEMCpp-method","text":"object object class Rcpp_SEMCpp ... used","code":""},{"path":"/reference/coef-Rcpp_SEMCpp-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"coef — coef,Rcpp_SEMCpp-method","text":"coefficients model transformed form","code":""},{"path":"/reference/coef-Rcpp_mgSEM-method.html","id":null,"dir":"Reference","previous_headings":"","what":"coef — coef,Rcpp_mgSEM-method","title":"coef — coef,Rcpp_mgSEM-method","text":"coef","code":""},{"path":"/reference/coef-Rcpp_mgSEM-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"coef — coef,Rcpp_mgSEM-method","text":"","code":"# S4 method for Rcpp_mgSEM coef(object, ...)"},{"path":"/reference/coef-Rcpp_mgSEM-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"coef — coef,Rcpp_mgSEM-method","text":"object object class Rcpp_mgSEM ... used","code":""},{"path":"/reference/coef-Rcpp_mgSEM-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"coef — coef,Rcpp_mgSEM-method","text":"coefficients model transformed form","code":""},{"path":"/reference/coef-cvRegularizedSEM-method.html","id":null,"dir":"Reference","previous_headings":"","what":"coef — coef,cvRegularizedSEM-method","title":"coef — coef,cvRegularizedSEM-method","text":"Returns parameter estimates cvRegularizedSEM","code":""},{"path":"/reference/coef-cvRegularizedSEM-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"coef — coef,cvRegularizedSEM-method","text":"","code":"# S4 method for cvRegularizedSEM coef(object, ...)"},{"path":"/reference/coef-cvRegularizedSEM-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"coef — coef,cvRegularizedSEM-method","text":"object object class cvRegularizedSEM ... used","code":""},{"path":"/reference/coef-cvRegularizedSEM-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"coef — coef,cvRegularizedSEM-method","text":"parameter estimates cvRegularizedSEM","code":""},{"path":"/reference/coef-gpRegularized-method.html","id":null,"dir":"Reference","previous_headings":"","what":"coef — coef,gpRegularized-method","title":"coef — coef,gpRegularized-method","text":"Returns parameter estimates gpRegularized","code":""},{"path":"/reference/coef-gpRegularized-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"coef — coef,gpRegularized-method","text":"","code":"# S4 method for gpRegularized coef(object, ...)"},{"path":"/reference/coef-gpRegularized-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"coef — coef,gpRegularized-method","text":"object object class gpRegularized ... criterion can one : \"AIC\", \"BIC\". set NULL, parameters returned","code":""},{"path":"/reference/coef-gpRegularized-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"coef — coef,gpRegularized-method","text":"parameter estimates","code":""},{"path":"/reference/coef-regularizedSEM-method.html","id":null,"dir":"Reference","previous_headings":"","what":"coef — coef,regularizedSEM-method","title":"coef — coef,regularizedSEM-method","text":"Returns parameter estimates regularizedSEM","code":""},{"path":"/reference/coef-regularizedSEM-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"coef — coef,regularizedSEM-method","text":"","code":"# S4 method for regularizedSEM coef(object, ...)"},{"path":"/reference/coef-regularizedSEM-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"coef — coef,regularizedSEM-method","text":"object object class regularizedSEM ... criterion can one ones returned fitIndices. set NULL, parameters returned","code":""},{"path":"/reference/coef-regularizedSEM-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"coef — coef,regularizedSEM-method","text":"parameters model data.frame","code":""},{"path":"/reference/coef-regularizedSEMMixedPenalty-method.html","id":null,"dir":"Reference","previous_headings":"","what":"coef — coef,regularizedSEMMixedPenalty-method","title":"coef — coef,regularizedSEMMixedPenalty-method","text":"Returns parameter estimates regularizedSEMMixedPenalty","code":""},{"path":"/reference/coef-regularizedSEMMixedPenalty-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"coef — coef,regularizedSEMMixedPenalty-method","text":"","code":"# S4 method for regularizedSEMMixedPenalty coef(object, ...)"},{"path":"/reference/coef-regularizedSEMMixedPenalty-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"coef — coef,regularizedSEMMixedPenalty-method","text":"object object class regularizedSEMMixedPenalty ... criterion can one : \"AIC\", \"BIC\". set NULL, parameters returned","code":""},{"path":"/reference/coef-regularizedSEMMixedPenalty-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"coef — coef,regularizedSEMMixedPenalty-method","text":"parameters model data.frame","code":""},{"path":"/reference/coef-regularizedSEMWithCustomPenalty-method.html","id":null,"dir":"Reference","previous_headings":"","what":"coef — coef,regularizedSEMWithCustomPenalty-method","title":"coef — coef,regularizedSEMWithCustomPenalty-method","text":"Returns parameter estimates regularizedSEMWithCustomPenalty","code":""},{"path":"/reference/coef-regularizedSEMWithCustomPenalty-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"coef — coef,regularizedSEMWithCustomPenalty-method","text":"","code":"# S4 method for regularizedSEMWithCustomPenalty coef(object, ...)"},{"path":"/reference/coef-regularizedSEMWithCustomPenalty-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"coef — coef,regularizedSEMWithCustomPenalty-method","text":"object object class regularizedSEMWithCustomPenalty ... used","code":""},{"path":"/reference/coef-regularizedSEMWithCustomPenalty-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"coef — coef,regularizedSEMWithCustomPenalty-method","text":"data.frame parameter estimates","code":""},{"path":"/reference/controlBFGS.html","id":null,"dir":"Reference","previous_headings":"","what":"controlBFGS — controlBFGS","title":"controlBFGS — controlBFGS","text":"Control BFGS optimizer.","code":""},{"path":"/reference/controlBFGS.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"controlBFGS — controlBFGS","text":"","code":"controlBFGS(   startingValues = \"est\",   initialHessian = ifelse(all(startingValues == \"est\"), \"lavaan\", \"compute\"),   saveDetails = FALSE,   stepSize = 0.9,   sigma = 1e-05,   gamma = 0,   maxIterOut = 1000,   maxIterIn = 1000,   maxIterLine = 500,   breakOuter = 1e-08,   breakInner = 1e-10,   convergenceCriterion = 0,   verbose = 0,   nCores = 1 )"},{"path":"/reference/controlBFGS.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"controlBFGS — controlBFGS","text":"startingValues option provide initial starting values. used first lambda. Three options supported. Setting \"est\" use estimates lavaan model object. Setting \"start\" use starting values lavaan model. Finally, labeled vector parameter values can passed function used starting values. initialHessian option provide initial Hessian optimizer. Must row column names corresponding parameter labels. use getLavaanParameters(lavaanModel) see labels. set \"gradNorm\", maximum gradients starting values times stepSize used. adapted Optim.jl https://github.com/JuliaNLSolvers/Optim.jl/blob/f43e6084aacf2dabb2b142952acd3fbb0e268439/src/multivariate/solvers/first_order/bfgs.jl#L104 set single value, diagonal matrix single value along diagonal used. default \"lavaan\" extracts Hessian lavaanModel. Hessian typically deviate internal SEM represenation lessSEM (due transformation variances), works quite well practice. saveDetails set TRUE, additional details individual models save. Currently, Hessian implied means covariances. Note: may take lot memory! stepSize Initial stepSize outer iteration (theta_next = theta_previous + stepSize * Stepdirection) sigma relevant lineSearch = 'GLMNET'. Controls sigma parameter Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421. gamma Controls gamma parameter Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421. Defaults 0. maxIterOut Maximal number outer iterations maxIterIn Maximal number inner iterations maxIterLine Maximal number iterations line search procedure breakOuter Stopping criterion outer iterations breakInner Stopping criterion inner iterations convergenceCriterion convergence criterion used outer iterations? possible 0 = GLMNET, 1 = fitChange, 2 = gradients. Note case gradients GLMNET, divide gradients (Hessian) log-Likelihood N otherwise considerably difficult larger sample sizes reach convergence criteria. verbose 0 prints additional information, > 0 prints GLMNET iterations nCores number core use. Multi-core support provided RcppParallel supported SEM, general purpose optimization.","code":""},{"path":"/reference/controlBFGS.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"controlBFGS — controlBFGS","text":"object class controlBFGS","code":""},{"path":"/reference/controlBFGS.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"controlBFGS — controlBFGS","text":"","code":"control <- controlBFGS()"},{"path":"/reference/controlGlmnet.html","id":null,"dir":"Reference","previous_headings":"","what":"controlGlmnet — controlGlmnet","title":"controlGlmnet — controlGlmnet","text":"Control GLMNET optimizer.","code":""},{"path":"/reference/controlGlmnet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"controlGlmnet — controlGlmnet","text":"","code":"controlGlmnet(   startingValues = \"est\",   initialHessian = ifelse(all(startingValues == \"est\"), \"lavaan\", \"compute\"),   saveDetails = FALSE,   stepSize = 0.9,   sigma = 1e-05,   gamma = 0,   maxIterOut = 1000,   maxIterIn = 1000,   maxIterLine = 500,   breakOuter = 1e-08,   breakInner = 1e-10,   convergenceCriterion = 0,   verbose = 0,   nCores = 1 )"},{"path":"/reference/controlGlmnet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"controlGlmnet — controlGlmnet","text":"startingValues option provide initial starting values. used first lambda. Three options supported. Setting \"est\" use estimates lavaan model object. Setting \"start\" use starting values lavaan model. Finally, labeled vector parameter values can passed function used starting values. initialHessian option provide initial Hessian optimizer. Must row column names corresponding parameter labels. use getLavaanParameters(lavaanModel) see labels. set \"gradNorm\", maximum gradients starting values times stepSize used. adapted Optim.jl https://github.com/JuliaNLSolvers/Optim.jl/blob/f43e6084aacf2dabb2b142952acd3fbb0e268439/src/multivariate/solvers/first_order/bfgs.jl#L104 set \"compute\", initial hessian computed. set single value, diagonal matrix single value along diagonal used. default \"lavaan\" extracts Hessian lavaanModel. Hessian typically deviate internal SEM represenation lessSEM (due transformation variances), works quite well practice. saveDetails set TRUE, additional details individual models save. Currently, Hessian implied means covariances. Note: may take lot memory! stepSize Initial stepSize outer iteration (theta_next = theta_previous + stepSize * Stepdirection) sigma relevant lineSearch = 'GLMNET'. Controls sigma parameter Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421. gamma Controls gamma parameter Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421. Defaults 0. maxIterOut Maximal number outer iterations maxIterIn Maximal number inner iterations maxIterLine Maximal number iterations line search procedure breakOuter Stopping criterion outer iterations breakInner Stopping criterion inner iterations convergenceCriterion convergence criterion used outer iterations? possible 0 = GLMNET, 1 = fitChange, 2 = gradients. Note case gradients GLMNET, divide gradients (Hessian) log-Likelihood N otherwise considerably difficult larger sample sizes reach convergence criteria. verbose 0 prints additional information, > 0 prints GLMNET iterations nCores number core use. Multi-core support provided RcppParallel supported SEM, general purpose optimization.","code":""},{"path":"/reference/controlGlmnet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"controlGlmnet — controlGlmnet","text":"object class controlGlmnet","code":""},{"path":"/reference/controlGlmnet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"controlGlmnet — controlGlmnet","text":"","code":"control <- controlGlmnet()"},{"path":"/reference/controlIsta.html","id":null,"dir":"Reference","previous_headings":"","what":"controlIsta — controlIsta","title":"controlIsta — controlIsta","text":"controlIsta","code":""},{"path":"/reference/controlIsta.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"controlIsta — controlIsta","text":"","code":"controlIsta(   startingValues = \"est\",   saveDetails = FALSE,   L0 = 0.1,   eta = 2,   accelerate = TRUE,   maxIterOut = 10000,   maxIterIn = 1000,   breakOuter = 1e-08,   convCritInner = 1,   sigma = 0.1,   stepSizeInheritance = ifelse(accelerate, 1, 3),   verbose = 0,   nCores = 1 )"},{"path":"/reference/controlIsta.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"controlIsta — controlIsta","text":"startingValues option provide initial starting values. used first lambda. Three options supported. Setting \"est\" use estimates lavaan model object. Setting \"start\" use starting values lavaan model. Finally, labeled vector parameter values can passed function used starting values. saveDetails set TRUE, additional details individual models save. Currently, implied means covariances. Note: may take lot memory! L0 L0 controls step size used first iteration eta eta controls much step size changes inner iterations (eta^)*L, inner iteration accelerate boolean: acceleration outlined Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231., p. 152 used? maxIterOut maximal number outer iterations maxIterIn maximal number inner iterations breakOuter change fit required break outer iteration. Note: value multiplied internally sample size N -2log-Likelihood depends directly sample size convCritInner related inner breaking condition. 0 = ista, presented Beck & Teboulle (2009); see Remark 3.1 p. 191 (ISTA backtracking) 1 = gist, presented Gong et al. (2013) (Equation 3) sigma sigma (0,1) used gist convergence criterion. larger sigma enforce larger improvement fit stepSizeInheritance step sizes carried forward iteration iteration? 0 = resets step size L0 iteration 1 = takes previous step size initial value next iteration 3 = Barzilai-Borwein procedure 4 = Barzilai-Borwein procedure, sometimes resets step size; can help optimizer caught bad spot. verbose set value > 0, fit every \"verbose\" iterations printed. nCores number core use. Multi-core support provided RcppParallel supported SEM, general purpose optimization.","code":""},{"path":"/reference/controlIsta.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"controlIsta — controlIsta","text":"object class controlIsta","code":""},{"path":"/reference/controlIsta.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"controlIsta — controlIsta","text":"","code":"control <- controlIsta()"},{"path":"/reference/covariances.html","id":null,"dir":"Reference","previous_headings":"","what":"covariances — covariances","title":"covariances — covariances","text":"Extract labels covariances found lavaan model.","code":""},{"path":"/reference/covariances.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"covariances — covariances","text":"","code":"covariances(lavaanModel)"},{"path":"/reference/covariances.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"covariances — covariances","text":"lavaanModel fitted lavaan model","code":""},{"path":"/reference/covariances.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"covariances — covariances","text":"vector parameter labels","code":""},{"path":"/reference/covariances.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"covariances — covariances","text":"","code":"# The following is adapted from ?lavaan::sem library(lessSEM) model <- '    # latent variable definitions   ind60 =~ x1 + x2 + x3   dem60 =~ y1 + a*y2 + b*y3 + c*y4   dem65 =~ y5 + a*y6 + b*y7 + c*y8    # regressions   dem60 ~ ind60   dem65 ~ ind60 + dem60    # residual correlations   y1 ~~ y5   y2 ~~ y4 + y6   y3 ~~ y7   y4 ~~ y8   y6 ~~ y8 '  fit <- sem(model, data = PoliticalDemocracy)  covariances(fit)"},{"path":"/reference/createSubsets.html","id":null,"dir":"Reference","previous_headings":"","what":"createSubsets — createSubsets","title":"createSubsets — createSubsets","text":"create subsets cross-validation","code":""},{"path":"/reference/createSubsets.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"createSubsets — createSubsets","text":"","code":"createSubsets(N, k)"},{"path":"/reference/createSubsets.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"createSubsets — createSubsets","text":"N number samples data set k number subsets create","code":""},{"path":"/reference/createSubsets.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"createSubsets — createSubsets","text":"matrix subsets","code":""},{"path":"/reference/createSubsets.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"createSubsets — createSubsets","text":"","code":"createSubsets(N=100, k = 5)"},{"path":"/reference/curveLambda.html","id":null,"dir":"Reference","previous_headings":"","what":"curveLambda — curveLambda","title":"curveLambda — curveLambda","text":"generates lambda values 0 lambdaMax using function described : https://math.stackexchange.com/questions/384613/exponential-function--values--0--1--x-values--0--1. function identical one implemented regCtsem package.","code":""},{"path":"/reference/curveLambda.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"curveLambda — curveLambda","text":"","code":"curveLambda(maxLambda, lambdasAutoCurve, lambdasAutoLength)"},{"path":"/reference/curveLambda.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"curveLambda — curveLambda","text":"maxLambda maximal lambda value lambdasAutoCurve controls curve. value close 1 result linear increase, larger values lambdas concentrated around 0 lambdasAutoLength number lambda values generate","code":""},{"path":"/reference/curveLambda.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"curveLambda — curveLambda","text":"numeric vector","code":""},{"path":"/reference/curveLambda.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"curveLambda — curveLambda","text":"","code":"library(lessSEM) plot(curveLambda(maxLambda = 10, lambdasAutoCurve = 1, lambdasAutoLength = 100)) plot(curveLambda(maxLambda = 10, lambdasAutoCurve = 5, lambdasAutoLength = 100)) plot(curveLambda(maxLambda = 10, lambdasAutoCurve = 100, lambdasAutoLength = 100))"},{"path":"/reference/cvAdaptiveLasso.html","id":null,"dir":"Reference","previous_headings":"","what":"cvAdaptiveLasso — cvAdaptiveLasso","title":"cvAdaptiveLasso — cvAdaptiveLasso","text":"Implements cross-validated adaptive lasso regularization structural equation models. penalty function given : $$p( x_j) = p( x_j) = \\frac{1}{w_j}\\lambda| x_j|$$ Adaptive lasso regularization set parameters zero \\(\\lambda\\) large enough.","code":""},{"path":"/reference/cvAdaptiveLasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"cvAdaptiveLasso — cvAdaptiveLasso","text":"","code":"cvAdaptiveLasso(   lavaanModel,   regularized,   weights = NULL,   lambdas,   k = 5,   standardize = FALSE,   returnSubsetParameters = FALSE,   method = \"glmnet\",   modifyModel = lessSEM::modifyModel(),   control = lessSEM::controlGlmnet() )"},{"path":"/reference/cvAdaptiveLasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"cvAdaptiveLasso — cvAdaptiveLasso","text":"lavaanModel model class lavaan regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object weights labeled vector weights parameters model. unsure parameters called, use getLavaanParameters(model) lavaan model object. set NULL, default weights used: inverse absolute values unregularized parameter estimates lambdas numeric vector: values tuning parameter lambda k number cross-validation folds. Alternatively, can pass matrix booleans (TRUE, FALSE) indicates person subset belongs . See ?lessSEM::createSubsets example matrix look like. standardize Standardizing data prior analysis can undermine cross- validation. Set standardize=TRUE automatically standardize data. returnSubsetParameters set TRUE return parameters training set method optimizer used? Currently implemented ista glmnet. ista, control argument can used switch related procedures (currently gist). modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. element generated controlIsta controlGlmnet functions. See ?controlIsta ?controlGlmnet details.","code":""},{"path":"/reference/cvAdaptiveLasso.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"cvAdaptiveLasso — cvAdaptiveLasso","text":"model class cvRegularizedSEM","code":""},{"path":"/reference/cvAdaptiveLasso.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"cvAdaptiveLasso — cvAdaptiveLasso","text":"Identical regsem, models specified using lavaan. Currenlty, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. Adaptive lasso regularization: Zou, H. (2006). adaptive lasso oracle properties. Journal American Statistical Association, 101(476), 1418–1429. https://doi.org/10.1198/016214506000000735 Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classification. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/cvAdaptiveLasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"cvAdaptiveLasso — cvAdaptiveLasso","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +      l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +      l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Regularization:  lsem <- cvAdaptiveLasso(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   # names of the regularized parameters:   regularized = paste0(\"l\", 6:15),   lambdas = seq(0,1,.1))  # use the plot-function to plot the cross-validation fit plot(lsem)  # the coefficients can be accessed with: coef(lsem) # if you are only interested in the estimates and not the tuning parameters, use coef(lsem)@estimates # or estimates(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters  # The best parameters can also be extracted with: estimates(lsem)"},{"path":"/reference/cvCappedL1.html","id":null,"dir":"Reference","previous_headings":"","what":"cvCappedL1 — cvCappedL1","title":"cvCappedL1 — cvCappedL1","text":"Implements cappedL1 regularization structural equation models. penalty function given : $$p( x_j) = \\lambda \\min(| x_j|, \\theta)$$ \\(\\theta > 0\\). cappedL1 penalty identical lasso parameters \\(\\theta\\) identical constant parameters \\(\\theta\\). adding constant fitting function change minimum, larger parameters can stay unregularized smaller ones set zero.","code":""},{"path":"/reference/cvCappedL1.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"cvCappedL1 — cvCappedL1","text":"","code":"cvCappedL1(   lavaanModel,   regularized,   lambdas,   thetas,   k = 5,   standardize = FALSE,   returnSubsetParameters = FALSE,   modifyModel = lessSEM::modifyModel(),   method = \"glmnet\",   control = lessSEM::controlGlmnet() )"},{"path":"/reference/cvCappedL1.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"cvCappedL1 — cvCappedL1","text":"lavaanModel model class lavaan regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda thetas parameters whose absolute value threshold penalized constant (theta) k number cross-validation folds. Alternatively, can pass matrix booleans (TRUE, FALSE) indicates person subset belongs . See ?lessSEM::createSubsets example matrix look like. standardize Standardizing data prior analysis can undermine cross- validation. Set standardize=TRUE automatically standardize data. returnSubsetParameters set TRUE return parameters training set modifyModel used modify lavaanModel. See ?modifyModel. method optimizer used? Currently implemented ista glmnet. ista, control argument can used switch related procedures. control used control optimizer. element generated controlIsta function. See ?controlIsta details.","code":""},{"path":"/reference/cvCappedL1.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"cvCappedL1 — cvCappedL1","text":"model class cvRegularizedSEM","code":""},{"path":"/reference/cvCappedL1.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"cvCappedL1 — cvCappedL1","text":"Identical regsem, models specified using lavaan. Currenlty, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. CappedL1 regularization: Zhang, T. (2010). Analysis Multi-stage Convex Relaxation Sparse Regularization. Journal Machine Learning Research, 11, 1081–1107. Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classification. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/cvCappedL1.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"cvCappedL1 — cvCappedL1","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +      l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +      l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Regularization:  lsem <- cvCappedL1(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   # names of the regularized parameters:   regularized = paste0(\"l\", 6:15),   lambdas = seq(0,1,length.out = 5),   thetas = seq(0.01,2,length.out = 3))  # the coefficients can be accessed with: coef(lsem) # if you are only interested in the estimates and not the tuning parameters, use coef(lsem)@estimates # or estimates(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters  # optional: plotting the cross-validation fit requires installation of plotly # plot(lsem)"},{"path":"/reference/cvElasticNet.html","id":null,"dir":"Reference","previous_headings":"","what":"cvElasticNet — cvElasticNet","title":"cvElasticNet — cvElasticNet","text":"Implements elastic net regularization structural equation models. penalty function given : $$p( x_j) = \\alpha\\lambda| x_j| + (1-\\alpha)\\lambda x_j^2$$ Note elastic net combines ridge lasso regularization. \\(\\alpha = 0\\), elastic net reduces ridge regularization. \\(\\alpha = 1\\) reduces lasso regularization. , elastic net compromise shrinkage lasso ridge penalty.","code":""},{"path":"/reference/cvElasticNet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"cvElasticNet — cvElasticNet","text":"","code":"cvElasticNet(   lavaanModel,   regularized,   lambdas,   alphas,   k = 5,   standardize = FALSE,   returnSubsetParameters = FALSE,   method = \"glmnet\",   modifyModel = lessSEM::modifyModel(),   control = lessSEM::controlGlmnet() )"},{"path":"/reference/cvElasticNet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"cvElasticNet — cvElasticNet","text":"lavaanModel model class lavaan regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda alphas numeric vector values tuning parameter alpha. Must 0 1. 0 = ridge, 1 = lasso. k number cross-validation folds. Alternatively, can pass matrix booleans (TRUE, FALSE) indicates person subset belongs . See ?lessSEM::createSubsets example matrix look like. standardize Standardizing data prior analysis can undermine cross- validation. Set standardize=TRUE automatically standardize data. returnSubsetParameters set TRUE return parameters training set method optimizer used? Currently implemented ista glmnet. ista, control argument can used switch related procedures. modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. element generated controlIsta controlGlmnet functions. See ?controlIsta ?controlGlmnet details.","code":""},{"path":"/reference/cvElasticNet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"cvElasticNet — cvElasticNet","text":"model class cvRegularizedSEM","code":""},{"path":"/reference/cvElasticNet.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"cvElasticNet — cvElasticNet","text":"Identical regsem, models specified using lavaan. Currenlty, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. Elastic net regularization: Zou, H., & Hastie, T. (2005). Regularization variable selection via elastic net. Journal Royal Statistical Society: Series B, 67(2), 301–320. https://doi.org/10.1111/j.1467-9868.2005.00503.x Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classification. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/cvElasticNet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"cvElasticNet — cvElasticNet","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +      l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +      l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Regularization:  lsem <- cvElasticNet(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   # names of the regularized parameters:   regularized = paste0(\"l\", 6:15),   lambdas = seq(0,1,length.out = 5),   alphas = seq(0,1,length.out = 3))  # the coefficients can be accessed with: coef(lsem) # if you are only interested in the estimates and not the tuning parameters, use coef(lsem)@estimates # or estimates(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters  # optional: plotting the cross-validation fit requires installation of plotly # plot(lsem)"},{"path":"/reference/cvLasso.html","id":null,"dir":"Reference","previous_headings":"","what":"cvLasso — cvLasso","title":"cvLasso — cvLasso","text":"Implements cross-validated lasso regularization structural equation models. penalty function given : $$p( x_j) = \\lambda |x_j|$$ Lasso regularization set parameters zero \\(\\lambda\\) large enough","code":""},{"path":"/reference/cvLasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"cvLasso — cvLasso","text":"","code":"cvLasso(   lavaanModel,   regularized,   lambdas,   k = 5,   standardize = FALSE,   returnSubsetParameters = FALSE,   method = \"glmnet\",   modifyModel = lessSEM::modifyModel(),   control = lessSEM::controlGlmnet() )"},{"path":"/reference/cvLasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"cvLasso — cvLasso","text":"lavaanModel model class lavaan regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda k number cross-validation folds. Alternatively, can pass matrix booleans (TRUE, FALSE) indicates person subset belongs . See ?lessSEM::createSubsets example matrix look like. standardize Standardizing data prior analysis can undermine cross- validation. Set standardize=TRUE automatically standardize data. returnSubsetParameters set TRUE return parameters training set method optimizer used? Currently implemented ista glmnet. ista, control argument can used switch related procedures. modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. element generated controlIsta controlGlmnet functions. See ?controlIsta ?controlGlmnet details.","code":""},{"path":"/reference/cvLasso.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"cvLasso — cvLasso","text":"model class cvRegularizedSEM","code":""},{"path":"/reference/cvLasso.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"cvLasso — cvLasso","text":"Identical regsem, models specified using lavaan. Currently, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. Lasso regularization: Tibshirani, R. (1996). Regression shrinkage selection via lasso. Journal Royal Statistical Society. Series B (Methodological), 58(1), 267–288. Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classification. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/cvLasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"cvLasso — cvLasso","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +       l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +       l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Regularization:  lsem <- cvLasso(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   # names of the regularized parameters:   regularized = paste0(\"l\", 6:15),   lambdas = seq(0,1,.1),   k = 5, # number of cross-validation folds   standardize = TRUE) # automatic standardization  # use the plot-function to plot the cross-validation fit: plot(lsem)  # the coefficients can be accessed with: coef(lsem) # if you are only interested in the estimates and not the tuning parameters, use coef(lsem)@estimates # or estimates(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters  # The best parameters can also be extracted with: estimates(lsem)"},{"path":"/reference/cvLsp.html","id":null,"dir":"Reference","previous_headings":"","what":"cvLsp — cvLsp","title":"cvLsp — cvLsp","text":"Implements lsp regularization structural equation models. penalty function given : $$p( x_j) = \\lambda \\log(1 + |x_j|/\\theta)$$ \\(\\theta > 0\\).","code":""},{"path":"/reference/cvLsp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"cvLsp — cvLsp","text":"","code":"cvLsp(   lavaanModel,   regularized,   lambdas,   thetas,   k = 5,   standardize = FALSE,   returnSubsetParameters = FALSE,   modifyModel = lessSEM::modifyModel(),   method = \"glmnet\",   control = lessSEM::controlGlmnet() )"},{"path":"/reference/cvLsp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"cvLsp — cvLsp","text":"lavaanModel model class lavaan regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda thetas parameters whose absolute value threshold penalized constant (theta) k number cross-validation folds. Alternatively, can pass matrix booleans (TRUE, FALSE) indicates person subset belongs . See ?lessSEM::createSubsets example matrix look like. standardize Standardizing data prior analysis can undermine cross- validation. Set standardize=TRUE automatically standardize data. returnSubsetParameters set TRUE return parameters training set modifyModel used modify lavaanModel. See ?modifyModel. method optimizer used? Currently implemented ista glmnet. ista, control argument can used switch related procedures. control used control optimizer. element generated controlIsta function. See ?controlIsta","code":""},{"path":"/reference/cvLsp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"cvLsp — cvLsp","text":"model class cvRegularizedSEM","code":""},{"path":"/reference/cvLsp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"cvLsp — cvLsp","text":"Identical regsem, models specified using lavaan. Currenlty, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. lsp regularization: Candès, E. J., Wakin, M. B., & Boyd, S. P. (2008). Enhancing Sparsity Reweighted l1 Minimization. Journal Fourier Analysis Applications, 14(5–6), 877–905. https://doi.org/10.1007/s00041-008-9045-x Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classification. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/cvLsp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"cvLsp — cvLsp","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +      l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +      l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Regularization:  lsem <- cvLsp(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   # names of the regularized parameters:   regularized = paste0(\"l\", 6:15),   lambdas = seq(0,1,length.out = 5),   thetas = seq(0.01,2,length.out = 3))  # the coefficients can be accessed with: coef(lsem) # if you are only interested in the estimates and not the tuning parameters, use coef(lsem)@estimates # or estimates(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters  # optional: plotting the cross-validation fit requires installation of plotly # plot(lsem)"},{"path":"/reference/cvMcp.html","id":null,"dir":"Reference","previous_headings":"","what":"cvMcp — cvMcp","title":"cvMcp — cvMcp","text":"Implements mcp regularization structural equation models. penalty function given : $$p( x_j) = \\begin{cases} \\lambda |x_j| - x_j^2/(2\\theta) & \\text{} |x_j| \\leq \\theta\\lambda\\\\ \\theta\\lambda^2/2 & \\text{} |x_j| > \\lambda\\theta \\end{cases}$$ \\(\\theta > 0\\).","code":""},{"path":"/reference/cvMcp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"cvMcp — cvMcp","text":"","code":"cvMcp(   lavaanModel,   regularized,   lambdas,   thetas,   k = 5,   standardize = FALSE,   returnSubsetParameters = FALSE,   modifyModel = lessSEM::modifyModel(),   method = \"ista\",   control = lessSEM::controlIsta() )"},{"path":"/reference/cvMcp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"cvMcp — cvMcp","text":"lavaanModel model class lavaan regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda thetas parameters whose absolute value threshold penalized constant (theta) k number cross-validation folds. Alternatively, can pass matrix booleans (TRUE, FALSE) indicates person subset belongs . See ?lessSEM::createSubsets example matrix look like. standardize Standardizing data prior analysis can undermine cross- validation. Set standardize=TRUE automatically standardize data. returnSubsetParameters set TRUE return parameters training set modifyModel used modify lavaanModel. See ?modifyModel. method optimizer used? Currently implemented ista glmnet. ista, control argument can used switch related procedures. control used control optimizer. element generated controlIsta function. See ?controlIsta","code":""},{"path":"/reference/cvMcp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"cvMcp — cvMcp","text":"model class cvRegularizedSEM","code":""},{"path":"/reference/cvMcp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"cvMcp — cvMcp","text":"Identical regsem, models specified using lavaan. Currenlty, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. mcp regularization: Zhang, C.-H. (2010). Nearly unbiased variable selection minimax concave penalty. Annals Statistics, 38(2), 894–942. https://doi.org/10.1214/09-AOS729 Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classification. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/cvMcp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"cvMcp — cvMcp","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +      l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +      l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Regularization:  lsem <- cvMcp(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   # names of the regularized parameters:   regularized = paste0(\"l\", 6:15),   lambdas = seq(0,1,length.out = 5),   thetas = seq(0.01,2,length.out = 3))  # the coefficients can be accessed with: coef(lsem) # if you are only interested in the estimates and not the tuning parameters, use coef(lsem)@estimates # or estimates(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters  # optional: plotting the cross-validation fit requires installation of plotly # plot(lsem)"},{"path":"/reference/cvRegularizedSEM-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Class for cross-validated regularized SEM — cvRegularizedSEM-class","title":"Class for cross-validated regularized SEM — cvRegularizedSEM-class","text":"Class cross-validated regularized SEM","code":""},{"path":"/reference/cvRegularizedSEM-class.html","id":"slots","dir":"Reference","previous_headings":"","what":"Slots","title":"Class for cross-validated regularized SEM — cvRegularizedSEM-class","text":"parameters data.frame parameter estimates best combination tuning parameters transformations transformed parameters cvfits data.frame combinations tuning parameters sum cross-validation fits parameterLabels character vector names parameters regularized character vector names regularized parameters cvfitsDetails data.frame cross-validation fits subset subsets matrix indicating person subset subsetParameters optional: data.frame parameter estimates combinations tuning parameters subsets misc list additional return elements notes internal notes come fitting model","code":""},{"path":"/reference/cvRidge.html","id":null,"dir":"Reference","previous_headings":"","what":"cvRidge — cvRidge","title":"cvRidge — cvRidge","text":"Implements ridge regularization structural equation models. penalty function given : $$p( x_j) = \\lambda x_j^2$$ Note ridge regularization set parameters zero result shrinkage towards zero.","code":""},{"path":"/reference/cvRidge.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"cvRidge — cvRidge","text":"","code":"cvRidge(   lavaanModel,   regularized,   lambdas,   k = 5,   standardize = FALSE,   returnSubsetParameters = FALSE,   method = \"glmnet\",   modifyModel = lessSEM::modifyModel(),   control = lessSEM::controlGlmnet() )"},{"path":"/reference/cvRidge.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"cvRidge — cvRidge","text":"lavaanModel model class lavaan regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda k number cross-validation folds. Alternatively, can pass matrix booleans (TRUE, FALSE) indicates person subset belongs . See ?lessSEM::createSubsets example matrix look like. standardize Standardizing data prior analysis can undermine cross- validation. Set standardize=TRUE automatically standardize data. returnSubsetParameters set TRUE return parameters training set method optimizer used? Currently implemented ista glmnet. ista, control argument can used switch related procedures (currently gist). modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. element generated controlIsta controlGlmnet functions. See ?controlIsta ?controlGlmnet details.","code":""},{"path":"/reference/cvRidge.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"cvRidge — cvRidge","text":"model class cvRegularizedSEM","code":""},{"path":"/reference/cvRidge.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"cvRidge — cvRidge","text":"Identical regsem, models specified using lavaan. Currenlty, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. Ridge regularization: Hoerl, . E., & Kennard, R. W. (1970). Ridge Regression: Biased Estimation Nonorthogonal Problems. Technometrics, 12(1), 55–67. https://doi.org/10.1080/00401706.1970.10488634 Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classification. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/cvRidge.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"cvRidge — cvRidge","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +      l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +      l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Regularization:  lsem <- cvRidge(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   # names of the regularized parameters:   regularized = paste0(\"l\", 6:15),   lambdas = seq(0,1,length.out = 20))  # use the plot-function to plot the cross-validation fit: plot(lsem)  # the coefficients can be accessed with: coef(lsem) # if you are only interested in the estimates and not the tuning parameters, use coef(lsem)@estimates # or estimates(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters"},{"path":"/reference/cvRidgeBfgs.html","id":null,"dir":"Reference","previous_headings":"","what":"cvRidgeBfgs — cvRidgeBfgs","title":"cvRidgeBfgs — cvRidgeBfgs","text":"Implements cross-validated ridge regularization structural equation models. penalty function given : $$p( x_j) = \\lambda x_j^2$$ Note ridge regularization set parameters zero result shrinkage towards zero.","code":""},{"path":"/reference/cvRidgeBfgs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"cvRidgeBfgs — cvRidgeBfgs","text":"","code":"cvRidgeBfgs(   lavaanModel,   regularized,   lambdas,   k = 5,   standardize = FALSE,   returnSubsetParameters = FALSE,   modifyModel = lessSEM::modifyModel(),   control = lessSEM::controlBFGS() )"},{"path":"/reference/cvRidgeBfgs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"cvRidgeBfgs — cvRidgeBfgs","text":"lavaanModel model class lavaan regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda k number cross-validation folds. Alternatively, can pass matrix booleans (TRUE, FALSE) indicates person subset belongs . See ?lessSEM::createSubsets example matrix look like. standardize Standardizing data prior analysis can undermine cross- validation. Set standardize=TRUE automatically standardize data. returnSubsetParameters set TRUE return parameters training set modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. element generated controlBFGS function. See ?controlBFGS details.","code":""},{"path":"/reference/cvRidgeBfgs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"cvRidgeBfgs — cvRidgeBfgs","text":"model class cvRegularizedSEM","code":""},{"path":"/reference/cvRidgeBfgs.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"cvRidgeBfgs — cvRidgeBfgs","text":"Identical regsem, models specified using lavaan. Currently, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. Ridge regularization: Hoerl, . E., & Kennard, R. W. (1970). Ridge Regression: Biased Estimation Nonorthogonal Problems. Technometrics, 12(1), 55–67. https://doi.org/10.1080/00401706.1970.10488634 Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793","code":""},{"path":"/reference/cvRidgeBfgs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"cvRidgeBfgs — cvRidgeBfgs","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +      l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +      l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Regularization:  lsem <- cvRidgeBfgs(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   # names of the regularized parameters:   regularized = paste0(\"l\", 6:15),   lambdas = seq(0,1,length.out = 20))  # use the plot-function to plot the cross-validation fit: plot(lsem)  # the coefficients can be accessed with: coef(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters"},{"path":"/reference/cvScad.html","id":null,"dir":"Reference","previous_headings":"","what":"cvScad — cvScad","title":"cvScad — cvScad","text":"Implements scad regularization structural equation models. penalty function given :  $$p( x_j) = \\begin{cases} \\lambda |x_j| & \\text{} |x_j| \\leq \\theta\\\\ \\frac{-x_j^2 + 2\\theta\\lambda |x_j| - \\lambda^2}{2(\\theta -1)} &  \\text{} \\lambda < |x_j| \\leq \\lambda\\theta \\\\ (\\theta + 1) \\lambda^2/2 & \\text{} |x_j| \\geq \\theta\\lambda\\\\ \\end{cases}$$ \\(\\theta > 2\\).","code":""},{"path":"/reference/cvScad.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"cvScad — cvScad","text":"","code":"cvScad(   lavaanModel,   regularized,   lambdas,   thetas,   k = 5,   standardize = FALSE,   returnSubsetParameters = FALSE,   modifyModel = lessSEM::modifyModel(),   method = \"glmnet\",   control = lessSEM::controlGlmnet() )"},{"path":"/reference/cvScad.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"cvScad — cvScad","text":"lavaanModel model class lavaan regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda thetas parameters whose absolute value threshold penalized constant (theta) k number cross-validation folds. Alternatively, can pass matrix booleans (TRUE, FALSE) indicates person subset belongs . See ?lessSEM::createSubsets example matrix look like. standardize Standardizing data prior analysis can undermine cross- validation. Set standardize=TRUE automatically standardize data. returnSubsetParameters set TRUE return parameters training set modifyModel used modify lavaanModel. See ?modifyModel. method optimizer used? Currently implemented ista glmnet. ista, control argument can used switch related procedures. control used control optimizer. element generated controlIsta function. See ?controlIsta","code":""},{"path":"/reference/cvScad.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"cvScad — cvScad","text":"model class cvRegularizedSEM","code":""},{"path":"/reference/cvScad.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"cvScad — cvScad","text":"Identical regsem, models specified using lavaan. Currenlty, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. scad regularization: Fan, J., & Li, R. (2001). Variable selection via nonconcave penalized likelihood oracle properties. Journal American Statistical Association, 96(456), 1348–1360. https://doi.org/10.1198/016214501753382273 Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classification. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/cvScad.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"cvScad — cvScad","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +      l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +      l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Regularization:  lsem <- cvScad(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   # names of the regularized parameters:   regularized = paste0(\"l\", 6:15),   lambdas = seq(0,1,length.out = 3),   thetas = seq(2.01,5,length.out = 3))  # the coefficients can be accessed with: coef(lsem) # if you are only interested in the estimates and not the tuning parameters, use coef(lsem)@estimates # or estimates(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters  # optional: plotting the cross-validation fit requires installation of plotly # plot(lsem)"},{"path":"/reference/cvScaler.html","id":null,"dir":"Reference","previous_headings":"","what":"cvScaler — cvScaler","title":"cvScaler — cvScaler","text":"uses means standard deviations training set standardize test set. See, e.g., https://scikit-learn.org/stable/modules/cross_validation.html .","code":""},{"path":"/reference/cvScaler.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"cvScaler — cvScaler","text":"","code":"cvScaler(testSet, means, standardDeviations)"},{"path":"/reference/cvScaler.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"cvScaler — cvScaler","text":"testSet test data set means means training set standardDeviations standard deviations training set","code":""},{"path":"/reference/cvScaler.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"cvScaler — cvScaler","text":"scaled test set","code":""},{"path":"/reference/cvScaler.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"cvScaler — cvScaler","text":"","code":"library(lessSEM) data <- matrix(rnorm(50),10,5)  cvScaler(testSet = data,           means = 1:5,           standardDeviations = 1:5)"},{"path":"/reference/cvSmoothAdaptiveLasso.html","id":null,"dir":"Reference","previous_headings":"","what":"cvSmoothAdaptiveLasso — cvSmoothAdaptiveLasso","title":"cvSmoothAdaptiveLasso — cvSmoothAdaptiveLasso","text":"Implements cross-validated smooth adaptive lasso regularization structural equation models. penalty function given : $$p( x_j) = p( x_j) = \\frac{1}{w_j}\\lambda\\sqrt{(x_j + \\epsilon)^2}$$","code":""},{"path":"/reference/cvSmoothAdaptiveLasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"cvSmoothAdaptiveLasso — cvSmoothAdaptiveLasso","text":"","code":"cvSmoothAdaptiveLasso(   lavaanModel,   regularized,   weights = NULL,   lambdas,   epsilon,   k = 5,   standardize = FALSE,   returnSubsetParameters = FALSE,   modifyModel = lessSEM::modifyModel(),   control = lessSEM::controlBFGS() )"},{"path":"/reference/cvSmoothAdaptiveLasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"cvSmoothAdaptiveLasso — cvSmoothAdaptiveLasso","text":"lavaanModel model class lavaan regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object weights labeled vector weights parameters model. unsure parameters called, use getLavaanParameters(model) lavaan model object. set NULL, default weights used: inverse absolute values unregularized parameter estimates lambdas numeric vector: values tuning parameter lambda epsilon epsilon > 0; controls smoothness approximation. Larger values = smoother k number cross-validation folds. Alternatively, can pass matrix booleans (TRUE, FALSE) indicates person subset belongs . See ?lessSEM::createSubsets example matrix look like. standardize Standardizing data prior analysis can undermine cross- validation. Set standardize=TRUE automatically standardize data. returnSubsetParameters set TRUE return parameters training set modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. element generated controlBFGS function. See ?controlBFGS details.","code":""},{"path":"/reference/cvSmoothAdaptiveLasso.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"cvSmoothAdaptiveLasso — cvSmoothAdaptiveLasso","text":"model class cvRegularizedSEM","code":""},{"path":"/reference/cvSmoothAdaptiveLasso.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"cvSmoothAdaptiveLasso — cvSmoothAdaptiveLasso","text":"Identical regsem, models specified using lavaan. Currently, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. Adaptive lasso regularization: Zou, H. (2006). adaptive lasso oracle properties. Journal American Statistical Association, 101(476), 1418–1429. https://doi.org/10.1198/016214506000000735 Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793","code":""},{"path":"/reference/cvSmoothAdaptiveLasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"cvSmoothAdaptiveLasso — cvSmoothAdaptiveLasso","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +      l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +      l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Regularization:  lsem <- cvSmoothAdaptiveLasso(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   # names of the regularized parameters:   regularized = paste0(\"l\", 6:15),   lambdas = seq(0,1,.1),   epsilon = 1e-8)  # use the plot-function to plot the cross-validation fit plot(lsem)  # the coefficients can be accessed with: coef(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters  # The best parameters can also be extracted with: coef(lsem)"},{"path":"/reference/cvSmoothElasticNet.html","id":null,"dir":"Reference","previous_headings":"","what":"cvSmoothElasticNet — cvSmoothElasticNet","title":"cvSmoothElasticNet — cvSmoothElasticNet","text":"Implements cross-validated  smooth elastic net regularization structural equation models. penalty function given : $$p( x_j) = \\alpha\\lambda\\sqrt{(x_j + \\epsilon)^2} + (1-\\alpha)\\lambda x_j^2$$ Note smooth elastic net combines ridge smooth lasso regularization. \\(\\alpha = 0\\), elastic net reduces ridge regularization. \\(\\alpha = 1\\) reduces smooth lasso regularization. , elastic net compromise shrinkage lasso ridge penalty.","code":""},{"path":"/reference/cvSmoothElasticNet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"cvSmoothElasticNet — cvSmoothElasticNet","text":"","code":"cvSmoothElasticNet(   lavaanModel,   regularized,   lambdas,   alphas,   epsilon,   k = 5,   standardize = FALSE,   returnSubsetParameters = FALSE,   modifyModel = lessSEM::modifyModel(),   control = lessSEM::controlBFGS() )"},{"path":"/reference/cvSmoothElasticNet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"cvSmoothElasticNet — cvSmoothElasticNet","text":"lavaanModel model class lavaan regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda alphas numeric vector values tuning parameter alpha. Must 0 1. 0 = ridge, 1 = lasso. epsilon epsilon > 0; controls smoothness approximation. Larger values = smoother k number cross-validation folds. Alternatively, can pass matrix booleans (TRUE, FALSE) indicates person subset belongs . See ?lessSEM::createSubsets example matrix look like. standardize Standardizing data prior analysis can undermine cross- validation. Set standardize=TRUE automatically standardize data. returnSubsetParameters set TRUE return parameters training set modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. element generated controlBFGS function. See ?controlBFGS details.","code":""},{"path":"/reference/cvSmoothElasticNet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"cvSmoothElasticNet — cvSmoothElasticNet","text":"model class cvRegularizedSEM","code":""},{"path":"/reference/cvSmoothElasticNet.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"cvSmoothElasticNet — cvSmoothElasticNet","text":"Identical regsem, models specified using lavaan. Currently, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. Elastic net regularization: Zou, H., & Hastie, T. (2005). Regularization variable selection via elastic net. Journal Royal Statistical Society: Series B, 67(2), 301–320. https://doi.org/10.1111/j.1467-9868.2005.00503.x Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793","code":""},{"path":"/reference/cvSmoothElasticNet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"cvSmoothElasticNet — cvSmoothElasticNet","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +      l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +      l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Regularization:  lsem <- cvSmoothElasticNet(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   # names of the regularized parameters:   regularized = paste0(\"l\", 6:15),   epsilon = 1e-8,   lambdas = seq(0,1,length.out = 5),   alphas = .3)  # the coefficients can be accessed with: coef(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters  # optional: plotting the cross-validation fit requires installation of plotly # plot(lsem)"},{"path":"/reference/cvSmoothLasso.html","id":null,"dir":"Reference","previous_headings":"","what":"cvSmoothLasso — cvSmoothLasso","title":"cvSmoothLasso — cvSmoothLasso","text":"Implements cross-validated smooth lasso regularization structural equation models. penalty function given : $$p( x_j) = \\lambda \\sqrt{(x_j + \\epsilon)^2}$$","code":""},{"path":"/reference/cvSmoothLasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"cvSmoothLasso — cvSmoothLasso","text":"","code":"cvSmoothLasso(   lavaanModel,   regularized,   lambdas,   epsilon,   k = 5,   standardize = FALSE,   returnSubsetParameters = FALSE,   modifyModel = lessSEM::modifyModel(),   control = lessSEM::controlBFGS() )"},{"path":"/reference/cvSmoothLasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"cvSmoothLasso — cvSmoothLasso","text":"lavaanModel model class lavaan regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda epsilon epsilon > 0; controls smoothness approximation. Larger values = smoother k number cross-validation folds. Alternatively, can pass matrix booleans (TRUE, FALSE) indicates person subset belongs . See ?lessSEM::createSubsets example matrix look like. standardize Standardizing data prior analysis can undermine cross- validation. Set standardize=TRUE automatically standardize data. returnSubsetParameters set TRUE return parameters training set modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. element generated controlBFGS function. See ?controlBFGS details.","code":""},{"path":"/reference/cvSmoothLasso.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"cvSmoothLasso — cvSmoothLasso","text":"model class cvRegularizedSEM","code":""},{"path":"/reference/cvSmoothLasso.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"cvSmoothLasso — cvSmoothLasso","text":"Identical regsem, models specified using lavaan. Currently, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. Lasso regularization: Tibshirani, R. (1996). Regression shrinkage selection via lasso. Journal Royal Statistical Society. Series B (Methodological), 58(1), 267–288. Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793","code":""},{"path":"/reference/cvSmoothLasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"cvSmoothLasso — cvSmoothLasso","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +       l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +       l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Regularization:  lsem <- cvSmoothLasso(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   # names of the regularized parameters:   regularized = paste0(\"l\", 6:15),   lambdas = seq(0,1,.1),   k = 5, # number of cross-validation folds   epsilon = 1e-8,   standardize = TRUE) # automatic standardization  # use the plot-function to plot the cross-validation fit: plot(lsem)  # the coefficients can be accessed with: coef(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters  # The best parameters can also be extracted with: coef(lsem)"},{"path":"/reference/dot-SEMFromLavaan.html","id":null,"dir":"Reference","previous_headings":"","what":".SEMFromLavaan — .SEMFromLavaan","title":".SEMFromLavaan — .SEMFromLavaan","text":"internal function. Translates object class lavaan internal model representation.","code":""},{"path":"/reference/dot-SEMFromLavaan.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".SEMFromLavaan — .SEMFromLavaan","text":"","code":".SEMFromLavaan(   lavaanModel,   whichPars = \"est\",   fit = TRUE,   addMeans = TRUE,   activeSet = NULL,   dataSet = NULL,   transformations = NULL,   transformationList = list(),   transformationGradientStepSize = 1e-06 )"},{"path":"/reference/dot-SEMFromLavaan.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".SEMFromLavaan — .SEMFromLavaan","text":"lavaanModel model class lavaan whichPars parameters used initialize model. set \"est\", parameters set estimated parameters lavaan model. set \"start\", starting values lavaan used. latter can useful parameters optimized afterwards setting parameters \"est\" may result model getting stuck local minimum. fit model fitted compared lavaanModel? addMeans lavaanModel meanstructure = FALSE, addMeans = TRUE add mean structure. FALSE set means observed variables average activeSet Option use subset individuals data set. Logical vector length N indicating subjects remain sample. dataSet optional: Pass alternative data set lessSEM:::.SEMFromLavaan replace original data set lavaanModel. transformationGradientStepSize step size used compute gradients transformations","code":""},{"path":"/reference/dot-SEMFromLavaan.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".SEMFromLavaan — .SEMFromLavaan","text":"Object class Rcpp_SEMCpp","code":""},{"path":"/reference/dot-SEMdata.html","id":null,"dir":"Reference","previous_headings":"","what":".SEMdata — .SEMdata","title":".SEMdata — .SEMdata","text":"internal function. Creates internal data representation","code":""},{"path":"/reference/dot-SEMdata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".SEMdata — .SEMdata","text":"","code":".SEMdata(rawData)"},{"path":"/reference/dot-SEMdata.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".SEMdata — .SEMdata","text":"rawData matrix raw data set","code":""},{"path":"/reference/dot-SEMdata.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".SEMdata — .SEMdata","text":"list internal representation data","code":""},{"path":"/reference/dot-SEMdataWLS.html","id":null,"dir":"Reference","previous_headings":"","what":".SEMdataWLS — .SEMdataWLS","title":".SEMdataWLS — .SEMdataWLS","text":"internal function. Creates internal data representation","code":""},{"path":"/reference/dot-SEMdataWLS.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".SEMdataWLS — .SEMdataWLS","text":"","code":".SEMdataWLS(rawData, lavaanModel)"},{"path":"/reference/dot-SEMdataWLS.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".SEMdataWLS — .SEMdataWLS","text":"rawData matrix raw data set lavaanModel lavaan model","code":""},{"path":"/reference/dot-SEMdataWLS.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".SEMdataWLS — .SEMdataWLS","text":"list internal representation data","code":""},{"path":"/reference/dot-adaptBreakingForWls.html","id":null,"dir":"Reference","previous_headings":"","what":".adaptBreakingForWls — .adaptBreakingForWls","title":".adaptBreakingForWls — .adaptBreakingForWls","text":"wls needs smaller breaking points ml","code":""},{"path":"/reference/dot-adaptBreakingForWls.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".adaptBreakingForWls — .adaptBreakingForWls","text":"","code":".adaptBreakingForWls(lavaanModel, currentBreaking, selectedDefault)"},{"path":"/reference/dot-adaptBreakingForWls.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".adaptBreakingForWls — .adaptBreakingForWls","text":"lavaanModel single model vector models currentBreaking current breaking condition value selectedDefault default breaking condition selected?","code":""},{"path":"/reference/dot-adaptBreakingForWls.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".adaptBreakingForWls — .adaptBreakingForWls","text":"updated breaking","code":""},{"path":"/reference/dot-addMeanStructure.html","id":null,"dir":"Reference","previous_headings":"","what":".addMeanStructure — .addMeanStructure","title":".addMeanStructure — .addMeanStructure","text":"adds mean strucuture parameter table","code":""},{"path":"/reference/dot-addMeanStructure.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".addMeanStructure — .addMeanStructure","text":"","code":".addMeanStructure(parameterTable, manifestNames, MvectorElements)"},{"path":"/reference/dot-addMeanStructure.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".addMeanStructure — .addMeanStructure","text":"parameterTable table parameters manifestNames names manifest variables MvectorElements elements means vector","code":""},{"path":"/reference/dot-addMeanStructure.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".addMeanStructure — .addMeanStructure","text":"parameterTable","code":""},{"path":"/reference/dot-checkLavaanModel.html","id":null,"dir":"Reference","previous_headings":"","what":".checkLavaanModel — .checkLavaanModel","title":".checkLavaanModel — .checkLavaanModel","text":"checks model type lavaan","code":""},{"path":"/reference/dot-checkLavaanModel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".checkLavaanModel — .checkLavaanModel","text":"","code":".checkLavaanModel(lavaanModel)"},{"path":"/reference/dot-checkLavaanModel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".checkLavaanModel — .checkLavaanModel","text":"lavaanModel m0del type lavaan","code":""},{"path":"/reference/dot-checkLavaanModel.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".checkLavaanModel — .checkLavaanModel","text":"nothing","code":""},{"path":"/reference/dot-checkPenalties.html","id":null,"dir":"Reference","previous_headings":"","what":".checkPenalties — .checkPenalties","title":".checkPenalties — .checkPenalties","text":"Internal function check mixedPenalty object","code":""},{"path":"/reference/dot-checkPenalties.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".checkPenalties — .checkPenalties","text":"","code":".checkPenalties(mixedPenalty)"},{"path":"/reference/dot-checkPenalties.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".checkPenalties — .checkPenalties","text":"mixedPenalty object class mixedPenalty. object can created mixedPenalty function. Penalties can added addCappedL1, addLasso, addLsp, addMcp, addScad functions.","code":""},{"path":"/reference/dot-compileTransformations.html","id":null,"dir":"Reference","previous_headings":"","what":".compileTransformations — .compileTransformations","title":".compileTransformations — .compileTransformations","text":"compile user defined parameter transformations pass SEM","code":""},{"path":"/reference/dot-compileTransformations.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".compileTransformations — .compileTransformations","text":"","code":".compileTransformations(syntax, parameterLabels, compile = TRUE, notes = NULL)"},{"path":"/reference/dot-compileTransformations.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".compileTransformations — .compileTransformations","text":"syntax string user defined transformations parameterLabels names parameters model compile set FALSE, function compiled -> visual inspection notes option pass notes function. notes current function added","code":""},{"path":"/reference/dot-compileTransformations.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".compileTransformations — .compileTransformations","text":"list parameter names two Rcpp functions: (1) transformation function (2) function create pointer transformation function. starting values defined, returned well.","code":""},{"path":"/reference/dot-computeInitialHessian.html","id":null,"dir":"Reference","previous_headings":"","what":".computeInitialHessian — .computeInitialHessian","title":".computeInitialHessian — .computeInitialHessian","text":"computes initial Hessian used optimization. use parameter estimates lavaan starting values, typcially makes sense just use Hessian lavaan model initial Hessian","code":""},{"path":"/reference/dot-computeInitialHessian.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".computeInitialHessian — .computeInitialHessian","text":"","code":".computeInitialHessian(   initialHessian,   rawParameters,   lavaanModel,   SEM,   addMeans,   stepSize,   notes = NULL )"},{"path":"/reference/dot-computeInitialHessian.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".computeInitialHessian — .computeInitialHessian","text":"initialHessian option provide initial Hessian optimizer. Must row column names corresponding parameter labels. use getLavaanParameters(lavaanModel) see labels. set \"scoreBased\", outer product scores used approximation (see https://en.wikipedia.org/wiki/Berndt%E2%80%93Hall%E2%80%93Hall%E2%80%93Hausman_algorithm). set \"compute\", initial hessian computed. set single value, diagonal matrix single value along diagonal used. default \"lavaan\" extracts Hessian lavaanModel. Hessian typically deviate internal SEM represenation lessSEM (due transformation variances), works quite well practice. rawParameters vector raw parameters lavaanModel lavaan model object SEM internal SEM representation addMeans mean structure added model? stepSize initial step size notes option pass notes function. notes current function added","code":""},{"path":"/reference/dot-computeInitialHessian.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".computeInitialHessian — .computeInitialHessian","text":"Hessian matrix notes","code":""},{"path":"/reference/dot-createMultiGroupTransformations.html","id":null,"dir":"Reference","previous_headings":"","what":".createMultiGroupTransformations — .createMultiGroupTransformations","title":".createMultiGroupTransformations — .createMultiGroupTransformations","text":"compiles transformation function adapts parameter vector","code":""},{"path":"/reference/dot-createMultiGroupTransformations.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".createMultiGroupTransformations — .createMultiGroupTransformations","text":"","code":".createMultiGroupTransformations(transformations, parameterValues)"},{"path":"/reference/dot-createMultiGroupTransformations.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".createMultiGroupTransformations — .createMultiGroupTransformations","text":"transformations string transformations parameterValues values parameters already model","code":""},{"path":"/reference/dot-createMultiGroupTransformations.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".createMultiGroupTransformations — .createMultiGroupTransformations","text":"list extended parameter vector transformation function pointer","code":""},{"path":"/reference/dot-createParameterTable.html","id":null,"dir":"Reference","previous_headings":"","what":".createParameterTable — .createParameterTable","title":".createParameterTable — .createParameterTable","text":"create parameter table using elements extracted lavaan","code":""},{"path":"/reference/dot-createParameterTable.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".createParameterTable — .createParameterTable","text":"","code":".createParameterTable(   parameterValues,   parameterLabels,   modelParameters,   parameterIDs )"},{"path":"/reference/dot-createParameterTable.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".createParameterTable — .createParameterTable","text":"parameterValues values parameters parameterLabels names parameters modelParameters model parameters lavaan parameterIDs unique parameter ids lavaan -> identify parameter unique number","code":""},{"path":"/reference/dot-createParameterTable.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".createParameterTable — .createParameterTable","text":"parameter table lessSEM","code":""},{"path":"/reference/dot-createRcppTransformationFunction.html","id":null,"dir":"Reference","previous_headings":"","what":".createRcppTransformationFunction — .createRcppTransformationFunction","title":".createRcppTransformationFunction — .createRcppTransformationFunction","text":"create Rcpp function uses user-defined parameter transformation","code":""},{"path":"/reference/dot-createRcppTransformationFunction.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".createRcppTransformationFunction — .createRcppTransformationFunction","text":"","code":".createRcppTransformationFunction(syntax, parameters)"},{"path":"/reference/dot-createRcppTransformationFunction.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".createRcppTransformationFunction — .createRcppTransformationFunction","text":"syntax syntax user defined transformations parameters labels parameters used transformations","code":""},{"path":"/reference/dot-createRcppTransformationFunction.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".createRcppTransformationFunction — .createRcppTransformationFunction","text":"string functions compilations Rcpp","code":""},{"path":"/reference/dot-createTransformations.html","id":null,"dir":"Reference","previous_headings":"","what":".createTransformations — .createTransformations","title":".createTransformations — .createTransformations","text":"compiles transformation function adapts parameterTable","code":""},{"path":"/reference/dot-createTransformations.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".createTransformations — .createTransformations","text":"","code":".createTransformations(transformations, parameterLabels, parameterTable)"},{"path":"/reference/dot-createTransformations.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".createTransformations — .createTransformations","text":"transformations string transformations parameterLabels labels parameteres already model parameterTable existing parameter table","code":""},{"path":"/reference/dot-createTransformations.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".createTransformations — .createTransformations","text":"list parameterTable transformation function pointer","code":""},{"path":"/reference/dot-cvRegularizeSEMInternal.html","id":null,"dir":"Reference","previous_headings":"","what":".cvRegularizeSEMInternal — .cvRegularizeSEMInternal","title":".cvRegularizeSEMInternal — .cvRegularizeSEMInternal","text":"Combination regularized structural equation model cross-validation","code":""},{"path":"/reference/dot-cvRegularizeSEMInternal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".cvRegularizeSEMInternal — .cvRegularizeSEMInternal","text":"","code":".cvRegularizeSEMInternal(   lavaanModel,   k,   standardize,   penalty,   weights,   returnSubsetParameters,   tuningParameters,   method,   modifyModel,   control )"},{"path":"/reference/dot-cvRegularizeSEMInternal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".cvRegularizeSEMInternal — .cvRegularizeSEMInternal","text":"lavaanModel model class lavaan k number cross-validation folds. Alternatively, matrix pre-defined subsets can passed function. See ?lessSEM::cvLasso example standardize training test sets standardized? penalty string: name penalty used model weights labeled vector weights parameters model. returnSubsetParameters set TRUE, parameter estimates individual cross-validation training sets returned tuningParameters data.frame tuning parameter values method optimizer used? Currently implemented ista glmnet. ista, control argument can used switch related procedures (currently gist). modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. element generated controlIsta() controlGlmnet() functions.","code":""},{"path":"/reference/dot-cvRegularizeSEMInternal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".cvRegularizeSEMInternal — .cvRegularizeSEMInternal","text":"model class cvRegularizedSEM","code":""},{"path":"/reference/dot-cvRegularizeSEMInternal.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":".cvRegularizeSEMInternal — .cvRegularizeSEMInternal","text":"Internal function: function computes regularized models penalty functions implemented glmnet gist. Use dedicated penalty functions (e.g., lessSEM::cvLasso) penalize model.","code":""},{"path":"/reference/dot-cvRegularizeSmoothSEMInternal.html","id":null,"dir":"Reference","previous_headings":"","what":".cvRegularizeSmoothSEMInternal — .cvRegularizeSmoothSEMInternal","title":".cvRegularizeSmoothSEMInternal — .cvRegularizeSmoothSEMInternal","text":"Combination smoothly regularized structural equation model cross-validation","code":""},{"path":"/reference/dot-cvRegularizeSmoothSEMInternal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".cvRegularizeSmoothSEMInternal — .cvRegularizeSmoothSEMInternal","text":"","code":".cvRegularizeSmoothSEMInternal(   lavaanModel,   k,   standardize,   penalty,   weights,   returnSubsetParameters,   tuningParameters,   epsilon,   modifyModel,   method = \"bfgs\",   control )"},{"path":"/reference/dot-cvRegularizeSmoothSEMInternal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".cvRegularizeSmoothSEMInternal — .cvRegularizeSmoothSEMInternal","text":"lavaanModel model class lavaan k number cross-validation folds. Alternatively, matrix pre-defined subsets can passed function. See ?lessSEM::cvSmoothLasso example standardize training test sets standardized? penalty string: name penalty used model weights labeled vector weights parameters model. returnSubsetParameters set TRUE, parameter estimates individual cross-validation training sets returned tuningParameters data.frame tuning parameter values epsilon epsilon > 0; controls smoothness approximation. Larger values = smoother modifyModel used modify lavaanModel. See ?modifyModel. method optimizer used. Currently \"bfgs\" supported. control used control optimizer. element generated controlBFGS function. See ?controlBFGS details.","code":""},{"path":"/reference/dot-cvRegularizeSmoothSEMInternal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".cvRegularizeSmoothSEMInternal — .cvRegularizeSmoothSEMInternal","text":"model class cvRegularizedSEM","code":""},{"path":"/reference/dot-cvRegularizeSmoothSEMInternal.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":".cvRegularizeSmoothSEMInternal — .cvRegularizeSmoothSEMInternal","text":"Internal function: function computes regularized models penalty functions implemented bfgs. Use dedicated penalty functions (e.g., lessSEM::cvSmoothLasso) penalize model.","code":""},{"path":"/reference/dot-cvregsem2LavaanParameters.html","id":null,"dir":"Reference","previous_headings":"","what":".cvregsem2LavaanParameters — .cvregsem2LavaanParameters","title":".cvregsem2LavaanParameters — .cvregsem2LavaanParameters","text":"helper function: regsem lavaan use slightly different parameter labels. function can used translate parameter labels cv_regsem object lavaan labels","code":""},{"path":"/reference/dot-cvregsem2LavaanParameters.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".cvregsem2LavaanParameters — .cvregsem2LavaanParameters","text":"","code":".cvregsem2LavaanParameters(cvregsemModel, lavaanModel)"},{"path":"/reference/dot-cvregsem2LavaanParameters.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".cvregsem2LavaanParameters — .cvregsem2LavaanParameters","text":"cvregsemModel model class cvregsem lavaanModel model class lavaan","code":""},{"path":"/reference/dot-cvregsem2LavaanParameters.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".cvregsem2LavaanParameters — .cvregsem2LavaanParameters","text":"regsem parameters lavaan labels","code":""},{"path":"/reference/dot-defineDerivatives.html","id":null,"dir":"Reference","previous_headings":"","what":".defineDerivatives — .defineDerivatives","title":".defineDerivatives — .defineDerivatives","text":"adds elements required compute derivatives fitting function respect parameters SEMList","code":""},{"path":"/reference/dot-defineDerivatives.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".defineDerivatives — .defineDerivatives","text":"","code":".defineDerivatives(SEMList, parameterTable, modelMatrices)"},{"path":"/reference/dot-defineDerivatives.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".defineDerivatives — .defineDerivatives","text":"SEMList list representing SEM parameterTable table parameters modelMatrices matrices RAM model","code":""},{"path":"/reference/dot-defineDerivatives.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".defineDerivatives — .defineDerivatives","text":"SEMList","code":""},{"path":"/reference/dot-extractParametersFromSyntax.html","id":null,"dir":"Reference","previous_headings":"","what":".extractParametersFromSyntax — .extractParametersFromSyntax","title":".extractParametersFromSyntax — .extractParametersFromSyntax","text":"extract names parameters syntax","code":""},{"path":"/reference/dot-extractParametersFromSyntax.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".extractParametersFromSyntax — .extractParametersFromSyntax","text":"","code":".extractParametersFromSyntax(syntax, parameterLabels)"},{"path":"/reference/dot-extractParametersFromSyntax.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".extractParametersFromSyntax — .extractParametersFromSyntax","text":"syntax syntax parameter transformations parameterLabels names parameters model","code":""},{"path":"/reference/dot-extractParametersFromSyntax.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".extractParametersFromSyntax — .extractParametersFromSyntax","text":"vector names parameters used syntax vector boolean indicating parameter transformation result","code":""},{"path":"/reference/dot-extractSEMFromLavaan.html","id":null,"dir":"Reference","previous_headings":"","what":".extractSEMFromLavaan — .extractSEMFromLavaan","title":".extractSEMFromLavaan — .extractSEMFromLavaan","text":"internal function. Translates object class lavaan internal model representation.","code":""},{"path":"/reference/dot-extractSEMFromLavaan.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".extractSEMFromLavaan — .extractSEMFromLavaan","text":"","code":".extractSEMFromLavaan(   lavaanModel,   whichPars = \"est\",   fit = TRUE,   addMeans = TRUE,   activeSet = NULL,   dataSet = NULL,   transformations = NULL )"},{"path":"/reference/dot-extractSEMFromLavaan.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".extractSEMFromLavaan — .extractSEMFromLavaan","text":"lavaanModel model class lavaan whichPars parameters used initialize model. set \"est\", parameters set estimated parameters lavaan model. set \"start\", starting values lavaan used. latter can useful parameters optimized afterwards setting parameters \"est\" may result model getting stuck local minimum. fit model fitted compared lavaanModel? addMeans lavaanModel meanstructure = FALSE, addMeans = TRUE add mean structure. FALSE set means observed variables average activeSet Option use subset individuals data set. Logical vector length N indicating subjects remain sample. dataSet optional: Pass alternative data set lessSEM:::.SEMFromLavaan replace original data set lavaanModel. transformations optional: transform parameter values.","code":""},{"path":"/reference/dot-extractSEMFromLavaan.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".extractSEMFromLavaan — .extractSEMFromLavaan","text":"list SEMList (model RAM representation) fit (boolean indicating model fit compared lavaan)","code":""},{"path":"/reference/dot-fit.html","id":null,"dir":"Reference","previous_headings":"","what":".fit — .fit","title":".fit — .fit","text":"fits object class Rcpp_SEMCpp.","code":""},{"path":"/reference/dot-fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".fit — .fit","text":"","code":".fit(SEM)"},{"path":"/reference/dot-fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".fit — .fit","text":"SEM model class Rcpp_SEMCpp.","code":""},{"path":"/reference/dot-fit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".fit — .fit","text":"fitted SEM","code":""},{"path":"/reference/dot-fitElasticNetMix.html","id":null,"dir":"Reference","previous_headings":"","what":".fitElasticNetMix — .fitElasticNetMix","title":".fitElasticNetMix — .fitElasticNetMix","text":"Optimizes object mixed penalty. See ?mixedPenalty details.","code":""},{"path":"/reference/dot-fitElasticNetMix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".fitElasticNetMix — .fitElasticNetMix","text":"","code":".fitElasticNetMix(mixedPenalty)"},{"path":"/reference/dot-fitElasticNetMix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".fitElasticNetMix — .fitElasticNetMix","text":"mixedPenalty object class mixedPenalty. object can created mixedPenalty function. Penalties can added addCappedL1, addElastiNet, addLasso, addLsp, addMcp, addScad functions.","code":""},{"path":"/reference/dot-fitElasticNetMix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".fitElasticNetMix — .fitElasticNetMix","text":"object class regularizedSEMMixedPenalty","code":""},{"path":"/reference/dot-fitFunction.html","id":null,"dir":"Reference","previous_headings":"","what":".fitFunction — .fitFunction","title":".fitFunction — .fitFunction","text":"internal function returns objective value fitting function object class Rcpp_SEMCpp. function can used optimizers","code":""},{"path":"/reference/dot-fitFunction.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".fitFunction — .fitFunction","text":"","code":".fitFunction(par, SEM, raw)"},{"path":"/reference/dot-fitFunction.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".fitFunction — .fitFunction","text":"par labeled vector parameter values SEM model class Rcpp_SEMCpp. raw controls internal transformations lessSEM used.","code":""},{"path":"/reference/dot-fitFunction.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".fitFunction — .fitFunction","text":"objective value fitting function","code":""},{"path":"/reference/dot-fitMix.html","id":null,"dir":"Reference","previous_headings":"","what":".fitMix — .fitMix","title":".fitMix — .fitMix","text":"Optimizes object mixed penalty. See ?mixedPenalty details.","code":""},{"path":"/reference/dot-fitMix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".fitMix — .fitMix","text":"","code":".fitMix(mixedPenalty)"},{"path":"/reference/dot-fitMix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".fitMix — .fitMix","text":"mixedPenalty object class mixedPenalty. object can created mixedPenalty function. Penalties can added addCappedL1, addElastiNet, addLasso, addLsp, addMcp, addScad functions.","code":""},{"path":"/reference/dot-fitMix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".fitMix — .fitMix","text":"object class regularizedSEMMixedPenalty","code":""},{"path":"/reference/dot-getGradients.html","id":null,"dir":"Reference","previous_headings":"","what":".getGradients — .getGradients","title":".getGradients — .getGradients","text":"returns gradients model class Rcpp_SEMCpp. internal model representation. Models class can generated lessSEM:::.SEMFromLavaan-function.","code":""},{"path":"/reference/dot-getGradients.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".getGradients — .getGradients","text":"","code":".getGradients(SEM, raw)"},{"path":"/reference/dot-getGradients.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".getGradients — .getGradients","text":"SEM model class Rcpp_SEMCpp raw controls internal transformations lessSEM used. lessSEM use exponential function variances avoid negative variances. set TRUE, gradients given internal parameter representation. Set FALSE get usual gradients","code":""},{"path":"/reference/dot-getGradients.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".getGradients — .getGradients","text":"vector derivatives -2log-Likelihood respect parameter","code":""},{"path":"/reference/dot-getHessian.html","id":null,"dir":"Reference","previous_headings":"","what":".getHessian — .getHessian","title":".getHessian — .getHessian","text":"returns Hessian model class Rcpp_SEMCpp. internal model representation. Models class can generated lessSEM:::.SEMFromLavaan-function. function adapted lavaan::lav_model_hessian.","code":""},{"path":"/reference/dot-getHessian.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".getHessian — .getHessian","text":"","code":".getHessian(SEM, raw, eps = 1e-07)"},{"path":"/reference/dot-getHessian.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".getHessian — .getHessian","text":"SEM model class Rcpp_SEMCpp raw controls internal transformations lessSEM used. lessSEM use exponential function variances avoid negative variances. set TRUE, gradients given internal parameter representation. Set FALSE get usual gradients eps eps controls step size numerical approximation.","code":""},{"path":"/reference/dot-getHessian.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".getHessian — .getHessian","text":"matrix second derivatives -2log-Likelihood respect parameter","code":""},{"path":"/reference/dot-getMaxLambda_C.html","id":null,"dir":"Reference","previous_headings":"","what":".getMaxLambda_C — .getMaxLambda_C","title":".getMaxLambda_C — .getMaxLambda_C","text":"generates first lambda sets regularized parameters zero","code":""},{"path":"/reference/dot-getMaxLambda_C.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".getMaxLambda_C — .getMaxLambda_C","text":"","code":".getMaxLambda_C(   regularizedModel,   SEM,   rawParameters,   weights,   N,   approx = FALSE )"},{"path":"/reference/dot-getMaxLambda_C.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".getMaxLambda_C — .getMaxLambda_C","text":"regularizedModel Model combining likelihood lasso type penalty SEM model class Rcpp_SEMCpp rawParameters labeled vector starting values weights weights given parameter penalty function N sample size approx set TRUE, .Machine$double.xmax^(.01) used instead .Machine$double.xmax^(.05)","code":""},{"path":"/reference/dot-getMaxLambda_C.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".getMaxLambda_C — .getMaxLambda_C","text":"first lambda value sets regularized parameters zero (plus tolerance)","code":""},{"path":"/reference/dot-getParameters.html","id":null,"dir":"Reference","previous_headings":"","what":".getParameters — .getParameters","title":".getParameters — .getParameters","text":"returns parameters internal model representation.","code":""},{"path":"/reference/dot-getParameters.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".getParameters — .getParameters","text":"","code":".getParameters(SEM, raw = FALSE, transformations = FALSE)"},{"path":"/reference/dot-getParameters.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".getParameters — .getParameters","text":"SEM model class Rcpp_SEMCpp. Models class raw controls parameter returned raw format transformed transformations transformed parameters included?","code":""},{"path":"/reference/dot-getParameters.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".getParameters — .getParameters","text":"labeled vector parameter values","code":""},{"path":"/reference/dot-getRawData.html","id":null,"dir":"Reference","previous_headings":"","what":".getRawData — .getRawData","title":".getRawData — .getRawData","text":"Extracts raw data lavaan adapts user supplied data set structure lavaan data","code":""},{"path":"/reference/dot-getRawData.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".getRawData — .getRawData","text":"","code":".getRawData(lavaanModel, dataSet, estimator)"},{"path":"/reference/dot-getRawData.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".getRawData — .getRawData","text":"lavaanModel model fitted lavaan dataSet user supplied data set estimator estimator used?","code":""},{"path":"/reference/dot-getRawData.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".getRawData — .getRawData","text":"raw data","code":""},{"path":"/reference/dot-getScores.html","id":null,"dir":"Reference","previous_headings":"","what":".getScores — .getScores","title":".getScores — .getScores","text":"returns scores model class Rcpp_SEMCpp. internal model representation. Models class can generated lessSEM:::.SEMFromLavaan-function.","code":""},{"path":"/reference/dot-getScores.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".getScores — .getScores","text":"","code":".getScores(SEM, raw)"},{"path":"/reference/dot-getScores.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".getScores — .getScores","text":"SEM model class Rcpp_SEMCpp raw controls internal transformations lessSEM used. lessSEM use exponential function variances avoid negative variances. set TRUE, scores given internal parameter representation. Set FALSE get usual scores","code":""},{"path":"/reference/dot-getScores.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".getScores — .getScores","text":"matrix derivatives -2log-Likelihood person parameter (rows persons, columns parameters)","code":""},{"path":"/reference/dot-gpGetMaxLambda.html","id":null,"dir":"Reference","previous_headings":"","what":".gpGetMaxLambda — .gpGetMaxLambda","title":".gpGetMaxLambda — .gpGetMaxLambda","text":"generates first lambda sets regularized parameters zero","code":""},{"path":"/reference/dot-gpGetMaxLambda.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".gpGetMaxLambda — .gpGetMaxLambda","text":"","code":".gpGetMaxLambda(   regularizedModel,   par,   fitFunction,   gradientFunction,   userSuppliedArguments,   weights )"},{"path":"/reference/dot-gpGetMaxLambda.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".gpGetMaxLambda — .gpGetMaxLambda","text":"regularizedModel Model combining likelihood lasso type penalty par labeled vector starting values fitFunction R fit function gradientFunction R gradient functions userSuppliedArguments list arguments fitFunction gradientFunction weights weights given parameter penalty function","code":""},{"path":"/reference/dot-gpGetMaxLambda.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".gpGetMaxLambda — .gpGetMaxLambda","text":"first lambda value sets regularized parameters zero (plus tolerance)","code":""},{"path":"/reference/dot-gpOptimizationInternal.html","id":null,"dir":"Reference","previous_headings":"","what":".gpOptimizationInternal — .gpOptimizationInternal","title":".gpOptimizationInternal — .gpOptimizationInternal","text":"Internal function: function computes regularized models penaltiy functions implemented glmnet gist. Use dedicated penalty functions (e.g., lessSEM::gpLasso) penalize model.","code":""},{"path":"/reference/dot-gpOptimizationInternal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".gpOptimizationInternal — .gpOptimizationInternal","text":"","code":".gpOptimizationInternal(   par,   weights,   fn,   gr = NULL,   additionalArguments,   isCpp = FALSE,   penalty,   tuningParameters,   method,   control )"},{"path":"/reference/dot-gpOptimizationInternal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".gpOptimizationInternal — .gpOptimizationInternal","text":"par labeled vector starting values weights labeled vector weights parameters model. fn R function takes parameters labels input returns fit value (single value) gr R function takes parameters labels input returns gradients objective function. set NULL, numDeriv used approximate gradients additionalArguments additional argument passed fn gr isCpp boolean: fn gr C++ function pointers? penalty string: name penalty used model tuningParameters data.frame tuning parameter values method optimizer used? Currently implemented ista glmnet. ista, control argument can used switch related procedures (currently gist). control used control optimizer. element generated controlIsta() controlGlmnet() functions.","code":""},{"path":"/reference/dot-gpOptimizationInternal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".gpOptimizationInternal — .gpOptimizationInternal","text":"Object class gpRegularized","code":""},{"path":"/reference/dot-gradientFunction.html","id":null,"dir":"Reference","previous_headings":"","what":".gradientFunction — .gradientFunction","title":".gradientFunction — .gradientFunction","text":"internal function returns gradients object class Rcpp_SEMCpp. function can used optimizers","code":""},{"path":"/reference/dot-gradientFunction.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".gradientFunction — .gradientFunction","text":"","code":".gradientFunction(par, SEM, raw)"},{"path":"/reference/dot-gradientFunction.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".gradientFunction — .gradientFunction","text":"par labeled vector parameter values SEM model class Rcpp_SEMCpp. raw controls internal transformations lessSEM used.","code":""},{"path":"/reference/dot-gradientFunction.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".gradientFunction — .gradientFunction","text":"gradients model","code":""},{"path":"/reference/dot-initializeMultiGroupSEMForRegularization.html","id":null,"dir":"Reference","previous_headings":"","what":".initializeMultiGroupSEMForRegularization — .initializeMultiGroupSEMForRegularization","title":".initializeMultiGroupSEMForRegularization — .initializeMultiGroupSEMForRegularization","text":"initializes internal C++ SEM regularization functions","code":""},{"path":"/reference/dot-initializeMultiGroupSEMForRegularization.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".initializeMultiGroupSEMForRegularization — .initializeMultiGroupSEMForRegularization","text":"","code":".initializeMultiGroupSEMForRegularization(   lavaanModels,   startingValues,   modifyModel )"},{"path":"/reference/dot-initializeMultiGroupSEMForRegularization.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".initializeMultiGroupSEMForRegularization — .initializeMultiGroupSEMForRegularization","text":"lavaanModels vector models class lavaan startingValues either set est, start, labeled vector starting values modifyModel user supplied model modifications","code":""},{"path":"/reference/dot-initializeMultiGroupSEMForRegularization.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".initializeMultiGroupSEMForRegularization — .initializeMultiGroupSEMForRegularization","text":"model used regularization procedure","code":""},{"path":"/reference/dot-initializeSEMForRegularization.html","id":null,"dir":"Reference","previous_headings":"","what":".initializeSEMForRegularization — .initializeSEMForRegularization","title":".initializeSEMForRegularization — .initializeSEMForRegularization","text":"initializes internal C++ SEM regularization functions","code":""},{"path":"/reference/dot-initializeSEMForRegularization.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".initializeSEMForRegularization — .initializeSEMForRegularization","text":"","code":".initializeSEMForRegularization(lavaanModel, startingValues, modifyModel)"},{"path":"/reference/dot-initializeSEMForRegularization.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".initializeSEMForRegularization — .initializeSEMForRegularization","text":"lavaanModel model class lavaan startingValues either set est, start, labeled vector starting values modifyModel user supplied model modifications","code":""},{"path":"/reference/dot-initializeSEMForRegularization.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".initializeSEMForRegularization — .initializeSEMForRegularization","text":"model used regularization procedure","code":""},{"path":"/reference/dot-initializeWeights.html","id":null,"dir":"Reference","previous_headings":"","what":".initializeWeights — .initializeWeights","title":".initializeWeights — .initializeWeights","text":"initialize adaptive lasso weights","code":""},{"path":"/reference/dot-initializeWeights.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".initializeWeights — .initializeWeights","text":"","code":".initializeWeights(   weights,   penalty,   method,   createAdaptiveLassoWeights,   control,   lavaanModel,   modifyModel,   startingValues,   rawParameters )"},{"path":"/reference/dot-initializeWeights.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".initializeWeights — .initializeWeights","text":"weights weight argument passed function penalty penalty used createAdaptiveLassoWeights adaptive lasso weights created? control list control elements optimizer lavaanModel model type lavaan modifyModel list model modifications startingValues either set est, start, labeled vector starting values rawParameters raw parameters","code":""},{"path":"/reference/dot-initializeWeights.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".initializeWeights — .initializeWeights","text":"vector weights","code":""},{"path":"/reference/dot-labelLavaanParameters.html","id":null,"dir":"Reference","previous_headings":"","what":".labelLavaanParameters — .labelLavaanParameters","title":".labelLavaanParameters — .labelLavaanParameters","text":"Adds labels unlabeled parameters lavaan parameter table. Also removes fixed parameters.","code":""},{"path":"/reference/dot-labelLavaanParameters.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".labelLavaanParameters — .labelLavaanParameters","text":"","code":".labelLavaanParameters(lavaanModel)"},{"path":"/reference/dot-labelLavaanParameters.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".labelLavaanParameters — .labelLavaanParameters","text":"lavaanModel fitted lavaan model","code":""},{"path":"/reference/dot-labelLavaanParameters.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".labelLavaanParameters — .labelLavaanParameters","text":"parameterTable labeled parameters","code":""},{"path":"/reference/dot-lavaan2regsemLabels.html","id":null,"dir":"Reference","previous_headings":"","what":".lavaan2regsemLabels — .lavaan2regsemLabels","title":".lavaan2regsemLabels — .lavaan2regsemLabels","text":"helper function: regsem lavaan use slightly different parameter labels. function can used get sets labels.","code":""},{"path":"/reference/dot-lavaan2regsemLabels.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".lavaan2regsemLabels — .lavaan2regsemLabels","text":"","code":".lavaan2regsemLabels(lavaanModel)"},{"path":"/reference/dot-lavaan2regsemLabels.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".lavaan2regsemLabels — .lavaan2regsemLabels","text":"lavaanModel model class lavaan","code":""},{"path":"/reference/dot-lavaan2regsemLabels.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".lavaan2regsemLabels — .lavaan2regsemLabels","text":"list lavaan regsem labels","code":""},{"path":"/reference/dot-likelihoodRatioFit.html","id":null,"dir":"Reference","previous_headings":"","what":".likelihoodRatioFit — .likelihoodRatioFit","title":".likelihoodRatioFit — .likelihoodRatioFit","text":"internal function returns likelihood ratio fit statistic","code":""},{"path":"/reference/dot-likelihoodRatioFit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".likelihoodRatioFit — .likelihoodRatioFit","text":"","code":".likelihoodRatioFit(par, SEM, raw)"},{"path":"/reference/dot-likelihoodRatioFit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".likelihoodRatioFit — .likelihoodRatioFit","text":"par labeled vector parameter values SEM model class Rcpp_SEMCpp. raw controls internal transformations lessSEM used.","code":""},{"path":"/reference/dot-likelihoodRatioFit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".likelihoodRatioFit — .likelihoodRatioFit","text":"likelihood ratio fit statistic","code":""},{"path":"/reference/dot-makeSingleLine.html","id":null,"dir":"Reference","previous_headings":"","what":".makeSingleLine — .makeSingleLine","title":".makeSingleLine — .makeSingleLine","text":"checks parameter: start: statement spans multiple lines reduces one line.","code":""},{"path":"/reference/dot-makeSingleLine.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".makeSingleLine — .makeSingleLine","text":"","code":".makeSingleLine(syntax, what)"},{"path":"/reference/dot-makeSingleLine.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".makeSingleLine — .makeSingleLine","text":"syntax reduced syntax statement look (parameters start)","code":""},{"path":"/reference/dot-makeSingleLine.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".makeSingleLine — .makeSingleLine","text":"syntax multi-line statements condensed one line","code":""},{"path":"/reference/dot-multiGroupSEMFromLavaan.html","id":null,"dir":"Reference","previous_headings":"","what":".multiGroupSEMFromLavaan — .multiGroupSEMFromLavaan","title":".multiGroupSEMFromLavaan — .multiGroupSEMFromLavaan","text":"internal function. Translates vector objects class lavaan internal model representation.","code":""},{"path":"/reference/dot-multiGroupSEMFromLavaan.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".multiGroupSEMFromLavaan — .multiGroupSEMFromLavaan","text":"","code":".multiGroupSEMFromLavaan(   lavaanModels,   whichPars = \"est\",   fit = TRUE,   addMeans = TRUE,   transformations = NULL,   transformationList = list(),   transformationGradientStepSize = 1e-06 )"},{"path":"/reference/dot-multiGroupSEMFromLavaan.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".multiGroupSEMFromLavaan — .multiGroupSEMFromLavaan","text":"lavaanModels vector lavaan models whichPars parameters used initialize model. set \"est\", parameters set estimated parameters lavaan model. set \"start\", starting values lavaan used. latter can useful parameters optimized afterwards setting parameters \"est\" may result model getting stuck local minimum. fit model fitted addMeans lavaanModel meanstructure = FALSE, addMeans = TRUE add mean structure. FALSE set means observed variables average transformations string transformations transformationList list transformations transformationGradientStepSize step size used compute gradients transformations","code":""},{"path":"/reference/dot-multiGroupSEMFromLavaan.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".multiGroupSEMFromLavaan — .multiGroupSEMFromLavaan","text":"Object class Rcpp_mgSEMCpp","code":""},{"path":"/reference/dot-noDotDotDot.html","id":null,"dir":"Reference","previous_headings":"","what":".noDotDotDot — .noDotDotDot","title":".noDotDotDot — .noDotDotDot","text":"remplaces dot dot dot part fitting gradient fuction","code":""},{"path":"/reference/dot-noDotDotDot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".noDotDotDot — .noDotDotDot","text":"","code":".noDotDotDot(fn, fnName, ...)"},{"path":"/reference/dot-noDotDotDot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".noDotDotDot — .noDotDotDot","text":"fn fit gradient function. IMPORTANT: FIRST ARGUMENT FUNCTION MUST PARAMETER VECTOR fnName name function fn ... additional arguments","code":""},{"path":"/reference/dot-noDotDotDot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".noDotDotDot — .noDotDotDot","text":"list (1) new function wraps fn (2) list arguments passed fn","code":""},{"path":"/reference/dot-penaltyTypes.html","id":null,"dir":"Reference","previous_headings":"","what":".penaltyTypes — .penaltyTypes","title":".penaltyTypes — .penaltyTypes","text":"translates penalty numeric value character character numeric value. numeric value used C++ backend.","code":""},{"path":"/reference/dot-penaltyTypes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".penaltyTypes — .penaltyTypes","text":"","code":".penaltyTypes(penalty)"},{"path":"/reference/dot-penaltyTypes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".penaltyTypes — .penaltyTypes","text":"penalty either number name penalty","code":""},{"path":"/reference/dot-penaltyTypes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".penaltyTypes — .penaltyTypes","text":"number corresponding one penalties","code":""},{"path":"/reference/dot-reduceSyntax.html","id":null,"dir":"Reference","previous_headings":"","what":".reduceSyntax — .reduceSyntax","title":".reduceSyntax — .reduceSyntax","text":"reduce user defined parameter transformation syntax basic elements","code":""},{"path":"/reference/dot-reduceSyntax.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".reduceSyntax — .reduceSyntax","text":"","code":".reduceSyntax(syntax)"},{"path":"/reference/dot-reduceSyntax.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".reduceSyntax — .reduceSyntax","text":"syntax string user defined transformations","code":""},{"path":"/reference/dot-reduceSyntax.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".reduceSyntax — .reduceSyntax","text":"cut simplified version syntax","code":""},{"path":"/reference/dot-regularizeSEMInternal.html","id":null,"dir":"Reference","previous_headings":"","what":".regularizeSEMInternal — .regularizeSEMInternal","title":".regularizeSEMInternal — .regularizeSEMInternal","text":"Internal function: function computes regularized models penaltiy functions implemented glmnet gist. Use dedicated penalty functions (e.g., lessSEM::lasso) penalize model.","code":""},{"path":"/reference/dot-regularizeSEMInternal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".regularizeSEMInternal — .regularizeSEMInternal","text":"","code":".regularizeSEMInternal(   lavaanModel,   penalty,   weights,   tuningParameters,   method,   modifyModel,   control,   notes = NULL )"},{"path":"/reference/dot-regularizeSEMInternal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".regularizeSEMInternal — .regularizeSEMInternal","text":"lavaanModel model class lavaan penalty string: name penalty used model weights labeled vector weights parameters model. tuningParameters data.frame tuning parameter values method optimizer used? Currently implemented ista glmnet. ista, control argument can used switch related procedures (currently gist). modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. element generated controlIsta() controlGlmnet() functions. notes option pass notes function. notes current function added","code":""},{"path":"/reference/dot-regularizeSEMInternal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".regularizeSEMInternal — .regularizeSEMInternal","text":"regularized SEM","code":""},{"path":"/reference/dot-regularizeSEMWithCustomPenaltyRsolnp.html","id":null,"dir":"Reference","previous_headings":"","what":".regularizeSEMWithCustomPenaltyRsolnp — .regularizeSEMWithCustomPenaltyRsolnp","title":".regularizeSEMWithCustomPenaltyRsolnp — .regularizeSEMWithCustomPenaltyRsolnp","text":"Optimize SEM custom penalty function using Rsolnp optimizer (see ?Rsolnp::solnp). optimizer default regsem (see ?regsem::cv_regsem).","code":""},{"path":"/reference/dot-regularizeSEMWithCustomPenaltyRsolnp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".regularizeSEMWithCustomPenaltyRsolnp — .regularizeSEMWithCustomPenaltyRsolnp","text":"","code":".regularizeSEMWithCustomPenaltyRsolnp(   lavaanModel,   individualPenaltyFunction,   tuningParameters,   penaltyFunctionArguments,   startingValues = \"est\",   carryOverParameters = TRUE,   control = list(trace = 0) )"},{"path":"/reference/dot-regularizeSEMWithCustomPenaltyRsolnp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".regularizeSEMWithCustomPenaltyRsolnp — .regularizeSEMWithCustomPenaltyRsolnp","text":"lavaanModel model class lavaan individualPenaltyFunction penalty function takes current parameter values first argument, tuning parameters second, penaltyFunctionArguments third argument returns single value - value penalty function single person. true penalty function non-differentiable (e.g., lasso) smooth approximation function provided. tuningParameters data.frame tuning parameter values. Important: function iterate rows tuning parameters pass penalty function penaltyFunctionArguments arguments passed individualPenaltyFunction, individualPenaltyFunctionGradient, individualPenaltyFunctionHessian startingValues option provide initial starting values. used first lambda. Three options supported. Setting \"est\" use estimates lavaan model object. Setting \"start\" use starting values lavaan model. Finally, labeled vector parameter values can passed function used starting values. carryOverParameters parameters previous iteration used starting values next iteration? control option set parameters optimizer; see ?Rsolnp::solnp","code":""},{"path":"/reference/dot-regularizeSEMWithCustomPenaltyRsolnp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".regularizeSEMWithCustomPenaltyRsolnp — .regularizeSEMWithCustomPenaltyRsolnp","text":"Model class regularizedSEMWithCustomPenalty","code":""},{"path":"/reference/dot-regularizeSmoothSEMInternal.html","id":null,"dir":"Reference","previous_headings":"","what":".regularizeSmoothSEMInternal — .regularizeSmoothSEMInternal","title":".regularizeSmoothSEMInternal — .regularizeSmoothSEMInternal","text":"Internal function: function computes regularized models smooth penalty functions implemented bfgs. Use dedicated penalty functions (e.g., lessSEM::smoothLasso) penalize model.","code":""},{"path":"/reference/dot-regularizeSmoothSEMInternal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".regularizeSmoothSEMInternal — .regularizeSmoothSEMInternal","text":"","code":".regularizeSmoothSEMInternal(   lavaanModel,   penalty,   weights,   tuningParameters,   epsilon,   tau,   method = \"bfgs\",   modifyModel,   control,   notes = NULL )"},{"path":"/reference/dot-regularizeSmoothSEMInternal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".regularizeSmoothSEMInternal — .regularizeSmoothSEMInternal","text":"lavaanModel model class lavaan penalty string: name penalty used model weights labeled vector weights parameters model. tuningParameters data.frame tuning parameter values epsilon epsilon > 0; controls smoothness approximation. Larger values = smoother tau parameters threshold tau seen zeroed method optimizer used. Currently \"bfgs\" supported. modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. element generated controlBFGS function. See ?controlBFGS details. notes option pass notes function. notes current function added","code":""},{"path":"/reference/dot-regularizeSmoothSEMInternal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".regularizeSmoothSEMInternal — .regularizeSmoothSEMInternal","text":"regularizedSEM","code":""},{"path":"/reference/dot-ridgeGradient.html","id":null,"dir":"Reference","previous_headings":"","what":".ridgeGradient — .ridgeGradient","title":".ridgeGradient — .ridgeGradient","text":"ridge gradient function","code":""},{"path":"/reference/dot-ridgeGradient.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".ridgeGradient — .ridgeGradient","text":"","code":".ridgeGradient(parameters, tuningParameters, penaltyFunctionArguments)"},{"path":"/reference/dot-ridgeGradient.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".ridgeGradient — .ridgeGradient","text":"parameters vector labeled parameter values tuningParameters list field lambda (tuning parameter value) penaltyFunctionArguments list field regularizedParameterLabels (labels regularized parameters)","code":""},{"path":"/reference/dot-ridgeGradient.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".ridgeGradient — .ridgeGradient","text":"gradient values","code":""},{"path":"/reference/dot-ridgeHessian.html","id":null,"dir":"Reference","previous_headings":"","what":".ridgeHessian — .ridgeHessian","title":".ridgeHessian — .ridgeHessian","text":"ridge Hessian function","code":""},{"path":"/reference/dot-ridgeHessian.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".ridgeHessian — .ridgeHessian","text":"","code":".ridgeHessian(parameters, tuningParameters, penaltyFunctionArguments)"},{"path":"/reference/dot-ridgeHessian.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".ridgeHessian — .ridgeHessian","text":"parameters vector labeled parameter values tuningParameters list field lambda (tuning parameter value) penaltyFunctionArguments list field regularizedParameterLabels (labels regularized parameters)","code":""},{"path":"/reference/dot-ridgeHessian.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".ridgeHessian — .ridgeHessian","text":"Hessian matrix","code":""},{"path":"/reference/dot-ridgeValue.html","id":null,"dir":"Reference","previous_headings":"","what":".ridgeValue — .ridgeValue","title":".ridgeValue — .ridgeValue","text":"ridge penalty function","code":""},{"path":"/reference/dot-ridgeValue.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".ridgeValue — .ridgeValue","text":"","code":".ridgeValue(parameters, tuningParameters, penaltyFunctionArguments)"},{"path":"/reference/dot-ridgeValue.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".ridgeValue — .ridgeValue","text":"parameters vector labeled parameter values tuningParameters list field lambda (tuning parameter value) penaltyFunctionArguments list field regularizedParameterLabels (labels regularized parameters)","code":""},{"path":"/reference/dot-ridgeValue.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".ridgeValue — .ridgeValue","text":"penalty function value","code":""},{"path":"/reference/dot-setAMatrix.html","id":null,"dir":"Reference","previous_headings":"","what":".setAMatrix — .setAMatrix","title":".setAMatrix — .setAMatrix","text":"internal function. Populates matrix directed effects RAM notation","code":""},{"path":"/reference/dot-setAMatrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".setAMatrix — .setAMatrix","text":"","code":".setAMatrix(   model,   lavaanParameterTable,   nLatent,   nManifest,   latentNames,   manifestNames )"},{"path":"/reference/dot-setAMatrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".setAMatrix — .setAMatrix","text":"model model class lavaan lavaanParameterTable parameter table lavaan nLatent number latent variables nManifest number manifest variables latentNames names latent variables manifestNames names manifest variables","code":""},{"path":"/reference/dot-setFmatrix.html","id":null,"dir":"Reference","previous_headings":"","what":".setFmatrix — .setFmatrix","title":".setFmatrix — .setFmatrix","text":"returns filter matrix RAM","code":""},{"path":"/reference/dot-setFmatrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".setFmatrix — .setFmatrix","text":"","code":".setFmatrix(nManifest, manifestNames, nLatent, latentNames)"},{"path":"/reference/dot-setFmatrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".setFmatrix — .setFmatrix","text":"nManifest number manifest variables manifestNames names manifest variables nLatent number latent variables latentNames names latent variables","code":""},{"path":"/reference/dot-setFmatrix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".setFmatrix — .setFmatrix","text":"matrix","code":""},{"path":"/reference/dot-setMVector.html","id":null,"dir":"Reference","previous_headings":"","what":".setMVector — .setMVector","title":".setMVector — .setMVector","text":"internal function. Populates vector means RAM notation","code":""},{"path":"/reference/dot-setMVector.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".setMVector — .setMVector","text":"","code":".setMVector(   model,   lavaanParameterTable,   nLatent,   nManifest,   latentNames,   manifestNames,   rawData )"},{"path":"/reference/dot-setMVector.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".setMVector — .setMVector","text":"model model class lavaan lavaanParameterTable parameter table lavaan nLatent number latent variables nManifest number manifest variables latentNames names latent variables manifestNames names manifest variables rawData matrix raw data","code":""},{"path":"/reference/dot-setParameters.html","id":null,"dir":"Reference","previous_headings":"","what":".setParameters — .setParameters","title":".setParameters — .setParameters","text":"change parameters internal model representation.","code":""},{"path":"/reference/dot-setParameters.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".setParameters — .setParameters","text":"","code":".setParameters(SEM, labels, values, raw)"},{"path":"/reference/dot-setParameters.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".setParameters — .setParameters","text":"SEM model class Rcpp_SEMCpp. Models class labels vector parameter labels values vector parameter values raw parameters given raw format transformed?","code":""},{"path":"/reference/dot-setParameters.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".setParameters — .setParameters","text":"SEM changed parameter values","code":""},{"path":"/reference/dot-setSMatrix.html","id":null,"dir":"Reference","previous_headings":"","what":".setSMatrix — .setSMatrix","title":".setSMatrix — .setSMatrix","text":"internal function. Populates matrix undirected paths RAM notation","code":""},{"path":"/reference/dot-setSMatrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".setSMatrix — .setSMatrix","text":"","code":".setSMatrix(   model,   lavaanParameterTable,   nLatent,   nManifest,   latentNames,   manifestNames )"},{"path":"/reference/dot-setSMatrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".setSMatrix — .setSMatrix","text":"model model class lavaan lavaanParameterTable parameter table lavaan nLatent number latent variables nManifest number manifest variables latentNames names latent variables manifestNames names manifest variables","code":""},{"path":"/reference/dot-setupMulticore.html","id":null,"dir":"Reference","previous_headings":"","what":".setupMulticore — .setupMulticore","title":".setupMulticore — .setupMulticore","text":"setup multi-core support","code":""},{"path":"/reference/dot-setupMulticore.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".setupMulticore — .setupMulticore","text":"","code":".setupMulticore(control)"},{"path":"/reference/dot-setupMulticore.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".setupMulticore — .setupMulticore","text":"control object created controlBFGS, controlIsta controlGlmnet function","code":""},{"path":"/reference/dot-setupMulticore.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".setupMulticore — .setupMulticore","text":"nothing","code":""},{"path":"/reference/dot-smoothAdaptiveLASSOGradient.html","id":null,"dir":"Reference","previous_headings":"","what":".smoothAdaptiveLASSOGradient — .smoothAdaptiveLASSOGradient","title":".smoothAdaptiveLASSOGradient — .smoothAdaptiveLASSOGradient","text":"smoothed version non-differentiable adaptive LASSO gradient","code":""},{"path":"/reference/dot-smoothAdaptiveLASSOGradient.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".smoothAdaptiveLASSOGradient — .smoothAdaptiveLASSOGradient","text":"","code":".smoothAdaptiveLASSOGradient(   parameters,   tuningParameters,   penaltyFunctionArguments )"},{"path":"/reference/dot-smoothAdaptiveLASSOGradient.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".smoothAdaptiveLASSOGradient — .smoothAdaptiveLASSOGradient","text":"parameters vector labeled parameter values tuningParameters list fields lambdas (vector one tuning parameter value parameter) penaltyFunctionArguments list fields regularizedParameterLabels (labels regularized parameters), eps (controls smooth approximation non-differential penalty functions (e.g., lasso, adaptive lasso, elastic net). Smaller values result closer approximation, may also cause larger issues optimization.)","code":""},{"path":"/reference/dot-smoothAdaptiveLASSOGradient.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".smoothAdaptiveLASSOGradient — .smoothAdaptiveLASSOGradient","text":"gradient values","code":""},{"path":"/reference/dot-smoothAdaptiveLASSOHessian.html","id":null,"dir":"Reference","previous_headings":"","what":".smoothAdaptiveLASSOHessian — .smoothAdaptiveLASSOHessian","title":".smoothAdaptiveLASSOHessian — .smoothAdaptiveLASSOHessian","text":"smoothed version non-differentiable adaptive LASSO Hessian","code":""},{"path":"/reference/dot-smoothAdaptiveLASSOHessian.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".smoothAdaptiveLASSOHessian — .smoothAdaptiveLASSOHessian","text":"","code":".smoothAdaptiveLASSOHessian(   parameters,   tuningParameters,   penaltyFunctionArguments )"},{"path":"/reference/dot-smoothAdaptiveLASSOHessian.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".smoothAdaptiveLASSOHessian — .smoothAdaptiveLASSOHessian","text":"parameters vector labeled parameter values tuningParameters list fields lambdas (vector one tuning parameter value parameter) penaltyFunctionArguments list fields regularizedParameterLabels (labels regularized parameters), eps (controls smooth approximation non-differential penalty functions (e.g., lasso, adaptive lasso, elastic net). Smaller values result closer approximation, may also cause larger issues optimization.)","code":""},{"path":"/reference/dot-smoothAdaptiveLASSOHessian.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".smoothAdaptiveLASSOHessian — .smoothAdaptiveLASSOHessian","text":"Hessian matrix","code":""},{"path":"/reference/dot-smoothAdaptiveLASSOValue.html","id":null,"dir":"Reference","previous_headings":"","what":".smoothAdaptiveLASSOValue — .smoothAdaptiveLASSOValue","title":".smoothAdaptiveLASSOValue — .smoothAdaptiveLASSOValue","text":"smoothed version non-differentiable adaptive LASSO penalty","code":""},{"path":"/reference/dot-smoothAdaptiveLASSOValue.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".smoothAdaptiveLASSOValue — .smoothAdaptiveLASSOValue","text":"","code":".smoothAdaptiveLASSOValue(   parameters,   tuningParameters,   penaltyFunctionArguments )"},{"path":"/reference/dot-smoothAdaptiveLASSOValue.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".smoothAdaptiveLASSOValue — .smoothAdaptiveLASSOValue","text":"parameters vector labeled parameter values tuningParameters list fields lambdas (vector one tuning parameter value parameter) penaltyFunctionArguments list fields regularizedParameterLabels (labels regularized parameters), eps (controls smooth approximation non-differential penalty functions (e.g., lasso, adaptive lasso, elastic net). Smaller values result closer approximation, may also cause larger issues optimization.)","code":""},{"path":"/reference/dot-smoothAdaptiveLASSOValue.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".smoothAdaptiveLASSOValue — .smoothAdaptiveLASSOValue","text":"penalty function value","code":""},{"path":"/reference/dot-smoothCappedL1Value.html","id":null,"dir":"Reference","previous_headings":"","what":".smoothCappedL1Value — .smoothCappedL1Value","title":".smoothCappedL1Value — .smoothCappedL1Value","text":"smoothed version capped L1 penalty","code":""},{"path":"/reference/dot-smoothCappedL1Value.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".smoothCappedL1Value — .smoothCappedL1Value","text":"","code":".smoothCappedL1Value(parameters, tuningParameters, penaltyFunctionArguments)"},{"path":"/reference/dot-smoothCappedL1Value.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".smoothCappedL1Value — .smoothCappedL1Value","text":"parameters vector labeled parameter values tuningParameters list field lambda (tuning parameter value) penaltyFunctionArguments list field regularizedParameterLabels (labels regularized parameters), eps (controls smooth approximation non-differential penalty functions (e.g., lasso, adaptive lasso, elastic net). Smaller values result closer approximation, may also cause larger issues optimization.)","code":""},{"path":"/reference/dot-smoothCappedL1Value.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".smoothCappedL1Value — .smoothCappedL1Value","text":"penalty function value","code":""},{"path":"/reference/dot-smoothElasticNetGradient.html","id":null,"dir":"Reference","previous_headings":"","what":".smoothElasticNetGradient — .smoothElasticNetGradient","title":".smoothElasticNetGradient — .smoothElasticNetGradient","text":"smoothed version non-differentiable elastic LASSO gradient","code":""},{"path":"/reference/dot-smoothElasticNetGradient.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".smoothElasticNetGradient — .smoothElasticNetGradient","text":"","code":".smoothElasticNetGradient(   parameters,   tuningParameters,   penaltyFunctionArguments )"},{"path":"/reference/dot-smoothElasticNetGradient.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".smoothElasticNetGradient — .smoothElasticNetGradient","text":"parameters vector labeled parameter values tuningParameters list fields lambda (tuning parameter value), alpha (0<alpha<1. Controls weight ridge lasso terms. alpha = 1 lasso, alpha = 0 ridge) penaltyFunctionArguments list fields regularizedParameterLabels (labels regularized parameters), eps (controls smooth approximation non-differential penalty functions (e.g., lasso, adaptive lasso, elastic net). Smaller values result closer approximation, may also cause larger issues optimization.)","code":""},{"path":"/reference/dot-smoothElasticNetGradient.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".smoothElasticNetGradient — .smoothElasticNetGradient","text":"gradient values","code":""},{"path":"/reference/dot-smoothElasticNetHessian.html","id":null,"dir":"Reference","previous_headings":"","what":".smoothElasticNetHessian — .smoothElasticNetHessian","title":".smoothElasticNetHessian — .smoothElasticNetHessian","text":"smoothed version non-differentiable elastic LASSO Hessian","code":""},{"path":"/reference/dot-smoothElasticNetHessian.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".smoothElasticNetHessian — .smoothElasticNetHessian","text":"","code":".smoothElasticNetHessian(   parameters,   tuningParameters,   penaltyFunctionArguments )"},{"path":"/reference/dot-smoothElasticNetHessian.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".smoothElasticNetHessian — .smoothElasticNetHessian","text":"parameters vector labeled parameter values tuningParameters list fields lambda (tuning parameter value), alpha (0<alpha<1. Controls weight ridge lasso terms. alpha = 1 lasso, alpha = 0 ridge) penaltyFunctionArguments list fields regularizedParameterLabels (labels regularized parameters), eps (controls smooth approximation non-differential penalty functions (e.g., lasso, adaptive lasso, elastic net). Smaller values result closer approximation, may also cause larger issues optimization.)","code":""},{"path":"/reference/dot-smoothElasticNetHessian.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".smoothElasticNetHessian — .smoothElasticNetHessian","text":"Hessian matrix","code":""},{"path":"/reference/dot-smoothElasticNetValue.html","id":null,"dir":"Reference","previous_headings":"","what":".smoothElasticNetValue — .smoothElasticNetValue","title":".smoothElasticNetValue — .smoothElasticNetValue","text":"smoothed version non-differentiable elastic LASSO penalty","code":""},{"path":"/reference/dot-smoothElasticNetValue.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".smoothElasticNetValue — .smoothElasticNetValue","text":"","code":".smoothElasticNetValue(parameters, tuningParameters, penaltyFunctionArguments)"},{"path":"/reference/dot-smoothElasticNetValue.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".smoothElasticNetValue — .smoothElasticNetValue","text":"parameters vector labeled parameter values tuningParameters list fields lambda (tuning parameter value), alpha (0<alpha<1. Controls weight ridge lasso terms. alpha = 1 lasso, alpha = 0 ridge) penaltyFunctionArguments list fields regularizedParameterLabels (labels regularized parameters), eps (controls smooth approximation non-differential penalty functions (e.g., lasso, adaptive lasso, elastic net). Smaller values result closer approximation, may also cause larger issues optimization.)","code":""},{"path":"/reference/dot-smoothElasticNetValue.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".smoothElasticNetValue — .smoothElasticNetValue","text":"penalty function value","code":""},{"path":"/reference/dot-smoothLASSOGradient.html","id":null,"dir":"Reference","previous_headings":"","what":".smoothLASSOGradient — .smoothLASSOGradient","title":".smoothLASSOGradient — .smoothLASSOGradient","text":"smoothed version non-differentiable LASSO gradient","code":""},{"path":"/reference/dot-smoothLASSOGradient.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".smoothLASSOGradient — .smoothLASSOGradient","text":"","code":".smoothLASSOGradient(parameters, tuningParameters, penaltyFunctionArguments)"},{"path":"/reference/dot-smoothLASSOGradient.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".smoothLASSOGradient — .smoothLASSOGradient","text":"parameters vector labeled parameter values tuningParameters list field lambda (tuning parameter value) penaltyFunctionArguments list field regularizedParameterLabels (labels regularized parameters), eps (controls smooth approximation non-differential penalty functions (e.g., lasso, adaptive lasso, elastic net). Smaller values result closer approximation, may also cause larger issues optimization.)","code":""},{"path":"/reference/dot-smoothLASSOGradient.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".smoothLASSOGradient — .smoothLASSOGradient","text":"gradient values","code":""},{"path":"/reference/dot-smoothLASSOHessian.html","id":null,"dir":"Reference","previous_headings":"","what":".smoothLASSOHessian — .smoothLASSOHessian","title":".smoothLASSOHessian — .smoothLASSOHessian","text":"smoothed version non-differentiable LASSO Hessian","code":""},{"path":"/reference/dot-smoothLASSOHessian.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".smoothLASSOHessian — .smoothLASSOHessian","text":"","code":".smoothLASSOHessian(parameters, tuningParameters, penaltyFunctionArguments)"},{"path":"/reference/dot-smoothLASSOHessian.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".smoothLASSOHessian — .smoothLASSOHessian","text":"parameters vector labeled parameter values tuningParameters list field lambda (tuning parameter value) penaltyFunctionArguments list field regularizedParameterLabels (labels regularized parameters), eps (controls smooth approximation non-differential penalty functions (e.g., lasso, adaptive lasso, elastic net). Smaller values result closer approximation, may also cause larger issues optimization.)","code":""},{"path":"/reference/dot-smoothLASSOHessian.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".smoothLASSOHessian — .smoothLASSOHessian","text":"Hessian matrix","code":""},{"path":"/reference/dot-smoothLASSOValue.html","id":null,"dir":"Reference","previous_headings":"","what":".smoothLASSOValue — .smoothLASSOValue","title":".smoothLASSOValue — .smoothLASSOValue","text":"smoothed version non-differentiable LASSO penalty","code":""},{"path":"/reference/dot-smoothLASSOValue.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".smoothLASSOValue — .smoothLASSOValue","text":"","code":".smoothLASSOValue(parameters, tuningParameters, penaltyFunctionArguments)"},{"path":"/reference/dot-smoothLASSOValue.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".smoothLASSOValue — .smoothLASSOValue","text":"parameters vector labeled parameter values tuningParameters list field lambda (tuning parameter value) penaltyFunctionArguments list field regularizedParameterLabels (labels regularized parameters), eps (controls smooth approximation non-differential penalty functions (e.g., lasso, adaptive lasso, elastic net). Smaller values result closer approximation, may also cause larger issues optimization.)","code":""},{"path":"/reference/dot-smoothLASSOValue.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".smoothLASSOValue — .smoothLASSOValue","text":"penalty function value","code":""},{"path":"/reference/dot-smoothLspValue.html","id":null,"dir":"Reference","previous_headings":"","what":".smoothLspValue — .smoothLspValue","title":".smoothLspValue — .smoothLspValue","text":"smoothed version lsp penalty","code":""},{"path":"/reference/dot-smoothLspValue.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".smoothLspValue — .smoothLspValue","text":"","code":".smoothLspValue(parameters, tuningParameters, penaltyFunctionArguments)"},{"path":"/reference/dot-smoothLspValue.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".smoothLspValue — .smoothLspValue","text":"parameters vector labeled parameter values tuningParameters list field lambda (tuning parameter value) penaltyFunctionArguments list field regularizedParameterLabels (labels regularized parameters), eps (controls smooth approximation non-differential penalty functions (e.g., lasso, adaptive lasso, elastic net). Smaller values result closer approximation, may also cause larger issues optimization.)","code":""},{"path":"/reference/dot-smoothLspValue.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".smoothLspValue — .smoothLspValue","text":"penalty function value","code":""},{"path":"/reference/dot-smoothMcpValue.html","id":null,"dir":"Reference","previous_headings":"","what":".smoothMcpValue — .smoothMcpValue","title":".smoothMcpValue — .smoothMcpValue","text":"smoothed version mcp penalty","code":""},{"path":"/reference/dot-smoothMcpValue.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".smoothMcpValue — .smoothMcpValue","text":"","code":".smoothMcpValue(parameters, tuningParameters, penaltyFunctionArguments)"},{"path":"/reference/dot-smoothMcpValue.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".smoothMcpValue — .smoothMcpValue","text":"parameters vector labeled parameter values tuningParameters list field lambda (tuning parameter value) penaltyFunctionArguments list field regularizedParameterLabels (labels regularized parameters), eps (controls smooth approximation non-differential penalty functions (e.g., lasso, adaptive lasso, elastic net). Smaller values result closer approximation, may also cause larger issues optimization.)","code":""},{"path":"/reference/dot-smoothMcpValue.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".smoothMcpValue — .smoothMcpValue","text":"penalty function value","code":""},{"path":"/reference/dot-smoothScadValue.html","id":null,"dir":"Reference","previous_headings":"","what":".smoothScadValue — .smoothScadValue","title":".smoothScadValue — .smoothScadValue","text":"smoothed version scad penalty","code":""},{"path":"/reference/dot-smoothScadValue.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".smoothScadValue — .smoothScadValue","text":"","code":".smoothScadValue(parameters, tuningParameters, penaltyFunctionArguments)"},{"path":"/reference/dot-smoothScadValue.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".smoothScadValue — .smoothScadValue","text":"parameters vector labeled parameter values tuningParameters list field lambda (tuning parameter value) penaltyFunctionArguments list field regularizedParameterLabels (labels regularized parameters), eps (controls smooth approximation non-differential penalty functions (e.g., lasso, adaptive lasso, elastic net). Smaller values result closer approximation, may also cause larger issues optimization.)","code":""},{"path":"/reference/dot-smoothScadValue.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".smoothScadValue — .smoothScadValue","text":"penalty function value","code":""},{"path":"/reference/dot-standardErrors.html","id":null,"dir":"Reference","previous_headings":"","what":".standardErrors — .standardErrors","title":".standardErrors — .standardErrors","text":"compute standard errors fitted SEM. IMPORTANT: Assumes SEM fitted parameter estimates ordinary maximum likelihood estimates","code":""},{"path":"/reference/dot-standardErrors.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".standardErrors — .standardErrors","text":"","code":".standardErrors(SEM, raw)"},{"path":"/reference/dot-standardErrors.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".standardErrors — .standardErrors","text":"SEM model class Rcpp_SEMCpp. raw controls internal transformations lessSEM used. set TRUE, standard errors returned internally used parameter specification","code":""},{"path":"/reference/dot-standardErrors.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".standardErrors — .standardErrors","text":"vector standard errors","code":""},{"path":"/reference/dot-updateLavaan.html","id":null,"dir":"Reference","previous_headings":"","what":".updateLavaan — .updateLavaan","title":".updateLavaan — .updateLavaan","text":"updates lavaan model. lavaan update function exactly , seems work testthat. attempt hack around issue...","code":""},{"path":"/reference/dot-updateLavaan.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".updateLavaan — .updateLavaan","text":"","code":".updateLavaan(lavaanModel, key, value)"},{"path":"/reference/dot-updateLavaan.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".updateLavaan — .updateLavaan","text":"lavaanModel fitted lavaan model key label element updated value new value updated element","code":""},{"path":"/reference/dot-updateLavaan.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".updateLavaan — .updateLavaan","text":"lavaan model","code":""},{"path":"/reference/dot-useElasticNet.html","id":null,"dir":"Reference","previous_headings":"","what":".useElasticNet — .useElasticNet","title":".useElasticNet — .useElasticNet","text":"Internal function checking elastic net used","code":""},{"path":"/reference/dot-useElasticNet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".useElasticNet — .useElasticNet","text":"","code":".useElasticNet(mixedPenalty)"},{"path":"/reference/dot-useElasticNet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".useElasticNet — .useElasticNet","text":"mixedPenalty object class mixedPenalty. object can created mixedPenalty function. Penalties can added addCappedL1, addLasso, addLsp, addMcp, addScad functions.","code":""},{"path":"/reference/dot-useElasticNet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".useElasticNet — .useElasticNet","text":"TRUE elastic net, FALSE otherwise","code":""},{"path":"/reference/elasticNet.html","id":null,"dir":"Reference","previous_headings":"","what":"elasticNet — elasticNet","title":"elasticNet — elasticNet","text":"Implements elastic net regularization structural equation models. penalty function given : $$p( x_j) = \\alpha\\lambda| x_j| + (1-\\alpha)\\lambda x_j^2$$ Note elastic net combines ridge lasso regularization. \\(\\alpha = 0\\), elastic net reduces ridge regularization. \\(\\alpha = 1\\) reduces lasso regularization. , elastic net compromise shrinkage lasso ridge penalty.","code":""},{"path":"/reference/elasticNet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"elasticNet — elasticNet","text":"","code":"elasticNet(   lavaanModel,   regularized,   lambdas,   alphas,   method = \"glmnet\",   modifyModel = lessSEM::modifyModel(),   control = lessSEM::controlGlmnet() )"},{"path":"/reference/elasticNet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"elasticNet — elasticNet","text":"lavaanModel model class lavaan regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda alphas numeric vector values tuning parameter alpha. Must 0 1. 0 = ridge, 1 = lasso. method optimizer used? Currently implemented ista glmnet. ista, control argument can used switch related procedures (currently gist). modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. element generated lessSEM::controlIsta() controlGlmnet() functions.","code":""},{"path":"/reference/elasticNet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"elasticNet — elasticNet","text":"Model class regularizedSEM","code":""},{"path":"/reference/elasticNet.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"elasticNet — elasticNet","text":"Identical regsem, models specified using lavaan. Currently, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. Elastic net regularization: Zou, H., & Hastie, T. (2005). Regularization variable selection via elastic net. Journal Royal Statistical Society: Series B, 67(2), 301–320. https://doi.org/10.1111/j.1467-9868.2005.00503.x Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classification. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/elasticNet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"elasticNet — elasticNet","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +      l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +      l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Regularization:  lsem <- elasticNet(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   # names of the regularized parameters:   regularized = paste0(\"l\", 6:15),   lambdas = seq(0,1,length.out = 5),   alphas = seq(0,1,length.out = 3))  # the coefficients can be accessed with: coef(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters[1,]  # optional: plotting the paths requires installation of plotly # plot(lsem)  #### Advanced ### # Switching the optimizer # # Use the \"method\" argument to switch the optimizer. The control argument # must also be changed to the corresponding function: lsemIsta <- elasticNet(   lavaanModel = lavaanModel,   regularized = paste0(\"l\", 6:15),   lambdas = seq(0,1,length.out = 5),   alphas = seq(0,1,length.out = 3),   method = \"ista\",   control = controlIsta())  # Note: The results are basically identical: lsemIsta@parameters - lsem@parameters"},{"path":"/reference/estimates-cvRegularizedSEM-method.html","id":null,"dir":"Reference","previous_headings":"","what":"estimates — estimates,cvRegularizedSEM-method","title":"estimates — estimates,cvRegularizedSEM-method","text":"estimates","code":""},{"path":"/reference/estimates-cvRegularizedSEM-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"estimates — estimates,cvRegularizedSEM-method","text":"","code":"# S4 method for cvRegularizedSEM estimates(object, criterion = NULL, transformations = FALSE)"},{"path":"/reference/estimates-cvRegularizedSEM-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"estimates — estimates,cvRegularizedSEM-method","text":"object object class cvRegularizedSEM criterion used transformations boolean: transformations returned?","code":""},{"path":"/reference/estimates-cvRegularizedSEM-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"estimates — estimates,cvRegularizedSEM-method","text":"returns matrix estimates","code":""},{"path":"/reference/estimates-regularizedSEM-method.html","id":null,"dir":"Reference","previous_headings":"","what":"estimates — estimates,regularizedSEM-method","title":"estimates — estimates,regularizedSEM-method","text":"estimates","code":""},{"path":"/reference/estimates-regularizedSEM-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"estimates — estimates,regularizedSEM-method","text":"","code":"# S4 method for regularizedSEM estimates(object, criterion = NULL, transformations = FALSE)"},{"path":"/reference/estimates-regularizedSEM-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"estimates — estimates,regularizedSEM-method","text":"object object class regularizedSEM criterion fit index (e.g., AIC) used select parameters transformations boolean: transformations returned?","code":""},{"path":"/reference/estimates-regularizedSEM-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"estimates — estimates,regularizedSEM-method","text":"returns matrix estimates","code":""},{"path":"/reference/estimates-regularizedSEMMixedPenalty-method.html","id":null,"dir":"Reference","previous_headings":"","what":"estimates — estimates,regularizedSEMMixedPenalty-method","title":"estimates — estimates,regularizedSEMMixedPenalty-method","text":"estimates","code":""},{"path":"/reference/estimates-regularizedSEMMixedPenalty-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"estimates — estimates,regularizedSEMMixedPenalty-method","text":"","code":"# S4 method for regularizedSEMMixedPenalty estimates(object, criterion = NULL, transformations = FALSE)"},{"path":"/reference/estimates-regularizedSEMMixedPenalty-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"estimates — estimates,regularizedSEMMixedPenalty-method","text":"object object class regularizedSEMMixedPenalty criterion fit index (e.g., AIC) used select parameters transformations boolean: transformations returned?","code":""},{"path":"/reference/estimates-regularizedSEMMixedPenalty-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"estimates — estimates,regularizedSEMMixedPenalty-method","text":"returns matrix estimates","code":""},{"path":"/reference/estimates.html","id":null,"dir":"Reference","previous_headings":"","what":"S4 method to exract the estimates of an object — estimates","title":"S4 method to exract the estimates of an object — estimates","text":"S4 method exract estimates object","code":""},{"path":"/reference/estimates.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"S4 method to exract the estimates of an object — estimates","text":"","code":"estimates(object, criterion = NULL, transformations = FALSE)"},{"path":"/reference/estimates.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"S4 method to exract the estimates of an object — estimates","text":"object model fitted lessSEM criterion fitIndice used select parameters transformations boolean: transformations returned?","code":""},{"path":"/reference/estimates.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"S4 method to exract the estimates of an object — estimates","text":"returns matrix estimates","code":""},{"path":"/reference/fit.html","id":null,"dir":"Reference","previous_headings":"","what":"fit — fit","title":"fit — fit","text":"Optimizes object mixed penalty. See ?mixedPenalty details.","code":""},{"path":"/reference/fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"fit — fit","text":"","code":"fit(mixedPenalty)"},{"path":"/reference/fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"fit — fit","text":"mixedPenalty object class mixedPenalty. object can created mixedPenalty function. Penalties can added addCappedL1, addElastiNet, addLasso, addLsp, addMcp, addScad functions.","code":""},{"path":"/reference/fit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"fit — fit","text":"throws error case undefined penalty combinations.","code":""},{"path":"/reference/fit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"fit — fit","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +       l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +       l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # We can add mixed penalties as follows:  regularized <- lavaanModel |>   # create template for regularized model with mixed penalty:   mixedPenalty() |>   # add penalty on loadings l6 - l10:   addElasticNet(regularized = paste0(\"l\", 11:15),            lambdas = seq(0,1,.1),           alphas = .4) |>   # fit the model:   fit()"},{"path":"/reference/fitIndices-cvRegularizedSEM-method.html","id":null,"dir":"Reference","previous_headings":"","what":"fitIndices — fitIndices,cvRegularizedSEM-method","title":"fitIndices — fitIndices,cvRegularizedSEM-method","text":"fitIndices","code":""},{"path":"/reference/fitIndices-cvRegularizedSEM-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"fitIndices — fitIndices,cvRegularizedSEM-method","text":"","code":"# S4 method for cvRegularizedSEM fitIndices(object)"},{"path":"/reference/fitIndices-cvRegularizedSEM-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"fitIndices — fitIndices,cvRegularizedSEM-method","text":"object object class cvRegularizedSEM","code":""},{"path":"/reference/fitIndices-cvRegularizedSEM-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"fitIndices — fitIndices,cvRegularizedSEM-method","text":"returns data.frame fit indices","code":""},{"path":"/reference/fitIndices-regularizedSEM-method.html","id":null,"dir":"Reference","previous_headings":"","what":"fitIndices — fitIndices,regularizedSEM-method","title":"fitIndices — fitIndices,regularizedSEM-method","text":"fitIndices","code":""},{"path":"/reference/fitIndices-regularizedSEM-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"fitIndices — fitIndices,regularizedSEM-method","text":"","code":"# S4 method for regularizedSEM fitIndices(object)"},{"path":"/reference/fitIndices-regularizedSEM-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"fitIndices — fitIndices,regularizedSEM-method","text":"object object class regularizedSEM","code":""},{"path":"/reference/fitIndices-regularizedSEM-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"fitIndices — fitIndices,regularizedSEM-method","text":"returns data.frame fit indices","code":""},{"path":"/reference/fitIndices-regularizedSEMMixedPenalty-method.html","id":null,"dir":"Reference","previous_headings":"","what":"fitIndices — fitIndices,regularizedSEMMixedPenalty-method","title":"fitIndices — fitIndices,regularizedSEMMixedPenalty-method","text":"fitIndices","code":""},{"path":"/reference/fitIndices-regularizedSEMMixedPenalty-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"fitIndices — fitIndices,regularizedSEMMixedPenalty-method","text":"","code":"# S4 method for regularizedSEMMixedPenalty fitIndices(object)"},{"path":"/reference/fitIndices-regularizedSEMMixedPenalty-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"fitIndices — fitIndices,regularizedSEMMixedPenalty-method","text":"object object class regularizedSEMMixedPenalty","code":""},{"path":"/reference/fitIndices-regularizedSEMMixedPenalty-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"fitIndices — fitIndices,regularizedSEMMixedPenalty-method","text":"returns data.frame fit indices","code":""},{"path":"/reference/fitIndices.html","id":null,"dir":"Reference","previous_headings":"","what":"S4 method to compute fit indices (e.g., AIC, BIC, ...) — fitIndices","title":"S4 method to compute fit indices (e.g., AIC, BIC, ...) — fitIndices","text":"S4 method compute fit indices (e.g., AIC, BIC, ...)","code":""},{"path":"/reference/fitIndices.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"S4 method to compute fit indices (e.g., AIC, BIC, ...) — fitIndices","text":"","code":"fitIndices(object)"},{"path":"/reference/fitIndices.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"S4 method to compute fit indices (e.g., AIC, BIC, ...) — fitIndices","text":"object model fitted lessSEM","code":""},{"path":"/reference/fitIndices.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"S4 method to compute fit indices (e.g., AIC, BIC, ...) — fitIndices","text":"returns data.frame fit indices","code":""},{"path":"/reference/getLavaanParameters.html","id":null,"dir":"Reference","previous_headings":"","what":"getLavaanParameters — getLavaanParameters","title":"getLavaanParameters — getLavaanParameters","text":"helper function: returns labeled vector parameters lavaan","code":""},{"path":"/reference/getLavaanParameters.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"getLavaanParameters — getLavaanParameters","text":"","code":"getLavaanParameters(lavaanModel, removeDuplicates = TRUE)"},{"path":"/reference/getLavaanParameters.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"getLavaanParameters — getLavaanParameters","text":"lavaanModel model class lavaan removeDuplicates duplicated parameters removed?","code":""},{"path":"/reference/getLavaanParameters.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"getLavaanParameters — getLavaanParameters","text":"returns labeled vector parameters lavaan","code":""},{"path":"/reference/getLavaanParameters.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"getLavaanParameters — getLavaanParameters","text":"","code":"library(lessSEM)  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +      l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +      l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE) getLavaanParameters(lavaanModel)"},{"path":"/reference/getTuningParameterConfiguration.html","id":null,"dir":"Reference","previous_headings":"","what":"getTuningParameterConfiguration — getTuningParameterConfiguration","title":"getTuningParameterConfiguration — getTuningParameterConfiguration","text":"Returns lambda, theta, alpha values tuning parameters regularized SEM mixed penalty.","code":""},{"path":"/reference/getTuningParameterConfiguration.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"getTuningParameterConfiguration — getTuningParameterConfiguration","text":"","code":"getTuningParameterConfiguration(   regularizedSEMMixedPenalty,   tuningParameterConfiguration )"},{"path":"/reference/getTuningParameterConfiguration.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"getTuningParameterConfiguration — getTuningParameterConfiguration","text":"regularizedSEMMixedPenalty object type regularizedSEMMixedPenalty (see ?mixedPenalty) tuningParameterConfiguration integer indicating tuningParameterConfiguration extracted (e.g., 1). See entry row tuningParameterConfiguration regularizedSEMMixedPenalty@fits regularizedSEMMixedPenalty@parameters.","code":""},{"path":"/reference/getTuningParameterConfiguration.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"getTuningParameterConfiguration — getTuningParameterConfiguration","text":"data frame penalty tuning parameter settings","code":""},{"path":"/reference/getTuningParameterConfiguration.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"getTuningParameterConfiguration — getTuningParameterConfiguration","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +       l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +       l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # We can add mixed penalties as follows:  regularized <- lavaanModel |>   # create template for regularized model with mixed penalty:   mixedPenalty() |>   # add penalty on loadings l6 - l10:   addLsp(regularized = paste0(\"l\", 11:15),           lambdas = seq(0,1,.1),          thetas = 2.3) |>   # fit the model:   fit()  getTuningParameterConfiguration(regularizedSEMMixedPenalty = regularized,                                  tuningParameterConfiguration = 2)"},{"path":"/reference/glmnetCappedL1MgSEM.html","id":null,"dir":"Reference","previous_headings":"","what":"CappedL1 optimization with glmnet optimizer — glmnetCappedL1MgSEM","title":"CappedL1 optimization with glmnet optimizer — glmnetCappedL1MgSEM","text":"Object cappedL1 optimization glmnet optimizer","code":""},{"path":"/reference/glmnetCappedL1MgSEM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"CappedL1 optimization with glmnet optimizer — glmnetCappedL1MgSEM","text":"list fit results","code":""},{"path":"/reference/glmnetCappedL1MgSEM.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"CappedL1 optimization with glmnet optimizer — glmnetCappedL1MgSEM","text":"new creates new object. Requires (2) list control elements setHessian changes Hessian model. Expects matrix optimize optimize model. Expects vector starting values, SEM type SEM_Cpp, theta lambda value.","code":""},{"path":"/reference/glmnetCappedL1SEM.html","id":null,"dir":"Reference","previous_headings":"","what":"CappedL1 optimization with glmnet optimizer — glmnetCappedL1SEM","title":"CappedL1 optimization with glmnet optimizer — glmnetCappedL1SEM","text":"Object cappedL1 optimization glmnet optimizer","code":""},{"path":"/reference/glmnetCappedL1SEM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"CappedL1 optimization with glmnet optimizer — glmnetCappedL1SEM","text":"list fit results","code":""},{"path":"/reference/glmnetCappedL1SEM.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"CappedL1 optimization with glmnet optimizer — glmnetCappedL1SEM","text":"new creates new object. Requires list control elements setHessian changes Hessian model. Expects matrix optimize optimize model. Expects vector starting values, SEM type SEM_Cpp, theta lambda value.","code":""},{"path":"/reference/glmnetEnetGeneralPurpose.html","id":null,"dir":"Reference","previous_headings":"","what":"elastic net optimization with glmnet optimizer — glmnetEnetGeneralPurpose","title":"elastic net optimization with glmnet optimizer — glmnetEnetGeneralPurpose","text":"Object elastic net optimization glmnet optimizer","code":""},{"path":"/reference/glmnetEnetGeneralPurpose.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"elastic net optimization with glmnet optimizer — glmnetEnetGeneralPurpose","text":"list fit results","code":""},{"path":"/reference/glmnetEnetGeneralPurpose.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"elastic net optimization with glmnet optimizer — glmnetEnetGeneralPurpose","text":"new creates new object. Requires (1) vector weights parameter (2) list control elements setHessian changes Hessian model. Expects matrix optimize optimize model. Expects vector starting values, R function compute fit, R function compute gradients, list elements fit gradient function require, lambda alpha value.","code":""},{"path":"/reference/glmnetEnetGeneralPurposeCpp.html","id":null,"dir":"Reference","previous_headings":"","what":"elastic net optimization with glmnet optimizer — glmnetEnetGeneralPurposeCpp","title":"elastic net optimization with glmnet optimizer — glmnetEnetGeneralPurposeCpp","text":"Object elastic net optimization glmnet optimizer","code":""},{"path":"/reference/glmnetEnetGeneralPurposeCpp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"elastic net optimization with glmnet optimizer — glmnetEnetGeneralPurposeCpp","text":"list fit results","code":""},{"path":"/reference/glmnetEnetGeneralPurposeCpp.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"elastic net optimization with glmnet optimizer — glmnetEnetGeneralPurposeCpp","text":"new creates new object. Requires (1) vector weights parameter (2) list control elements setHessian changes Hessian model. Expects matrix optimize optimize model. Expects vector starting values, SEXP function pointer compute fit, SEXP function pointer compute gradients, list elements fit gradient function require, lambda alpha value.","code":""},{"path":"/reference/glmnetEnetMgSEM.html","id":null,"dir":"Reference","previous_headings":"","what":"elastic net optimization with glmnet optimizer — glmnetEnetMgSEM","title":"elastic net optimization with glmnet optimizer — glmnetEnetMgSEM","text":"Object elastic net optimization glmnet optimizer","code":""},{"path":"/reference/glmnetEnetMgSEM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"elastic net optimization with glmnet optimizer — glmnetEnetMgSEM","text":"list fit results","code":""},{"path":"/reference/glmnetEnetMgSEM.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"elastic net optimization with glmnet optimizer — glmnetEnetMgSEM","text":"new creates new object. Requires (1) vector weights parameter (2) list control elements setHessian changes Hessian model. Expects matrix optimize optimize model. Expects vector starting values, SEM type SEM_Cpp, lambda alpha value.","code":""},{"path":"/reference/glmnetEnetSEM.html","id":null,"dir":"Reference","previous_headings":"","what":"elastic net optimization with glmnet optimizer — glmnetEnetSEM","title":"elastic net optimization with glmnet optimizer — glmnetEnetSEM","text":"Object elastic net optimization glmnet optimizer","code":""},{"path":"/reference/glmnetEnetSEM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"elastic net optimization with glmnet optimizer — glmnetEnetSEM","text":"list fit results","code":""},{"path":"/reference/glmnetEnetSEM.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"elastic net optimization with glmnet optimizer — glmnetEnetSEM","text":"new creates new object. Requires (1) vector weights parameter (2) list control elements setHessian changes Hessian model. Expects matrix optimize optimize model. Expects vector starting values, SEM type SEM_Cpp, lambda alpha value.","code":""},{"path":"/reference/glmnetLspMgSEM.html","id":null,"dir":"Reference","previous_headings":"","what":"lsp optimization with glmnet optimizer — glmnetLspMgSEM","title":"lsp optimization with glmnet optimizer — glmnetLspMgSEM","text":"Object lsp optimization glmnet optimizer","code":""},{"path":"/reference/glmnetLspMgSEM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"lsp optimization with glmnet optimizer — glmnetLspMgSEM","text":"list fit results","code":""},{"path":"/reference/glmnetLspMgSEM.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"lsp optimization with glmnet optimizer — glmnetLspMgSEM","text":"new creates new object. Requires (2) list control elements setHessian changes Hessian model. Expects matrix optimize optimize model. Expects vector starting values, SEM type SEM_Cpp, theta lambda value.","code":""},{"path":"/reference/glmnetLspSEM.html","id":null,"dir":"Reference","previous_headings":"","what":"lsp optimization with glmnet optimizer — glmnetLspSEM","title":"lsp optimization with glmnet optimizer — glmnetLspSEM","text":"Object lsp optimization glmnet optimizer","code":""},{"path":"/reference/glmnetLspSEM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"lsp optimization with glmnet optimizer — glmnetLspSEM","text":"list fit results","code":""},{"path":"/reference/glmnetLspSEM.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"lsp optimization with glmnet optimizer — glmnetLspSEM","text":"new creates new object. Requires list control elements setHessian changes Hessian model. Expects matrix optimize optimize model. Expects vector starting values, SEM type SEM_Cpp, theta lambda value.","code":""},{"path":"/reference/glmnetMcpMgSEM.html","id":null,"dir":"Reference","previous_headings":"","what":"mcp optimization with glmnet optimizer — glmnetMcpMgSEM","title":"mcp optimization with glmnet optimizer — glmnetMcpMgSEM","text":"Object mcp optimization glmnet optimizer","code":""},{"path":"/reference/glmnetMcpMgSEM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"mcp optimization with glmnet optimizer — glmnetMcpMgSEM","text":"list fit results","code":""},{"path":"/reference/glmnetMcpMgSEM.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"mcp optimization with glmnet optimizer — glmnetMcpMgSEM","text":"new creates new object. Requires (2) list control elements setHessian changes Hessian model. Expects matrix optimize optimize model. Expects vector starting values, SEM type SEM_Cpp, theta lambda value.","code":""},{"path":"/reference/glmnetMcpSEM.html","id":null,"dir":"Reference","previous_headings":"","what":"mcp optimization with glmnet optimizer — glmnetMcpSEM","title":"mcp optimization with glmnet optimizer — glmnetMcpSEM","text":"Object mcp optimization glmnet optimizer","code":""},{"path":"/reference/glmnetMcpSEM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"mcp optimization with glmnet optimizer — glmnetMcpSEM","text":"list fit results","code":""},{"path":"/reference/glmnetMcpSEM.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"mcp optimization with glmnet optimizer — glmnetMcpSEM","text":"new creates new object. Requires list control elements setHessian changes Hessian model. Expects matrix optimize optimize model. Expects vector starting values, SEM type SEM_Cpp, theta lambda value.","code":""},{"path":"/reference/glmnetMixedMgSEM.html","id":null,"dir":"Reference","previous_headings":"","what":"mixed optimization with glmnet optimizer — glmnetMixedMgSEM","title":"mixed optimization with glmnet optimizer — glmnetMixedMgSEM","text":"Object mixed optimization glmnet optimizer","code":""},{"path":"/reference/glmnetMixedMgSEM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"mixed optimization with glmnet optimizer — glmnetMixedMgSEM","text":"list fit results","code":""},{"path":"/reference/glmnetMixedMgSEM.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"mixed optimization with glmnet optimizer — glmnetMixedMgSEM","text":"new creates new object. Requires (2) list control elements setHessian changes Hessian model. Expects matrix optimize optimize model. Expects vector starting values, SEM type SEM_Cpp, theta lambda value.","code":""},{"path":"/reference/glmnetMixedPenaltyGeneralPurpose.html","id":null,"dir":"Reference","previous_headings":"","what":"mixed optimization with glmnet optimizer — glmnetMixedPenaltyGeneralPurpose","title":"mixed optimization with glmnet optimizer — glmnetMixedPenaltyGeneralPurpose","text":"Object mixed optimization glmnet optimizer","code":""},{"path":"/reference/glmnetMixedPenaltyGeneralPurpose.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"mixed optimization with glmnet optimizer — glmnetMixedPenaltyGeneralPurpose","text":"list fit results","code":""},{"path":"/reference/glmnetMixedPenaltyGeneralPurpose.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"mixed optimization with glmnet optimizer — glmnetMixedPenaltyGeneralPurpose","text":"new creates new object. Requires list control elements setHessian changes Hessian model. Expects matrix optimize optimize model. Expects vector starting values, SEM type SEM_Cpp, theta lambda value.","code":""},{"path":"/reference/glmnetMixedPenaltyGeneralPurposeCpp.html","id":null,"dir":"Reference","previous_headings":"","what":"mixed optimization with glmnet optimizer — glmnetMixedPenaltyGeneralPurposeCpp","title":"mixed optimization with glmnet optimizer — glmnetMixedPenaltyGeneralPurposeCpp","text":"Object mixed optimization glmnet optimizer","code":""},{"path":"/reference/glmnetMixedPenaltyGeneralPurposeCpp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"mixed optimization with glmnet optimizer — glmnetMixedPenaltyGeneralPurposeCpp","text":"list fit results","code":""},{"path":"/reference/glmnetMixedPenaltyGeneralPurposeCpp.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"mixed optimization with glmnet optimizer — glmnetMixedPenaltyGeneralPurposeCpp","text":"new creates new object. Requires list control elements setHessian changes Hessian model. Expects matrix optimize optimize model. Expects vector starting values, SEM type SEM_Cpp, theta lambda value.","code":""},{"path":"/reference/glmnetMixedSEM.html","id":null,"dir":"Reference","previous_headings":"","what":"mixed optimization with glmnet optimizer — glmnetMixedSEM","title":"mixed optimization with glmnet optimizer — glmnetMixedSEM","text":"Object mixed optimization glmnet optimizer","code":""},{"path":"/reference/glmnetMixedSEM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"mixed optimization with glmnet optimizer — glmnetMixedSEM","text":"list fit results","code":""},{"path":"/reference/glmnetMixedSEM.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"mixed optimization with glmnet optimizer — glmnetMixedSEM","text":"new creates new object. Requires list control elements setHessian changes Hessian model. Expects matrix optimize optimize model. Expects vector starting values, SEM type SEM_Cpp, theta lambda value.","code":""},{"path":"/reference/glmnetScadMgSEM.html","id":null,"dir":"Reference","previous_headings":"","what":"scad optimization with glmnet optimizer — glmnetScadMgSEM","title":"scad optimization with glmnet optimizer — glmnetScadMgSEM","text":"Object scad optimization glmnet optimizer","code":""},{"path":"/reference/glmnetScadMgSEM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"scad optimization with glmnet optimizer — glmnetScadMgSEM","text":"list fit results","code":""},{"path":"/reference/glmnetScadMgSEM.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"scad optimization with glmnet optimizer — glmnetScadMgSEM","text":"new creates new object. Requires (2) list control elements setHessian changes Hessian model. Expects matrix optimize optimize model. Expects vector starting values, SEM type SEM_Cpp, theta lambda value.","code":""},{"path":"/reference/glmnetScadSEM.html","id":null,"dir":"Reference","previous_headings":"","what":"scad optimization with glmnet optimizer — glmnetScadSEM","title":"scad optimization with glmnet optimizer — glmnetScadSEM","text":"Object scad optimization glmnet optimizer","code":""},{"path":"/reference/glmnetScadSEM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"scad optimization with glmnet optimizer — glmnetScadSEM","text":"list fit results","code":""},{"path":"/reference/glmnetScadSEM.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"scad optimization with glmnet optimizer — glmnetScadSEM","text":"new creates new object. Requires list control elements setHessian changes Hessian model. Expects matrix optimize optimize model. Expects vector starting values, SEM type SEM_Cpp, theta lambda value.","code":""},{"path":"/reference/gpAdaptiveLasso.html","id":null,"dir":"Reference","previous_headings":"","what":"gpAdaptiveLasso — gpAdaptiveLasso","title":"gpAdaptiveLasso — gpAdaptiveLasso","text":"Implements adaptive lasso regularization general purpose optimization problems. penalty function given : $$p( x_j) = p( x_j) = \\frac{1}{w_j}\\lambda| x_j|$$ Adaptive lasso regularization set parameters zero \\(\\lambda\\) large enough.","code":""},{"path":"/reference/gpAdaptiveLasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"gpAdaptiveLasso — gpAdaptiveLasso","text":"","code":"gpAdaptiveLasso(   par,   regularized,   weights = NULL,   fn,   gr = NULL,   lambdas = NULL,   nLambdas = NULL,   reverse = TRUE,   curve = 1,   ...,   method = \"glmnet\",   control = lessSEM::controlGlmnet() )"},{"path":"/reference/gpAdaptiveLasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"gpAdaptiveLasso — gpAdaptiveLasso","text":"par labeled vector starting values regularized vector names parameters regularized. weights labeled vector adaptive lasso weights. NULL use 1/abs(par) fn R function takes parameters input returns fit value (single value) gr R function takes parameters input returns gradients objective function. set NULL, numDeriv used approximate gradients lambdas numeric vector: values tuning parameter lambda nLambdas alternative lambda: alpha = 1, lessSEM can automatically compute first lambda value sets regularized parameters zero. generate nLambda values 0 computed lambda. reverse set TRUE nLambdas used, lessSEM start largest lambda gradually decrease lambda. Otherwise, lessSEM start smallest lambda gradually increase . curve Allows unequally spaced lambda steps (e.g., .01,.02,.05,1,5,20). curve close 1 lambda values equally spaced, curve large lambda values concentrated close 0. See ?lessSEM::curveLambda information. ... additional arguments passed fn gr method optimizer used? Currently implemented ista glmnet. control used control optimizer. element generated controlIsta controlGlmnet functions. See ?controlIsta ?controlGlmnet details.","code":""},{"path":"/reference/gpAdaptiveLasso.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"gpAdaptiveLasso — gpAdaptiveLasso","text":"Object class gpRegularized","code":""},{"path":"/reference/gpAdaptiveLasso.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"gpAdaptiveLasso — gpAdaptiveLasso","text":"interface similar optim. Users supply vector starting values (important: vector must labels) fitting function. fitting functions must take labeled vector parameter values first argument. remaining arguments passed ... argument. similar optim. gradient function gr optional. set NULL, numDeriv package used approximate gradients. Supplying gradient function can result considerable speed improvements. Adaptive lasso regularization: Zou, H. (2006). adaptive lasso oracle properties. Journal American Statistical Association, 101(476), 1418–1429. https://doi.org/10.1198/016214506000000735 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classification. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/gpAdaptiveLasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"gpAdaptiveLasso — gpAdaptiveLasso","text":"","code":"# This example shows how to use the optimizers # for other objective functions. We will use # a linear regression as an example. Note that # this is not a useful application of the optimizers # as there are specialized packages for linear regression # (e.g., glmnet)  library(lessSEM) set.seed(123)  # first, we simulate data for our # linear regression. N <- 100 # number of persons p <- 10 # number of predictors X <- matrix(rnorm(N*p),  nrow = N, ncol = p) # design matrix b <- c(rep(1,4),         rep(0,6)) # true regression weights y <- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)  # First, we must construct a fiting function # which returns a single value. We will use # the residual sum squared as fitting function.  # Let's start setting up the fitting function: fittingFunction <- function(par, y, X, N){   # par is the parameter vector   # y is the observed dependent variable   # X is the design matrix   # N is the sample size   pred <- X %*% matrix(par, ncol = 1) #be explicit here:    # we need par to be a column vector   sse <- sum((y - pred)^2)   # we scale with .5/N to get the same results as glmnet   return((.5/N)*sse) }  # let's define the starting values: b <- c(solve(t(X)%*%X)%*%t(X)%*%y) # we will use the lm estimates names(b) <- paste0(\"b\", 1:length(b)) # names of regularized parameters regularized <- paste0(\"b\",1:p)  # define the weight for each of the parameters weights <- 1/abs(b) # we will re-scale the weights for equivalence to glmnet. # see ?glmnet for more details weights <- length(b)*weights/sum(weights)  # optimize adaptiveLassoPen <- gpAdaptiveLasso(   par = b,    regularized = regularized,    weights = weights,   fn = fittingFunction,    lambdas = seq(0,1,.01),    X = X,   y = y,   N = N ) plot(adaptiveLassoPen) # You can access the fit results as follows: adaptiveLassoPen@fits # Note that we won't compute any fit measures automatically, as # we cannot be sure how the AIC, BIC, etc are defined for your objective function   # for comparison: # library(glmnet) # coef(glmnet(x = X, #            y = y, #            penalty.factor = weights, #            lambda = adaptiveLassoPen@fits$lambda[20], #            intercept = FALSE, #            standardize = FALSE))[,1] # adaptiveLassoPen@parameters[20,]"},{"path":"/reference/gpAdaptiveLassoCpp.html","id":null,"dir":"Reference","previous_headings":"","what":"gpAdaptiveLassoCpp — gpAdaptiveLassoCpp","title":"gpAdaptiveLassoCpp — gpAdaptiveLassoCpp","text":"Implements adaptive lasso regularization general purpose optimization problems C++ functions. penalty function given : $$p( x_j) = p( x_j) = \\frac{1}{w_j}\\lambda| x_j|$$ Adaptive lasso regularization set parameters zero \\(\\lambda\\) large enough.","code":""},{"path":"/reference/gpAdaptiveLassoCpp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"gpAdaptiveLassoCpp — gpAdaptiveLassoCpp","text":"","code":"gpAdaptiveLassoCpp(   par,   regularized,   weights = NULL,   fn,   gr,   lambdas = NULL,   nLambdas = NULL,   curve = 1,   additionalArguments,   method = \"glmnet\",   control = lessSEM::controlGlmnet() )"},{"path":"/reference/gpAdaptiveLassoCpp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"gpAdaptiveLassoCpp — gpAdaptiveLassoCpp","text":"par labeled vector starting values regularized vector names parameters regularized. weights labeled vector adaptive lasso weights. NULL use 1/abs(par) fn R function takes parameters input returns fit value (single value) gr R function takes parameters input returns gradients objective function. set NULL, numDeriv used approximate gradients lambdas numeric vector: values tuning parameter lambda nLambdas alternative lambda: alpha = 1, lessSEM can automatically compute first lambda value sets regularized parameters zero. generate nLambda values 0 computed lambda. curve Allows unequally spaced lambda steps (e.g., .01,.02,.05,1,5,20). curve close 1 lambda values equally spaced, curve large lambda values concentrated close 0. See ?lessSEM::curveLambda information. additionalArguments list additional arguments passed fn gr method optimizer used? Currently implemented ista glmnet. control used control optimizer. element generated controlIsta controlGlmnet functions. See ?controlIsta ?controlGlmnet details.","code":""},{"path":"/reference/gpAdaptiveLassoCpp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"gpAdaptiveLassoCpp — gpAdaptiveLassoCpp","text":"Object class gpRegularized","code":""},{"path":"/reference/gpAdaptiveLassoCpp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"gpAdaptiveLassoCpp — gpAdaptiveLassoCpp","text":"interface inspired optim, bit restrictive. Users supply vector starting values (important: vector must labels), fitting function, gradient function. fitting functions must take const Rcpp::NumericVector& parameter values first argument Rcpp::List& second argument Adaptive lasso regularization: Zou, H. (2006). adaptive lasso oracle properties. Journal American Statistical Association, 101(476), 1418–1429. https://doi.org/10.1198/016214506000000735 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classification. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/gpAdaptiveLassoCpp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"gpAdaptiveLassoCpp — gpAdaptiveLassoCpp","text":"","code":"# \\donttest{ # This example shows how to use the optimizers # for C++ objective functions. We will use # a linear regression as an example. Note that # this is not a useful application of the optimizers # as there are specialized packages for linear regression # (e.g., glmnet)  library(Rcpp) library(lessSEM)  linreg <- ' // [[Rcpp::depends(RcppArmadillo)]] #include <RcppArmadillo.h>  // [[Rcpp::export]] double fitfunction(const Rcpp::NumericVector& parameters, Rcpp::List& data){   // extract all required elements:   arma::colvec b = Rcpp::as<arma::colvec>(parameters);   arma::colvec y = Rcpp::as<arma::colvec>(data[\"y\"]); // the dependent variable   arma::mat X = Rcpp::as<arma::mat>(data[\"X\"]); // the design matrix      // compute the sum of squared errors:     arma::mat sse = arma::trans(y-X*b)*(y-X*b);          // other packages, such as glmnet, scale the sse with      // 1/(2*N), where N is the sample size. We will do that here as well          sse *= 1.0/(2.0 * y.n_elem);          // note: We must return a double, but the sse is a matrix     // To get a double, just return the single value that is in      // this matrix:       return(sse(0,0)); }  // [[Rcpp::export]] arma::rowvec gradientfunction(const Rcpp::NumericVector& parameters, Rcpp::List& data){   // extract all required elements:   arma::colvec b = Rcpp::as<arma::colvec>(parameters);   arma::colvec y = Rcpp::as<arma::colvec>(data[\"y\"]); // the dependent variable   arma::mat X = Rcpp::as<arma::mat>(data[\"X\"]); // the design matrix      // note: we want to return our gradients as row-vector; therefore,   // we have to transpose the resulting column-vector:     arma::rowvec gradients = arma::trans(-2.0*X.t() * y + 2.0*X.t()*X*b);          // other packages, such as glmnet, scale the sse with      // 1/(2*N), where N is the sample size. We will do that here as well          gradients *= (.5/y.n_rows);          return(gradients); }  // Dirk Eddelbuettel at // https://gallery.rcpp.org/articles/passing-cpp-function-pointers/ typedef double (*fitFunPtr)(const Rcpp::NumericVector&, //parameters                 Rcpp::List& //additional elements ); typedef Rcpp::XPtr<fitFunPtr> fitFunPtr_t;  typedef arma::rowvec (*gradientFunPtr)(const Rcpp::NumericVector&, //parameters                       Rcpp::List& //additional elements ); typedef Rcpp::XPtr<gradientFunPtr> gradientFunPtr_t;  // [[Rcpp::export]] fitFunPtr_t fitfunPtr() {         return(fitFunPtr_t(new fitFunPtr(&fitfunction))); }  // [[Rcpp::export]] gradientFunPtr_t gradfunPtr() {         return(gradientFunPtr_t(new gradientFunPtr(&gradientfunction))); } '  Rcpp::sourceCpp(code = linreg)  ffp <- fitfunPtr() gfp <- gradfunPtr()  N <- 100 # number of persons p <- 10 # number of predictors X <- matrix(rnorm(N*p),  nrow = N, ncol = p) # design matrix b <- c(rep(1,4),         rep(0,6)) # true regression weights y <- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)  data <- list(\"y\" = y,              \"X\" = cbind(1,X)) parameters <- rep(0, ncol(data$X)) names(parameters) <- paste0(\"b\", 0:(length(parameters)-1))  al1 <- gpAdaptiveLassoCpp(par = parameters,                   regularized = paste0(\"b\", 1:(length(b)-1)),                  fn = ffp,                   gr = gfp,                   lambdas = seq(0,1,.1),                   additionalArguments = data)  al1@parameters # }"},{"path":"/reference/gpCappedL1.html","id":null,"dir":"Reference","previous_headings":"","what":"gpCappedL1 — gpCappedL1","title":"gpCappedL1 — gpCappedL1","text":"Implements cappedL1 regularization general purpose optimization problems. penalty function given : $$p( x_j) = \\lambda \\min(| x_j|, \\theta)$$ \\(\\theta > 0\\). cappedL1 penalty identical lasso parameters \\(\\theta\\) identical constant parameters \\(\\theta\\). adding constant fitting function change minimum, larger parameters can stay unregularized smaller ones set zero.","code":""},{"path":"/reference/gpCappedL1.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"gpCappedL1 — gpCappedL1","text":"","code":"gpCappedL1(   par,   fn,   gr = NULL,   ...,   regularized,   lambdas,   thetas,   method = \"glmnet\",   control = lessSEM::controlGlmnet() )"},{"path":"/reference/gpCappedL1.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"gpCappedL1 — gpCappedL1","text":"par labeled vector starting values fn R function takes parameters labels input returns fit value (single value) gr R function takes parameters labels input returns gradients objective function. set NULL, numDeriv used approximate gradients ... additional arguments passed fn gr regularized vector names parameters regularized. lambdas numeric vector: values tuning parameter lambda thetas parameters whose absolute value threshold penalized constant (theta) method optimizer used? Currently implemented ista glmnet. control used control optimizer. element generated controlIsta controlGlmnet functions. See ?controlIsta ?controlGlmnet details.","code":""},{"path":"/reference/gpCappedL1.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"gpCappedL1 — gpCappedL1","text":"Object class gpRegularized","code":""},{"path":"/reference/gpCappedL1.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"gpCappedL1 — gpCappedL1","text":"interface similar optim. Users supply vector starting values (important: vector must labels) fitting function. fitting functions must take labeled vector parameter values first argument. remaining arguments passed ... argument. similar optim. gradient function gr optional. set NULL, numDeriv package used approximate gradients. Supplying gradient function can result considerable speed improvements. CappedL1 regularization: Zhang, T. (2010). Analysis Multi-stage Convex Relaxation Sparse Regularization. Journal Machine Learning Research, 11, 1081–1107. details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classification. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/gpCappedL1.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"gpCappedL1 — gpCappedL1","text":"","code":"# This example shows how to use the optimizers # for other objective functions. We will use # a linear regression as an example. Note that # this is not a useful application of the optimizers # as there are specialized packages for linear regression # (e.g., glmnet)  # This example shows how to use the optimizers # for other objective functions. We will use # a linear regression as an example. Note that # this is not a useful application of the optimizers # as there are specialized packages for linear regression # (e.g., glmnet)  library(lessSEM) set.seed(123)  # first, we simulate data for our # linear regression. N <- 100 # number of persons p <- 10 # number of predictors X <- matrix(rnorm(N*p),  nrow = N, ncol = p) # design matrix b <- c(rep(1,4),        rep(0,6)) # true regression weights y <- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)  # First, we must construct a fiting function # which returns a single value. We will use # the residual sum squared as fitting function.  # Let's start setting up the fitting function: fittingFunction <- function(par, y, X, N){   # par is the parameter vector   # y is the observed dependent variable   # X is the design matrix   # N is the sample size   pred <- X %*% matrix(par, ncol = 1) #be explicit here:   # we need par to be a column vector   sse <- sum((y - pred)^2)   # we scale with .5/N to get the same results as glmnet   return((.5/N)*sse) }  # let's define the starting values: b <- c(solve(t(X)%*%X)%*%t(X)%*%y) # we will use the lm estimates names(b) <- paste0(\"b\", 1:length(b)) # names of regularized parameters regularized <- paste0(\"b\",1:p)  # optimize cL1 <- gpCappedL1(   par = b,   regularized = regularized,   fn = fittingFunction,   lambdas = seq(0,1,.1),   thetas = c(0.001, .5, 1),   X = X,   y = y,   N = N )  # optional: plot requires plotly package # plot(cL1)  # for comparison  fittingFunction <- function(par, y, X, N, lambda, theta){   pred <- X %*% matrix(par, ncol = 1)   sse <- sum((y - pred)^2)   smoothAbs <- sqrt(par^2 + 1e-8)   pen <- lambda * ifelse(smoothAbs < theta, smoothAbs, theta)   return((.5/N)*sse + sum(pen)) }  round(   optim(par = b,       fn = fittingFunction,       y = y,       X = X,       N = N,       lambda =  cL1@fits$lambda[15],       theta =  cL1@fits$theta[15],       method = \"BFGS\")$par,   4) cL1@parameters[15,]"},{"path":"/reference/gpCappedL1Cpp.html","id":null,"dir":"Reference","previous_headings":"","what":"gpCappedL1Cpp — gpCappedL1Cpp","title":"gpCappedL1Cpp — gpCappedL1Cpp","text":"Implements cappedL1 regularization general purpose optimization problems C++ functions. penalty function given : $$p( x_j) = \\lambda \\min(| x_j|, \\theta)$$ \\(\\theta > 0\\). cappedL1 penalty identical lasso parameters \\(\\theta\\) identical constant parameters \\(\\theta\\). adding constant fitting function change minimum, larger parameters can stay unregularized smaller ones set zero.","code":""},{"path":"/reference/gpCappedL1Cpp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"gpCappedL1Cpp — gpCappedL1Cpp","text":"","code":"gpCappedL1Cpp(   par,   fn,   gr,   additionalArguments,   regularized,   lambdas,   thetas,   method = \"glmnet\",   control = lessSEM::controlGlmnet() )"},{"path":"/reference/gpCappedL1Cpp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"gpCappedL1Cpp — gpCappedL1Cpp","text":"par labeled vector starting values fn R function takes parameters labels input returns fit value (single value) gr R function takes parameters labels input returns gradients objective function. set NULL, numDeriv used approximate gradients additionalArguments list additional arguments passed fn gr regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda thetas parameters whose absolute value threshold penalized constant (theta) method optimizer used? Currently implemented ista glmnet. control used control optimizer. element generated controlIsta controlGlmnet functions. See ?controlIsta ?controlGlmnet details.","code":""},{"path":"/reference/gpCappedL1Cpp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"gpCappedL1Cpp — gpCappedL1Cpp","text":"Object class gpRegularized","code":""},{"path":"/reference/gpCappedL1Cpp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"gpCappedL1Cpp — gpCappedL1Cpp","text":"interface inspired optim, bit restrictive. Users supply vector starting values (important: vector must labels), fitting function, gradient function. fitting functions must take const Rcpp::NumericVector& parameter values first argument Rcpp::List& second argument CappedL1 regularization: Zhang, T. (2010). Analysis Multi-stage Convex Relaxation Sparse Regularization. Journal Machine Learning Research, 11, 1081–1107. details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classification. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/gpCappedL1Cpp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"gpCappedL1Cpp — gpCappedL1Cpp","text":"","code":"# \\donttest{ # This example shows how to use the optimizers # for C++ objective functions. We will use # a linear regression as an example. Note that # this is not a useful application of the optimizers # as there are specialized packages for linear regression # (e.g., glmnet)  library(Rcpp) library(lessSEM)  linreg <- ' // [[Rcpp::depends(RcppArmadillo)]] #include <RcppArmadillo.h>  // [[Rcpp::export]] double fitfunction(const Rcpp::NumericVector& parameters, Rcpp::List& data){   // extract all required elements:   arma::colvec b = Rcpp::as<arma::colvec>(parameters);   arma::colvec y = Rcpp::as<arma::colvec>(data[\"y\"]); // the dependent variable   arma::mat X = Rcpp::as<arma::mat>(data[\"X\"]); // the design matrix      // compute the sum of squared errors:     arma::mat sse = arma::trans(y-X*b)*(y-X*b);          // other packages, such as glmnet, scale the sse with      // 1/(2*N), where N is the sample size. We will do that here as well          sse *= 1.0/(2.0 * y.n_elem);          // note: We must return a double, but the sse is a matrix     // To get a double, just return the single value that is in      // this matrix:       return(sse(0,0)); }  // [[Rcpp::export]] arma::rowvec gradientfunction(const Rcpp::NumericVector& parameters, Rcpp::List& data){   // extract all required elements:   arma::colvec b = Rcpp::as<arma::colvec>(parameters);   arma::colvec y = Rcpp::as<arma::colvec>(data[\"y\"]); // the dependent variable   arma::mat X = Rcpp::as<arma::mat>(data[\"X\"]); // the design matrix      // note: we want to return our gradients as row-vector; therefore,   // we have to transpose the resulting column-vector:     arma::rowvec gradients = arma::trans(-2.0*X.t() * y + 2.0*X.t()*X*b);          // other packages, such as glmnet, scale the sse with      // 1/(2*N), where N is the sample size. We will do that here as well          gradients *= (.5/y.n_rows);          return(gradients); }  // https://gallery.rcpp.org/articles/passing-cpp-function-pointers/ typedef double (*fitFunPtr)(const Rcpp::NumericVector&, //parameters                 Rcpp::List& //additional elements ); typedef Rcpp::XPtr<fitFunPtr> fitFunPtr_t;  typedef arma::rowvec (*gradientFunPtr)(const Rcpp::NumericVector&, //parameters                       Rcpp::List& //additional elements ); typedef Rcpp::XPtr<gradientFunPtr> gradientFunPtr_t;  // [[Rcpp::export]] fitFunPtr_t fitfunPtr() {         return(fitFunPtr_t(new fitFunPtr(&fitfunction))); }  // [[Rcpp::export]] gradientFunPtr_t gradfunPtr() {         return(gradientFunPtr_t(new gradientFunPtr(&gradientfunction))); } '  Rcpp::sourceCpp(code = linreg)  ffp <- fitfunPtr() gfp <- gradfunPtr()  N <- 100 # number of persons p <- 10 # number of predictors X <- matrix(rnorm(N*p),  nrow = N, ncol = p) # design matrix b <- c(rep(1,4),         rep(0,6)) # true regression weights y <- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)  data <- list(\"y\" = y,              \"X\" = cbind(1,X)) parameters <- rep(0, ncol(data$X)) names(parameters) <- paste0(\"b\", 0:(length(parameters)-1))  cL1 <- gpCappedL1Cpp(par = parameters,                   regularized = paste0(\"b\", 1:(length(b)-1)),                  fn = ffp,                   gr = gfp,                   lambdas = seq(0,1,.1),                   thetas = seq(0.1,1,.1),                  additionalArguments = data)  cL1@parameters # }"},{"path":"/reference/gpElasticNet.html","id":null,"dir":"Reference","previous_headings":"","what":"gpElasticNet — gpElasticNet","title":"gpElasticNet — gpElasticNet","text":"Implements elastic net regularization general purpose optimization problems. penalty function given : $$p( x_j) = p( x_j) = \\frac{1}{w_j}\\lambda| x_j|$$ Note elastic net combines ridge lasso regularization. \\(\\alpha = 0\\), elastic net reduces ridge regularization. \\(\\alpha = 1\\) reduces lasso regularization. , elastic net compromise shrinkage lasso ridge penalty.","code":""},{"path":"/reference/gpElasticNet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"gpElasticNet — gpElasticNet","text":"","code":"gpElasticNet(   par,   regularized,   fn,   gr = NULL,   lambdas,   alphas,   ...,   method = \"glmnet\",   control = lessSEM::controlGlmnet() )"},{"path":"/reference/gpElasticNet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"gpElasticNet — gpElasticNet","text":"par labeled vector starting values regularized vector names parameters regularized. fn R function takes parameters labels input returns fit value (single value) gr R function takes parameters labels input returns gradients objective function. set NULL, numDeriv used approximate gradients lambdas numeric vector: values tuning parameter lambda alphas numeric vector values tuning parameter alpha. Must 0 1. 0 = ridge, 1 = lasso. ... additional arguments passed fn gr method optimizer used? Currently implemented ista glmnet. control used control optimizer. element generated controlIsta controlGlmnet functions. See ?controlIsta ?controlGlmnet details.","code":""},{"path":"/reference/gpElasticNet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"gpElasticNet — gpElasticNet","text":"Object class gpRegularized","code":""},{"path":"/reference/gpElasticNet.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"gpElasticNet — gpElasticNet","text":"interface similar optim. Users supply vector starting values (important: vector must labels) fitting function. fitting functions must take labeled vector parameter values first argument. remaining arguments passed ... argument. similar optim. gradient function gr optional. set NULL, numDeriv package used approximate gradients. Supplying gradient function can result considerable speed improvements. Elastic net regularization: Zou, H., & Hastie, T. (2005). Regularization variable selection via elastic net. Journal Royal Statistical Society: Series B, 67(2), 301–320. https://doi.org/10.1111/j.1467-9868.2005.00503.x details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classification. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/gpElasticNet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"gpElasticNet — gpElasticNet","text":"","code":"# This example shows how to use the optimizers # for other objective functions. We will use # a linear regression as an example. Note that # this is not a useful application of the optimizers # as there are specialized packages for linear regression # (e.g., glmnet)  library(lessSEM) set.seed(123)  # first, we simulate data for our # linear regression. N <- 100 # number of persons p <- 10 # number of predictors X <- matrix(rnorm(N*p),  nrow = N, ncol = p) # design matrix b <- c(rep(1,4),        rep(0,6)) # true regression weights y <- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)  # First, we must construct a fiting function # which returns a single value. We will use # the residual sum squared as fitting function.  # Let's start setting up the fitting function: fittingFunction <- function(par, y, X, N){   # par is the parameter vector   # y is the observed dependent variable   # X is the design matrix   # N is the sample size   pred <- X %*% matrix(par, ncol = 1) #be explicit here:   # we need par to be a column vector   sse <- sum((y - pred)^2)   # we scale with .5/N to get the same results as glmnet   return((.5/N)*sse) }  # let's define the starting values: b <- c(solve(t(X)%*%X)%*%t(X)%*%y) # we will use the lm estimates names(b) <- paste0(\"b\", 1:length(b)) # names of regularized parameters regularized <- paste0(\"b\",1:p)  # optimize elasticNetPen <- gpElasticNet(   par = b,   regularized = regularized,   fn = fittingFunction,   lambdas = seq(0,1,.1),   alphas = c(0, .5, 1),   X = X,   y = y,   N = N )  # optional: plot requires plotly package # plot(elasticNetPen)  # for comparison: fittingFunction <- function(par, y, X, N, lambda, alpha){   pred <- X %*% matrix(par, ncol = 1)   sse <- sum((y - pred)^2)   return((.5/N)*sse + (1-alpha)*lambda * sum(par^2) + alpha*lambda *sum(sqrt(par^2 + 1e-8))) }  round(   optim(par = b,       fn = fittingFunction,       y = y,       X = X,       N = N,       lambda =  elasticNetPen@fits$lambda[15],       alpha =  elasticNetPen@fits$alpha[15],       method = \"BFGS\")$par,   4) elasticNetPen@parameters[15,]"},{"path":"/reference/gpElasticNetCpp.html","id":null,"dir":"Reference","previous_headings":"","what":"gpElasticNetCpp — gpElasticNetCpp","title":"gpElasticNetCpp — gpElasticNetCpp","text":"Implements elastic net regularization general purpose optimization problems C++ functions. penalty function given : $$p( x_j) = p( x_j) = \\frac{1}{w_j}\\lambda| x_j|$$ Note elastic net combines ridge lasso regularization. \\(\\alpha = 0\\), elastic net reduces ridge regularization. \\(\\alpha = 1\\) reduces lasso regularization. , elastic net compromise shrinkage lasso ridge penalty.","code":""},{"path":"/reference/gpElasticNetCpp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"gpElasticNetCpp — gpElasticNetCpp","text":"","code":"gpElasticNetCpp(   par,   regularized,   fn,   gr,   lambdas,   alphas,   additionalArguments,   method = \"glmnet\",   control = lessSEM::controlGlmnet() )"},{"path":"/reference/gpElasticNetCpp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"gpElasticNetCpp — gpElasticNetCpp","text":"par labeled vector starting values regularized vector names parameters regularized. fn R function takes parameters labels input returns fit value (single value) gr R function takes parameters labels input returns gradients objective function. set NULL, numDeriv used approximate gradients lambdas numeric vector: values tuning parameter lambda alphas numeric vector values tuning parameter alpha. Must 0 1. 0 = ridge, 1 = lasso. additionalArguments list additional arguments passed fn gr method optimizer used? Currently implemented ista glmnet. control used control optimizer. element generated controlIsta controlGlmnet functions. See ?controlIsta ?controlGlmnet details.","code":""},{"path":"/reference/gpElasticNetCpp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"gpElasticNetCpp — gpElasticNetCpp","text":"Object class gpRegularized","code":""},{"path":"/reference/gpElasticNetCpp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"gpElasticNetCpp — gpElasticNetCpp","text":"interface inspired optim, bit restrictive. Users supply vector starting values (important: vector must labels), fitting function, gradient function. fitting functions must take const Rcpp::NumericVector& parameter values first argument Rcpp::List& second argument Elastic net regularization: Zou, H., & Hastie, T. (2005). Regularization variable selection via elastic net. Journal Royal Statistical Society: Series B, 67(2), 301–320. https://doi.org/10.1111/j.1467-9868.2005.00503.x details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classification. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/gpElasticNetCpp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"gpElasticNetCpp — gpElasticNetCpp","text":"","code":"# \\donttest{ # This example shows how to use the optimizers # for C++ objective functions. We will use # a linear regression as an example. Note that # this is not a useful application of the optimizers # as there are specialized packages for linear regression # (e.g., glmnet)  library(Rcpp) library(lessSEM)  linreg <- ' // [[Rcpp::depends(RcppArmadillo)]] #include <RcppArmadillo.h>  // [[Rcpp::export]] double fitfunction(const Rcpp::NumericVector& parameters, Rcpp::List& data){   // extract all required elements:   arma::colvec b = Rcpp::as<arma::colvec>(parameters);   arma::colvec y = Rcpp::as<arma::colvec>(data[\"y\"]); // the dependent variable   arma::mat X = Rcpp::as<arma::mat>(data[\"X\"]); // the design matrix      // compute the sum of squared errors:     arma::mat sse = arma::trans(y-X*b)*(y-X*b);          // other packages, such as glmnet, scale the sse with      // 1/(2*N), where N is the sample size. We will do that here as well          sse *= 1.0/(2.0 * y.n_elem);          // note: We must return a double, but the sse is a matrix     // To get a double, just return the single value that is in      // this matrix:       return(sse(0,0)); }  // [[Rcpp::export]] arma::rowvec gradientfunction(const Rcpp::NumericVector& parameters, Rcpp::List& data){   // extract all required elements:   arma::colvec b = Rcpp::as<arma::colvec>(parameters);   arma::colvec y = Rcpp::as<arma::colvec>(data[\"y\"]); // the dependent variable   arma::mat X = Rcpp::as<arma::mat>(data[\"X\"]); // the design matrix      // note: we want to return our gradients as row-vector; therefore,   // we have to transpose the resulting column-vector:     arma::rowvec gradients = arma::trans(-2.0*X.t() * y + 2.0*X.t()*X*b);          // other packages, such as glmnet, scale the sse with      // 1/(2*N), where N is the sample size. We will do that here as well          gradients *= (.5/y.n_rows);          return(gradients); }  // Dirk Eddelbuettel at // https://gallery.rcpp.org/articles/passing-cpp-function-pointers/ typedef double (*fitFunPtr)(const Rcpp::NumericVector&, //parameters                 Rcpp::List& //additional elements ); typedef Rcpp::XPtr<fitFunPtr> fitFunPtr_t;  typedef arma::rowvec (*gradientFunPtr)(const Rcpp::NumericVector&, //parameters                       Rcpp::List& //additional elements ); typedef Rcpp::XPtr<gradientFunPtr> gradientFunPtr_t;  // [[Rcpp::export]] fitFunPtr_t fitfunPtr() {         return(fitFunPtr_t(new fitFunPtr(&fitfunction))); }  // [[Rcpp::export]] gradientFunPtr_t gradfunPtr() {         return(gradientFunPtr_t(new gradientFunPtr(&gradientfunction))); } '  Rcpp::sourceCpp(code = linreg)  ffp <- fitfunPtr() gfp <- gradfunPtr()  N <- 100 # number of persons p <- 10 # number of predictors X <- matrix(rnorm(N*p),  nrow = N, ncol = p) # design matrix b <- c(rep(1,4),         rep(0,6)) # true regression weights y <- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)  data <- list(\"y\" = y,              \"X\" = cbind(1,X)) parameters <- rep(0, ncol(data$X)) names(parameters) <- paste0(\"b\", 0:(length(parameters)-1))  en <- gpElasticNetCpp(par = parameters,                   regularized = paste0(\"b\", 1:(length(b)-1)),                  fn = ffp,                   gr = gfp,                   lambdas = seq(0,1,.1),                   alphas = c(0,.5,1),                  additionalArguments = data)  en@parameters # }"},{"path":"/reference/gpLasso.html","id":null,"dir":"Reference","previous_headings":"","what":"gpLasso — gpLasso","title":"gpLasso — gpLasso","text":"Implements lasso regularization general purpose optimization problems. penalty function given : $$p( x_j) = \\lambda |x_j|$$ Lasso regularization set parameters zero \\(\\lambda\\) large enough","code":""},{"path":"/reference/gpLasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"gpLasso — gpLasso","text":"","code":"gpLasso(   par,   regularized,   fn,   gr = NULL,   lambdas = NULL,   nLambdas = NULL,   reverse = TRUE,   curve = 1,   ...,   method = \"glmnet\",   control = lessSEM::controlGlmnet() )"},{"path":"/reference/gpLasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"gpLasso — gpLasso","text":"par labeled vector starting values regularized vector names parameters regularized. fn R function takes parameters input returns fit value (single value) gr R function takes parameters input returns gradients objective function. set NULL, numDeriv used approximate gradients lambdas numeric vector: values tuning parameter lambda nLambdas alternative lambda: alpha = 1, lessSEM can automatically compute first lambda value sets regularized parameters zero. generate nLambda values 0 computed lambda. reverse set TRUE nLambdas used, lessSEM start largest lambda gradually decrease lambda. Otherwise, lessSEM start smallest lambda gradually increase . curve Allows unequally spaced lambda steps (e.g., .01,.02,.05,1,5,20). curve close 1 lambda values equally spaced, curve large lambda values concentrated close 0. See ?lessSEM::curveLambda information. ... additional arguments passed fn gr method optimizer used? Currently implemented ista glmnet. control used control optimizer. element generated controlIsta controlGlmnet functions. See ?controlIsta ?controlGlmnet details.","code":""},{"path":"/reference/gpLasso.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"gpLasso — gpLasso","text":"Object class gpRegularized","code":""},{"path":"/reference/gpLasso.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"gpLasso — gpLasso","text":"interface similar optim. Users supply vector starting values (important: vector must labels) fitting function. fitting functions must take labeled vector parameter values first argument. remaining arguments passed ... argument. similar optim. gradient function gr optional. set NULL, numDeriv package used approximate gradients. Supplying gradient function can result considerable speed improvements. Lasso regularization: Tibshirani, R. (1996). Regression shrinkage selection via lasso. Journal Royal Statistical Society. Series B (Methodological), 58(1), 267–288. details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classification. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/gpLasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"gpLasso — gpLasso","text":"","code":"# This example shows how to use the optimizers # for other objective functions. We will use # a linear regression as an example. Note that # this is not a useful application of the optimizers # as there are specialized packages for linear regression # (e.g., glmnet)  library(lessSEM) set.seed(123)  # first, we simulate data for our # linear regression. N <- 100 # number of persons p <- 10 # number of predictors X <- matrix(rnorm(N*p),  nrow = N, ncol = p) # design matrix b <- c(rep(1,4),         rep(0,6)) # true regression weights y <- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)  # First, we must construct a fiting function # which returns a single value. We will use # the residual sum squared as fitting function.  # Let's start setting up the fitting function: fittingFunction <- function(par, y, X, N){   # par is the parameter vector   # y is the observed dependent variable   # X is the design matrix   # N is the sample size   pred <- X %*% matrix(par, ncol = 1) #be explicit here:    # we need par to be a column vector   sse <- sum((y - pred)^2)   # we scale with .5/N to get the same results as glmnet   return((.5/N)*sse) }  # let's define the starting values: b <- rep(0,p) names(b) <- paste0(\"b\", 1:length(b)) # names of regularized parameters regularized <- paste0(\"b\",1:p)  # optimize lassoPen <- gpLasso(   par = b,    regularized = regularized,    fn = fittingFunction,    nLambdas = 100,    X = X,   y = y,   N = N ) plot(lassoPen)  # You can access the fit results as follows: lassoPen@fits # Note that we won't compute any fit measures automatically, as # we cannot be sure how the AIC, BIC, etc are defined for your objective function"},{"path":"/reference/gpLassoCpp.html","id":null,"dir":"Reference","previous_headings":"","what":"gpLassoCpp — gpLassoCpp","title":"gpLassoCpp — gpLassoCpp","text":"Implements lasso regularization general purpose optimization problems C++ functions. penalty function given : $$p( x_j) = \\lambda |x_j|$$ Lasso regularization set parameters zero \\(\\lambda\\) large enough","code":""},{"path":"/reference/gpLassoCpp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"gpLassoCpp — gpLassoCpp","text":"","code":"gpLassoCpp(   par,   regularized,   fn,   gr,   lambdas = NULL,   nLambdas = NULL,   curve = 1,   additionalArguments,   method = \"glmnet\",   control = lessSEM::controlGlmnet() )"},{"path":"/reference/gpLassoCpp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"gpLassoCpp — gpLassoCpp","text":"par labeled vector starting values regularized vector names parameters regularized. fn pointer Rcpp function takes parameters input returns fit value (single value) gr pointer Rcpp function takes parameters input returns gradients objective function. lambdas numeric vector: values tuning parameter lambda nLambdas alternative lambda: alpha = 1, lessSEM can automatically compute first lambda value sets regularized parameters zero. generate nLambda values 0 computed lambda. curve Allows unequally spaced lambda steps (e.g., .01,.02,.05,1,5,20). curve close 1 lambda values equally spaced, curve large lambda values concentrated close 0. See ?lessSEM::curveLambda information. additionalArguments list additional arguments passed fn gr method optimizer used? Currently implemented ista glmnet. control used control optimizer. element generated controlIsta controlGlmnet functions. See ?controlIsta ?controlGlmnet details.","code":""},{"path":"/reference/gpLassoCpp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"gpLassoCpp — gpLassoCpp","text":"Object class gpRegularized","code":""},{"path":"/reference/gpLassoCpp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"gpLassoCpp — gpLassoCpp","text":"interface inspired optim, bit restrictive. Users supply vector starting values (important: vector must labels), fitting function, gradient function. fitting functions must take const Rcpp::NumericVector& parameter values first argument Rcpp::List& second argument Lasso regularization: Tibshirani, R. (1996). Regression shrinkage selection via lasso. Journal Royal Statistical Society. Series B (Methodological), 58(1), 267–288. details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classification. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/gpLassoCpp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"gpLassoCpp — gpLassoCpp","text":"","code":"# \\donttest{ # This example shows how to use the optimizers # for C++ objective functions. We will use # a linear regression as an example. Note that # this is not a useful application of the optimizers # as there are specialized packages for linear regression # (e.g., glmnet)  library(Rcpp) library(lessSEM)  linreg <- ' // [[Rcpp::depends(RcppArmadillo)]] #include <RcppArmadillo.h>  // [[Rcpp::export]] double fitfunction(const Rcpp::NumericVector& parameters, Rcpp::List& data){   // extract all required elements:   arma::colvec b = Rcpp::as<arma::colvec>(parameters);   arma::colvec y = Rcpp::as<arma::colvec>(data[\"y\"]); // the dependent variable   arma::mat X = Rcpp::as<arma::mat>(data[\"X\"]); // the design matrix      // compute the sum of squared errors:     arma::mat sse = arma::trans(y-X*b)*(y-X*b);          // other packages, such as glmnet, scale the sse with      // 1/(2*N), where N is the sample size. We will do that here as well          sse *= 1.0/(2.0 * y.n_elem);          // note: We must return a double, but the sse is a matrix     // To get a double, just return the single value that is in      // this matrix:       return(sse(0,0)); }  // [[Rcpp::export]] arma::rowvec gradientfunction(const Rcpp::NumericVector& parameters, Rcpp::List& data){   // extract all required elements:   arma::colvec b = Rcpp::as<arma::colvec>(parameters);   arma::colvec y = Rcpp::as<arma::colvec>(data[\"y\"]); // the dependent variable   arma::mat X = Rcpp::as<arma::mat>(data[\"X\"]); // the design matrix      // note: we want to return our gradients as row-vector; therefore,   // we have to transpose the resulting column-vector:     arma::rowvec gradients = arma::trans(-2.0*X.t() * y + 2.0*X.t()*X*b);          // other packages, such as glmnet, scale the sse with      // 1/(2*N), where N is the sample size. We will do that here as well          gradients *= (.5/y.n_rows);          return(gradients); }  // Dirk Eddelbuettel at // https://gallery.rcpp.org/articles/passing-cpp-function-pointers/ typedef double (*fitFunPtr)(const Rcpp::NumericVector&, //parameters                 Rcpp::List& //additional elements ); typedef Rcpp::XPtr<fitFunPtr> fitFunPtr_t;  typedef arma::rowvec (*gradientFunPtr)(const Rcpp::NumericVector&, //parameters                       Rcpp::List& //additional elements ); typedef Rcpp::XPtr<gradientFunPtr> gradientFunPtr_t;  // [[Rcpp::export]] fitFunPtr_t fitfunPtr() {         return(fitFunPtr_t(new fitFunPtr(&fitfunction))); }  // [[Rcpp::export]] gradientFunPtr_t gradfunPtr() {         return(gradientFunPtr_t(new gradientFunPtr(&gradientfunction))); } '  Rcpp::sourceCpp(code = linreg)  ffp <- fitfunPtr() gfp <- gradfunPtr()  N <- 100 # number of persons p <- 10 # number of predictors X <- matrix(rnorm(N*p),  nrow = N, ncol = p) # design matrix b <- c(rep(1,4),         rep(0,6)) # true regression weights y <- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)  data <- list(\"y\" = y,              \"X\" = cbind(1,X)) parameters <- rep(0, ncol(data$X)) names(parameters) <- paste0(\"b\", 0:(length(parameters)-1))  l1 <- gpLassoCpp(par = parameters,                   regularized = paste0(\"b\", 1:(length(b)-1)),                  fn = ffp,                   gr = gfp,                   lambdas = seq(0,1,.1),                   additionalArguments = data)  l1@parameters  # }"},{"path":"/reference/gpLsp.html","id":null,"dir":"Reference","previous_headings":"","what":"gpLsp — gpLsp","title":"gpLsp — gpLsp","text":"Implements lsp regularization general purpose optimization problems. penalty function given :","code":""},{"path":"/reference/gpLsp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"gpLsp — gpLsp","text":"","code":"gpLsp(   par,   fn,   gr = NULL,   ...,   regularized,   lambdas,   thetas,   method = \"glmnet\",   control = lessSEM::controlGlmnet() )"},{"path":"/reference/gpLsp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"gpLsp — gpLsp","text":"par labeled vector starting values fn R function takes parameters labels input returns fit value (single value) gr R function takes parameters labels input returns gradients objective function. set NULL, numDeriv used approximate gradients ... additional arguments passed fn gr regularized vector names parameters regularized. lambdas numeric vector: values tuning parameter lambda thetas numeric vector: values tuning parameter theta method optimizer used? Currently implemented ista glmnet. control used control optimizer. element generated controlIsta controlGlmnet functions. See ?controlIsta ?controlGlmnet details.","code":""},{"path":"/reference/gpLsp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"gpLsp — gpLsp","text":"Object class gpRegularized","code":""},{"path":"/reference/gpLsp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"gpLsp — gpLsp","text":"interface similar optim. Users supply vector starting values (important: vector must labels) fitting function. fitting functions must take labeled vector parameter values first argument. remaining arguments passed ... argument. similar optim. gradient function gr optional. set NULL, numDeriv package used approximate gradients. Supplying gradient function can result considerable speed improvements. lsp regularization: Candès, E. J., Wakin, M. B., & Boyd, S. P. (2008). Enhancing Sparsity Reweighted l1 Minimization. Journal Fourier Analysis Applications, 14(5–6), 877–905. https://doi.org/10.1007/s00041-008-9045-x details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classification. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/gpLsp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"gpLsp — gpLsp","text":"","code":"library(lessSEM) set.seed(123)  # first, we simulate data for our # linear regression. N <- 100 # number of persons p <- 10 # number of predictors X <- matrix(rnorm(N*p),  nrow = N, ncol = p) # design matrix b <- c(rep(1,4),        rep(0,6)) # true regression weights y <- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)  # First, we must construct a fiting function # which returns a single value. We will use # the residual sum squared as fitting function.  # Let's start setting up the fitting function: fittingFunction <- function(par, y, X, N){   # par is the parameter vector   # y is the observed dependent variable   # X is the design matrix   # N is the sample size   pred <- X %*% matrix(par, ncol = 1) #be explicit here:   # we need par to be a column vector   sse <- sum((y - pred)^2)   # we scale with .5/N to get the same results as glmnet   return((.5/N)*sse) }  # let's define the starting values: b <- c(solve(t(X)%*%X)%*%t(X)%*%y) # we will use the lm estimates names(b) <- paste0(\"b\", 1:length(b)) # names of regularized parameters regularized <- paste0(\"b\",1:p)  # optimize lspPen <- gpLsp(   par = b,   regularized = regularized,   fn = fittingFunction,   lambdas = seq(0,1,.1),   thetas = c(0.001, .5, 1),   X = X,   y = y,   N = N )  # optional: plot requires plotly package # plot(lspPen)  # for comparison  fittingFunction <- function(par, y, X, N, lambda, theta){   pred <- X %*% matrix(par, ncol = 1)   sse <- sum((y - pred)^2)   smoothAbs <- sqrt(par^2 + 1e-8)   pen <- lambda * log(1.0 + smoothAbs / theta)   return((.5/N)*sse + sum(pen)) }  round(   optim(par = b,       fn = fittingFunction,       y = y,       X = X,       N = N,       lambda =  lspPen@fits$lambda[15],       theta =  lspPen@fits$theta[15],       method = \"BFGS\")$par,   4) lspPen@parameters[15,]"},{"path":"/reference/gpLspCpp.html","id":null,"dir":"Reference","previous_headings":"","what":"gpLspCpp — gpLspCpp","title":"gpLspCpp — gpLspCpp","text":"Implements lsp regularization general purpose optimization problems C++ functions. penalty function given : $$p( x_j) = \\lambda \\log(1 + |x_j|/\\theta)$$ \\(\\theta > 0\\).","code":""},{"path":"/reference/gpLspCpp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"gpLspCpp — gpLspCpp","text":"","code":"gpLspCpp(   par,   fn,   gr,   additionalArguments,   regularized,   lambdas,   thetas,   method = \"glmnet\",   control = lessSEM::controlGlmnet() )"},{"path":"/reference/gpLspCpp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"gpLspCpp — gpLspCpp","text":"par labeled vector starting values fn R function takes parameters labels input returns fit value (single value) gr R function takes parameters labels input returns gradients objective function. set NULL, numDeriv used approximate gradients additionalArguments list additional arguments passed fn gr regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda thetas numeric vector: values tuning parameter theta method optimizer used? Currently implemented ista glmnet. control used control optimizer. element generated controlIsta controlGlmnet functions. See ?controlIsta ?controlGlmnet details.","code":""},{"path":"/reference/gpLspCpp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"gpLspCpp — gpLspCpp","text":"Object class gpRegularized","code":""},{"path":"/reference/gpLspCpp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"gpLspCpp — gpLspCpp","text":"interface inspired optim, bit restrictive. Users supply vector starting values (important: vector must labels), fitting function, gradient function. fitting functions must take const Rcpp::NumericVector& parameter values first argument Rcpp::List& second argument lsp regularization: Candès, E. J., Wakin, M. B., & Boyd, S. P. (2008). Enhancing Sparsity Reweighted l1 Minimization. Journal Fourier Analysis Applications, 14(5–6), 877–905. https://doi.org/10.1007/s00041-008-9045-x details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classification. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/gpLspCpp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"gpLspCpp — gpLspCpp","text":"","code":"# \\donttest{ # This example shows how to use the optimizers # for C++ objective functions. We will use # a linear regression as an example. Note that # this is not a useful application of the optimizers # as there are specialized packages for linear regression # (e.g., glmnet)  library(Rcpp) library(lessSEM)  linreg <- ' // [[Rcpp::depends(RcppArmadillo)]] #include <RcppArmadillo.h>  // [[Rcpp::export]] double fitfunction(const Rcpp::NumericVector& parameters, Rcpp::List& data){   // extract all required elements:   arma::colvec b = Rcpp::as<arma::colvec>(parameters);   arma::colvec y = Rcpp::as<arma::colvec>(data[\"y\"]); // the dependent variable   arma::mat X = Rcpp::as<arma::mat>(data[\"X\"]); // the design matrix      // compute the sum of squared errors:     arma::mat sse = arma::trans(y-X*b)*(y-X*b);          // other packages, such as glmnet, scale the sse with      // 1/(2*N), where N is the sample size. We will do that here as well          sse *= 1.0/(2.0 * y.n_elem);          // note: We must return a double, but the sse is a matrix     // To get a double, just return the single value that is in      // this matrix:       return(sse(0,0)); }  // [[Rcpp::export]] arma::rowvec gradientfunction(const Rcpp::NumericVector& parameters, Rcpp::List& data){   // extract all required elements:   arma::colvec b = Rcpp::as<arma::colvec>(parameters);   arma::colvec y = Rcpp::as<arma::colvec>(data[\"y\"]); // the dependent variable   arma::mat X = Rcpp::as<arma::mat>(data[\"X\"]); // the design matrix      // note: we want to return our gradients as row-vector; therefore,   // we have to transpose the resulting column-vector:     arma::rowvec gradients = arma::trans(-2.0*X.t() * y + 2.0*X.t()*X*b);          // other packages, such as glmnet, scale the sse with      // 1/(2*N), where N is the sample size. We will do that here as well          gradients *= (.5/y.n_rows);          return(gradients); }  // Dirk Eddelbuettel at // https://gallery.rcpp.org/articles/passing-cpp-function-pointers/ typedef double (*fitFunPtr)(const Rcpp::NumericVector&, //parameters                 Rcpp::List& //additional elements ); typedef Rcpp::XPtr<fitFunPtr> fitFunPtr_t;  typedef arma::rowvec (*gradientFunPtr)(const Rcpp::NumericVector&, //parameters                       Rcpp::List& //additional elements ); typedef Rcpp::XPtr<gradientFunPtr> gradientFunPtr_t;  // [[Rcpp::export]] fitFunPtr_t fitfunPtr() {         return(fitFunPtr_t(new fitFunPtr(&fitfunction))); }  // [[Rcpp::export]] gradientFunPtr_t gradfunPtr() {         return(gradientFunPtr_t(new gradientFunPtr(&gradientfunction))); } '  Rcpp::sourceCpp(code = linreg)  ffp <- fitfunPtr() gfp <- gradfunPtr()  N <- 100 # number of persons p <- 10 # number of predictors X <- matrix(rnorm(N*p),  nrow = N, ncol = p) # design matrix b <- c(rep(1,4),         rep(0,6)) # true regression weights y <- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)  data <- list(\"y\" = y,              \"X\" = cbind(1,X)) parameters <- rep(0, ncol(data$X)) names(parameters) <- paste0(\"b\", 0:(length(parameters)-1))  l <- gpLspCpp(par = parameters,                   regularized = paste0(\"b\", 1:(length(b)-1)),                  fn = ffp,                   gr = gfp,                   lambdas = seq(0,1,.1),                   thetas = seq(0.1,1,.1),                  additionalArguments = data)  l@parameters # }"},{"path":"/reference/gpMcp.html","id":null,"dir":"Reference","previous_headings":"","what":"gpMcp — gpMcp","title":"gpMcp — gpMcp","text":"Implements mcp regularization general purpose optimization problems. penalty function given : $$p( x_j) = \\begin{cases} \\lambda |x_j| - x_j^2/(2\\theta) & \\text{} |x_j| \\leq \\theta\\lambda\\\\ \\theta\\lambda^2/2 & \\text{} |x_j| > \\lambda\\theta \\end{cases}$$ \\(\\theta > 0\\).","code":""},{"path":"/reference/gpMcp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"gpMcp — gpMcp","text":"","code":"gpMcp(   par,   fn,   gr = NULL,   ...,   regularized,   lambdas,   thetas,   method = \"glmnet\",   control = lessSEM::controlGlmnet() )"},{"path":"/reference/gpMcp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"gpMcp — gpMcp","text":"par labeled vector starting values fn R function takes parameters labels input returns fit value (single value) gr R function takes parameters labels input returns gradients objective function. set NULL, numDeriv used approximate gradients ... additional arguments passed fn gr regularized vector names parameters regularized. lambdas numeric vector: values tuning parameter lambda thetas numeric vector: values tuning parameter theta method optimizer used? Currently implemented ista glmnet. control used control optimizer. element generated controlIsta controlGlmnet functions. See ?controlIsta ?controlGlmnet details.","code":""},{"path":"/reference/gpMcp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"gpMcp — gpMcp","text":"Object class gpRegularized","code":""},{"path":"/reference/gpMcp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"gpMcp — gpMcp","text":"interface similar optim. Users supply vector starting values (important: vector must labels) fitting function. fitting functions must take labeled vector parameter values first argument. remaining arguments passed ... argument. similar optim. gradient function gr optional. set NULL, numDeriv package used approximate gradients. Supplying gradient function can result considerable speed improvements. mcp regularization: Zhang, C.-H. (2010). Nearly unbiased variable selection minimax concave penalty. Annals Statistics, 38(2), 894–942. https://doi.org/10.1214/09-AOS729 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classification. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/gpMcp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"gpMcp — gpMcp","text":"","code":"# This example shows how to use the optimizers # for other objective functions. We will use # a linear regression as an example. Note that # this is not a useful application of the optimizers # as there are specialized packages for linear regression # (e.g., glmnet)  library(lessSEM) set.seed(123)  # first, we simulate data for our # linear regression. N <- 100 # number of persons p <- 10 # number of predictors X <- matrix(rnorm(N*p),  nrow = N, ncol = p) # design matrix b <- c(rep(1,4),        rep(0,6)) # true regression weights y <- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)  # First, we must construct a fiting function # which returns a single value. We will use # the residual sum squared as fitting function.  # Let's start setting up the fitting function: fittingFunction <- function(par, y, X, N){   # par is the parameter vector   # y is the observed dependent variable   # X is the design matrix   # N is the sample size   pred <- X %*% matrix(par, ncol = 1) #be explicit here:   # we need par to be a column vector   sse <- sum((y - pred)^2)   # we scale with .5/N to get the same results as glmnet   return((.5/N)*sse) }  # let's define the starting values: # first, let's add an intercept X <- cbind(1, X)  b <- c(solve(t(X)%*%X)%*%t(X)%*%y) # we will use the lm estimates names(b) <- paste0(\"b\", 0:(length(b)-1)) # names of regularized parameters regularized <- paste0(\"b\",1:p)  # optimize mcpPen <- gpMcp(   par = b,   regularized = regularized,   fn = fittingFunction,   lambdas = seq(0,1,.1),   thetas = c(1.001, 1.5, 2),   X = X,   y = y,   N = N )  # optional: plot requires plotly package # plot(mcpPen)"},{"path":"/reference/gpMcpCpp.html","id":null,"dir":"Reference","previous_headings":"","what":"gpMcpCpp — gpMcpCpp","title":"gpMcpCpp — gpMcpCpp","text":"Implements mcp regularization general purpose optimization problems C++ functions. penalty function given : $$p( x_j) = \\begin{cases} \\lambda |x_j| - x_j^2/(2\\theta) & \\text{} |x_j| \\leq \\theta\\lambda\\\\ \\theta\\lambda^2/2 & \\text{} |x_j| > \\lambda\\theta \\end{cases}$$ \\(\\theta > 0\\).","code":""},{"path":"/reference/gpMcpCpp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"gpMcpCpp — gpMcpCpp","text":"","code":"gpMcpCpp(   par,   fn,   gr,   additionalArguments,   regularized,   lambdas,   thetas,   method = \"glmnet\",   control = lessSEM::controlGlmnet() )"},{"path":"/reference/gpMcpCpp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"gpMcpCpp — gpMcpCpp","text":"par labeled vector starting values fn R function takes parameters labels input returns fit value (single value) gr R function takes parameters labels input returns gradients objective function. set NULL, numDeriv used approximate gradients additionalArguments list additional arguments passed fn gr regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda thetas numeric vector: values tuning parameter theta method optimizer used? Currently implemented ista glmnet. control used control optimizer. element generated controlIsta controlGlmnet functions. See ?controlIsta ?controlGlmnet details.","code":""},{"path":"/reference/gpMcpCpp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"gpMcpCpp — gpMcpCpp","text":"Object class gpRegularized","code":""},{"path":"/reference/gpMcpCpp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"gpMcpCpp — gpMcpCpp","text":"interface inspired optim, bit restrictive. Users supply vector starting values (important: vector must labels), fitting function, gradient function. fitting functions must take const Rcpp::NumericVector& parameter values first argument Rcpp::List& second argument mcp regularization: Zhang, C.-H. (2010). Nearly unbiased variable selection minimax concave penalty. Annals Statistics, 38(2), 894–942. https://doi.org/10.1214/09-AOS729 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classification. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/gpMcpCpp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"gpMcpCpp — gpMcpCpp","text":"","code":"# \\donttest{ # This example shows how to use the optimizers # for C++ objective functions. We will use # a linear regression as an example. Note that # this is not a useful application of the optimizers # as there are specialized packages for linear regression # (e.g., glmnet)  library(Rcpp) library(lessSEM)  linreg <- ' // [[Rcpp::depends(RcppArmadillo)]] #include <RcppArmadillo.h>  // [[Rcpp::export]] double fitfunction(const Rcpp::NumericVector& parameters, Rcpp::List& data){   // extract all required elements:   arma::colvec b = Rcpp::as<arma::colvec>(parameters);   arma::colvec y = Rcpp::as<arma::colvec>(data[\"y\"]); // the dependent variable   arma::mat X = Rcpp::as<arma::mat>(data[\"X\"]); // the design matrix      // compute the sum of squared errors:     arma::mat sse = arma::trans(y-X*b)*(y-X*b);          // other packages, such as glmnet, scale the sse with      // 1/(2*N), where N is the sample size. We will do that here as well          sse *= 1.0/(2.0 * y.n_elem);          // note: We must return a double, but the sse is a matrix     // To get a double, just return the single value that is in      // this matrix:       return(sse(0,0)); }  // [[Rcpp::export]] arma::rowvec gradientfunction(const Rcpp::NumericVector& parameters, Rcpp::List& data){   // extract all required elements:   arma::colvec b = Rcpp::as<arma::colvec>(parameters);   arma::colvec y = Rcpp::as<arma::colvec>(data[\"y\"]); // the dependent variable   arma::mat X = Rcpp::as<arma::mat>(data[\"X\"]); // the design matrix      // note: we want to return our gradients as row-vector; therefore,   // we have to transpose the resulting column-vector:     arma::rowvec gradients = arma::trans(-2.0*X.t() * y + 2.0*X.t()*X*b);          // other packages, such as glmnet, scale the sse with      // 1/(2*N), where N is the sample size. We will do that here as well          gradients *= (.5/y.n_rows);          return(gradients); }  // Dirk Eddelbuettel at // https://gallery.rcpp.org/articles/passing-cpp-function-pointers/ typedef double (*fitFunPtr)(const Rcpp::NumericVector&, //parameters                 Rcpp::List& //additional elements ); typedef Rcpp::XPtr<fitFunPtr> fitFunPtr_t;  typedef arma::rowvec (*gradientFunPtr)(const Rcpp::NumericVector&, //parameters                       Rcpp::List& //additional elements ); typedef Rcpp::XPtr<gradientFunPtr> gradientFunPtr_t;  // [[Rcpp::export]] fitFunPtr_t fitfunPtr() {         return(fitFunPtr_t(new fitFunPtr(&fitfunction))); }  // [[Rcpp::export]] gradientFunPtr_t gradfunPtr() {         return(gradientFunPtr_t(new gradientFunPtr(&gradientfunction))); } '  Rcpp::sourceCpp(code = linreg)  ffp <- fitfunPtr() gfp <- gradfunPtr()  N <- 100 # number of persons p <- 10 # number of predictors X <- matrix(rnorm(N*p),  nrow = N, ncol = p) # design matrix b <- c(rep(1,4),         rep(0,6)) # true regression weights y <- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)  data <- list(\"y\" = y,              \"X\" = cbind(1,X)) parameters <- rep(0, ncol(data$X)) names(parameters) <- paste0(\"b\", 0:(length(parameters)-1))  m <- gpMcpCpp(par = parameters,                   regularized = paste0(\"b\", 1:(length(b)-1)),                  fn = ffp,                   gr = gfp,                   lambdas = seq(0,1,.1),                   thetas = seq(.1,1,.1),                  additionalArguments = data)  m@parameters # }"},{"path":"/reference/gpRegularized-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Class for regularized model using general purpose optimization interface — gpRegularized-class","title":"Class for regularized model using general purpose optimization interface — gpRegularized-class","text":"Class regularized model using general purpose optimization interface","code":""},{"path":"/reference/gpRegularized-class.html","id":"slots","dir":"Reference","previous_headings":"","what":"Slots","title":"Class for regularized model using general purpose optimization interface — gpRegularized-class","text":"penalty penalty used (e.g., \"lasso\") parameters data.frame parameter estimates fits data.frame fit results parameterLabels character vector names parameters weights vector weights given parameters penalty regularized character vector names regularized parameters internalOptimization list elements used internally inputArguments list elements passed user general purpose optimizer","code":""},{"path":"/reference/gpRidge.html","id":null,"dir":"Reference","previous_headings":"","what":"gpRidge — gpRidge","title":"gpRidge — gpRidge","text":"Implements ridge regularization general purpose optimization problems. penalty function given : $$p( x_j) = \\lambda x_j^2$$ Note ridge regularization set parameters zero result shrinkage towards zero.","code":""},{"path":"/reference/gpRidge.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"gpRidge — gpRidge","text":"","code":"gpRidge(   par,   regularized,   fn,   gr = NULL,   lambdas,   ...,   method = \"glmnet\",   control = lessSEM::controlGlmnet() )"},{"path":"/reference/gpRidge.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"gpRidge — gpRidge","text":"par labeled vector starting values regularized vector names parameters regularized. fn R function takes parameters input returns fit value (single value) gr R function takes parameters input returns gradients objective function. set NULL, numDeriv used approximate gradients lambdas numeric vector: values tuning parameter lambda ... additional arguments passed fn gr method optimizer used? Currently implemented ista glmnet. control used control optimizer. element generated controlIsta controlGlmnet functions. See ?controlIsta ?controlGlmnet details.","code":""},{"path":"/reference/gpRidge.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"gpRidge — gpRidge","text":"Object class gpRegularized","code":""},{"path":"/reference/gpRidge.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"gpRidge — gpRidge","text":"interface similar optim. Users supply vector starting values (important: vector must labels) fitting function. fitting functions must take labeled vector parameter values first argument. remaining arguments passed ... argument. similar optim. gradient function gr optional. set NULL, numDeriv package used approximate gradients. Supplying gradient function can result considerable speed improvements. Ridge regularization: Hoerl, . E., & Kennard, R. W. (1970). Ridge Regression: Biased Estimation Nonorthogonal Problems. Technometrics, 12(1), 55–67. https://doi.org/10.1080/00401706.1970.10488634 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classification. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/gpRidge.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"gpRidge — gpRidge","text":"","code":"# This example shows how to use the optimizers # for other objective functions. We will use # a linear regression as an example. Note that # this is not a useful application of the optimizers # as there are specialized packages for linear regression # (e.g., glmnet)  library(lessSEM) set.seed(123)  # first, we simulate data for our # linear regression. N <- 100 # number of persons p <- 10 # number of predictors X <- matrix(rnorm(N*p),  nrow = N, ncol = p) # design matrix b <- c(rep(1,4),        rep(0,6)) # true regression weights y <- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)  # First, we must construct a fiting function # which returns a single value. We will use # the residual sum squared as fitting function.  # Let's start setting up the fitting function: fittingFunction <- function(par, y, X, N){   # par is the parameter vector   # y is the observed dependent variable   # X is the design matrix   # N is the sample size   pred <- X %*% matrix(par, ncol = 1) #be explicit here:   # we need par to be a column vector   sse <- sum((y - pred)^2)   # we scale with .5/N to get the same results as glmnet   return((.5/N)*sse) }  # let's define the starting values: b <- c(solve(t(X)%*%X)%*%t(X)%*%y) # we will use the lm estimates names(b) <- paste0(\"b\", 1:length(b)) # names of regularized parameters regularized <- paste0(\"b\",1:p)  # optimize ridgePen <- gpRidge(   par = b,   regularized = regularized,   fn = fittingFunction,   lambdas = seq(0,1,.01),   X = X,   y = y,   N = N ) plot(ridgePen)  # for comparison: # fittingFunction <- function(par, y, X, N, lambda){ #   pred <- X %*% matrix(par, ncol = 1)  #   sse <- sum((y - pred)^2) #   return((.5/N)*sse + lambda * sum(par^2)) # } #  # optim(par = b,  #       fn = fittingFunction,  #       y = y, #       X = X, #       N = N, #       lambda =  ridgePen@fits$lambda[20],  #       method = \"BFGS\")$par # ridgePen@parameters[20,]"},{"path":"/reference/gpRidgeCpp.html","id":null,"dir":"Reference","previous_headings":"","what":"gpRidgeCpp — gpRidgeCpp","title":"gpRidgeCpp — gpRidgeCpp","text":"Implements ridge regularization general purpose optimization problems C++ functions. penalty function given : $$p( x_j) = \\lambda x_j^2$$ Note ridge regularization set parameters zero result shrinkage towards zero.","code":""},{"path":"/reference/gpRidgeCpp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"gpRidgeCpp — gpRidgeCpp","text":"","code":"gpRidgeCpp(   par,   regularized,   fn,   gr,   lambdas,   additionalArguments,   method = \"glmnet\",   control = lessSEM::controlGlmnet() )"},{"path":"/reference/gpRidgeCpp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"gpRidgeCpp — gpRidgeCpp","text":"par labeled vector starting values regularized vector names parameters regularized. fn R function takes parameters input returns fit value (single value) gr R function takes parameters input returns gradients objective function. set NULL, numDeriv used approximate gradients lambdas numeric vector: values tuning parameter lambda additionalArguments list additional arguments passed fn gr method optimizer used? Currently implemented ista glmnet. control used control optimizer. element generated controlIsta controlGlmnet functions. See ?controlIsta ?controlGlmnet details.","code":""},{"path":"/reference/gpRidgeCpp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"gpRidgeCpp — gpRidgeCpp","text":"Object class gpRegularized","code":""},{"path":"/reference/gpRidgeCpp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"gpRidgeCpp — gpRidgeCpp","text":"interface inspired optim, bit restrictive. Users supply vector starting values (important: vector must labels), fitting function, gradient function. fitting functions must take const Rcpp::NumericVector& parameter values first argument Rcpp::List& second argument Ridge regularization: Hoerl, . E., & Kennard, R. W. (1970). Ridge Regression: Biased Estimation Nonorthogonal Problems. Technometrics, 12(1), 55–67. https://doi.org/10.1080/00401706.1970.10488634 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classification. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/gpRidgeCpp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"gpRidgeCpp — gpRidgeCpp","text":"","code":"# \\donttest{ # This example shows how to use the optimizers # for C++ objective functions. We will use # a linear regression as an example. Note that # this is not a useful application of the optimizers # as there are specialized packages for linear regression # (e.g., glmnet)  library(Rcpp) library(lessSEM)  linreg <- ' // [[Rcpp::depends(RcppArmadillo)]] #include <RcppArmadillo.h>  // [[Rcpp::export]] double fitfunction(const Rcpp::NumericVector& parameters, Rcpp::List& data){   // extract all required elements:   arma::colvec b = Rcpp::as<arma::colvec>(parameters);   arma::colvec y = Rcpp::as<arma::colvec>(data[\"y\"]); // the dependent variable   arma::mat X = Rcpp::as<arma::mat>(data[\"X\"]); // the design matrix      // compute the sum of squared errors:     arma::mat sse = arma::trans(y-X*b)*(y-X*b);          // other packages, such as glmnet, scale the sse with      // 1/(2*N), where N is the sample size. We will do that here as well          sse *= 1.0/(2.0 * y.n_elem);          // note: We must return a double, but the sse is a matrix     // To get a double, just return the single value that is in      // this matrix:       return(sse(0,0)); }  // [[Rcpp::export]] arma::rowvec gradientfunction(const Rcpp::NumericVector& parameters, Rcpp::List& data){   // extract all required elements:   arma::colvec b = Rcpp::as<arma::colvec>(parameters);   arma::colvec y = Rcpp::as<arma::colvec>(data[\"y\"]); // the dependent variable   arma::mat X = Rcpp::as<arma::mat>(data[\"X\"]); // the design matrix      // note: we want to return our gradients as row-vector; therefore,   // we have to transpose the resulting column-vector:     arma::rowvec gradients = arma::trans(-2.0*X.t() * y + 2.0*X.t()*X*b);          // other packages, such as glmnet, scale the sse with      // 1/(2*N), where N is the sample size. We will do that here as well          gradients *= (.5/y.n_rows);          return(gradients); }  // https://gallery.rcpp.org/articles/passing-cpp-function-pointers/ typedef double (*fitFunPtr)(const Rcpp::NumericVector&, //parameters                 Rcpp::List& //additional elements ); typedef Rcpp::XPtr<fitFunPtr> fitFunPtr_t;  typedef arma::rowvec (*gradientFunPtr)(const Rcpp::NumericVector&, //parameters                       Rcpp::List& //additional elements ); typedef Rcpp::XPtr<gradientFunPtr> gradientFunPtr_t;  // [[Rcpp::export]] fitFunPtr_t fitfunPtr() {         return(fitFunPtr_t(new fitFunPtr(&fitfunction))); }  // [[Rcpp::export]] gradientFunPtr_t gradfunPtr() {         return(gradientFunPtr_t(new gradientFunPtr(&gradientfunction))); } '  Rcpp::sourceCpp(code = linreg)  ffp <- fitfunPtr() gfp <- gradfunPtr()  N <- 100 # number of persons p <- 10 # number of predictors X <- matrix(rnorm(N*p),  nrow = N, ncol = p) # design matrix b <- c(rep(1,4),         rep(0,6)) # true regression weights y <- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)  data <- list(\"y\" = y,              \"X\" = cbind(1,X)) parameters <- rep(0, ncol(data$X)) names(parameters) <- paste0(\"b\", 0:(length(parameters)-1))  r <- gpRidgeCpp(par = parameters,                   regularized = paste0(\"b\", 1:(length(b)-1)),                  fn = ffp,                   gr = gfp,                   lambdas = seq(0,1,.1),                   additionalArguments = data)  r@parameters # }"},{"path":"/reference/gpScad.html","id":null,"dir":"Reference","previous_headings":"","what":"gpScad — gpScad","title":"gpScad — gpScad","text":"Implements scad regularization general purpose optimization problems. penalty function given :  $$p( x_j) = \\begin{cases} \\lambda |x_j| & \\text{} |x_j| \\leq \\theta\\\\ \\frac{-x_j^2 + 2\\theta\\lambda |x_j| - \\lambda^2}{2(\\theta -1)} &  \\text{} \\lambda < |x_j| \\leq \\lambda\\theta \\\\ (\\theta + 1) \\lambda^2/2 & \\text{} |x_j| \\geq \\theta\\lambda\\\\ \\end{cases}$$ \\(\\theta > 2\\).","code":""},{"path":"/reference/gpScad.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"gpScad — gpScad","text":"","code":"gpScad(   par,   fn,   gr = NULL,   ...,   regularized,   lambdas,   thetas,   method = \"glmnet\",   control = lessSEM::controlGlmnet() )"},{"path":"/reference/gpScad.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"gpScad — gpScad","text":"par labeled vector starting values fn R function takes parameters labels input returns fit value (single value) gr R function takes parameters labels input returns gradients objective function. set NULL, numDeriv used approximate gradients ... additional arguments passed fn gr regularized vector names parameters regularized. lambdas numeric vector: values tuning parameter lambda thetas numeric vector: values tuning parameter theta method optimizer used? Currently implemented ista glmnet. control used control optimizer. element generated controlIsta controlGlmnet functions. See ?controlIsta ?controlGlmnet details.","code":""},{"path":"/reference/gpScad.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"gpScad — gpScad","text":"Object class gpRegularized","code":""},{"path":"/reference/gpScad.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"gpScad — gpScad","text":"interface similar optim. Users supply vector starting values (important: vector must labels) fitting function. fitting functions must take labeled vector parameter values first argument. remaining arguments passed ... argument. similar optim. gradient function gr optional. set NULL, numDeriv package used approximate gradients. Supplying gradient function can result considerable speed improvements. scad regularization: Fan, J., & Li, R. (2001). Variable selection via nonconcave penalized likelihood oracle properties. Journal American Statistical Association, 96(456), 1348–1360. https://doi.org/10.1198/016214501753382273 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classification. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/gpScad.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"gpScad — gpScad","text":"","code":"# This example shows how to use the optimizers # for other objective functions. We will use # a linear regression as an example. Note that # this is not a useful application of the optimizers # as there are specialized packages for linear regression # (e.g., glmnet)  library(lessSEM) set.seed(123)  # first, we simulate data for our # linear regression. N <- 100 # number of persons p <- 10 # number of predictors X <- matrix(rnorm(N*p),  nrow = N, ncol = p) # design matrix b <- c(rep(1,4),        rep(0,6)) # true regression weights y <- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)  # First, we must construct a fiting function # which returns a single value. We will use # the residual sum squared as fitting function.  # Let's start setting up the fitting function: fittingFunction <- function(par, y, X, N){   # par is the parameter vector   # y is the observed dependent variable   # X is the design matrix   # N is the sample size   pred <- X %*% matrix(par, ncol = 1) #be explicit here:   # we need par to be a column vector   sse <- sum((y - pred)^2)   # we scale with .5/N to get the same results as glmnet   return((.5/N)*sse) }  # let's define the starting values: # first, let's add an intercept X <- cbind(1, X)  b <- c(solve(t(X)%*%X)%*%t(X)%*%y) # we will use the lm estimates names(b) <- paste0(\"b\", 0:(length(b)-1)) # names of regularized parameters regularized <- paste0(\"b\",1:p)  # optimize scadPen <- gpScad(   par = b,   regularized = regularized,   fn = fittingFunction,   lambdas = seq(0,1,.1),   thetas = c(2.001, 2.5, 5),   X = X,   y = y,   N = N )  # optional: plot requires plotly package # plot(scadPen)  # for comparison #library(ncvreg) #scadFit <- ncvreg(X = X[,-1],  #                  y = y,  #                  penalty = \"SCAD\", #                  lambda =  scadPen@fits$lambda[15], #                  gamma =  scadPen@fits$theta[15]) #coef(scadFit) #scadPen@parameters[15,]"},{"path":"/reference/gpScadCpp.html","id":null,"dir":"Reference","previous_headings":"","what":"gpScadCpp — gpScadCpp","title":"gpScadCpp — gpScadCpp","text":"Implements scad regularization general purpose optimization problems C++ functions. penalty function given :  $$p( x_j) = \\begin{cases} \\lambda |x_j| & \\text{} |x_j| \\leq \\theta\\\\ \\frac{-x_j^2 + 2\\theta\\lambda |x_j| - \\lambda^2}{2(\\theta -1)} &  \\text{} \\lambda < |x_j| \\leq \\lambda\\theta \\\\ (\\theta + 1) \\lambda^2/2 & \\text{} |x_j| \\geq \\theta\\lambda\\\\ \\end{cases}$$ \\(\\theta > 2\\).","code":""},{"path":"/reference/gpScadCpp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"gpScadCpp — gpScadCpp","text":"","code":"gpScadCpp(   par,   fn,   gr,   additionalArguments,   regularized,   lambdas,   thetas,   method = \"glmnet\",   control = lessSEM::controlGlmnet() )"},{"path":"/reference/gpScadCpp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"gpScadCpp — gpScadCpp","text":"par labeled vector starting values fn R function takes parameters labels input returns fit value (single value) gr R function takes parameters labels input returns gradients objective function. set NULL, numDeriv used approximate gradients additionalArguments list additional arguments passed fn gr regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda thetas numeric vector: values tuning parameter theta method optimizer used? Currently implemented ista glmnet. control used control optimizer. element generated controlIsta controlGlmnet functions. See ?controlIsta ?controlGlmnet details.","code":""},{"path":"/reference/gpScadCpp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"gpScadCpp — gpScadCpp","text":"Object class gpRegularized","code":""},{"path":"/reference/gpScadCpp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"gpScadCpp — gpScadCpp","text":"interface inspired optim, bit restrictive. Users supply vector starting values (important: vector must labels), fitting function, gradient function. fitting functions must take const Rcpp::NumericVector& parameter values first argument Rcpp::List& second argument scad regularization: Fan, J., & Li, R. (2001). Variable selection via nonconcave penalized likelihood oracle properties. Journal American Statistical Association, 96(456), 1348–1360. https://doi.org/10.1198/016214501753382273 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classification. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/gpScadCpp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"gpScadCpp — gpScadCpp","text":"","code":"# \\donttest{ # This example shows how to use the optimizers # for C++ objective functions. We will use # a linear regression as an example. Note that # this is not a useful application of the optimizers # as there are specialized packages for linear regression # (e.g., glmnet)  library(Rcpp) library(lessSEM)  linreg <- ' // [[Rcpp::depends(RcppArmadillo)]] #include <RcppArmadillo.h>  // [[Rcpp::export]] double fitfunction(const Rcpp::NumericVector& parameters, Rcpp::List& data){   // extract all required elements:   arma::colvec b = Rcpp::as<arma::colvec>(parameters);   arma::colvec y = Rcpp::as<arma::colvec>(data[\"y\"]); // the dependent variable   arma::mat X = Rcpp::as<arma::mat>(data[\"X\"]); // the design matrix      // compute the sum of squared errors:     arma::mat sse = arma::trans(y-X*b)*(y-X*b);          // other packages, such as glmnet, scale the sse with      // 1/(2*N), where N is the sample size. We will do that here as well          sse *= 1.0/(2.0 * y.n_elem);          // note: We must return a double, but the sse is a matrix     // To get a double, just return the single value that is in      // this matrix:       return(sse(0,0)); }  // [[Rcpp::export]] arma::rowvec gradientfunction(const Rcpp::NumericVector& parameters, Rcpp::List& data){   // extract all required elements:   arma::colvec b = Rcpp::as<arma::colvec>(parameters);   arma::colvec y = Rcpp::as<arma::colvec>(data[\"y\"]); // the dependent variable   arma::mat X = Rcpp::as<arma::mat>(data[\"X\"]); // the design matrix      // note: we want to return our gradients as row-vector; therefore,   // we have to transpose the resulting column-vector:     arma::rowvec gradients = arma::trans(-2.0*X.t() * y + 2.0*X.t()*X*b);          // other packages, such as glmnet, scale the sse with      // 1/(2*N), where N is the sample size. We will do that here as well          gradients *= (.5/y.n_rows);          return(gradients); }  // Dirk Eddelbuettel at // https://gallery.rcpp.org/articles/passing-cpp-function-pointers/ typedef double (*fitFunPtr)(const Rcpp::NumericVector&, //parameters                 Rcpp::List& //additional elements ); typedef Rcpp::XPtr<fitFunPtr> fitFunPtr_t;  typedef arma::rowvec (*gradientFunPtr)(const Rcpp::NumericVector&, //parameters                       Rcpp::List& //additional elements ); typedef Rcpp::XPtr<gradientFunPtr> gradientFunPtr_t;  // [[Rcpp::export]] fitFunPtr_t fitfunPtr() {         return(fitFunPtr_t(new fitFunPtr(&fitfunction))); }  // [[Rcpp::export]] gradientFunPtr_t gradfunPtr() {         return(gradientFunPtr_t(new gradientFunPtr(&gradientfunction))); } '  Rcpp::sourceCpp(code = linreg)  ffp <- fitfunPtr() gfp <- gradfunPtr()  N <- 100 # number of persons p <- 10 # number of predictors X <- matrix(rnorm(N*p),  nrow = N, ncol = p) # design matrix b <- c(rep(1,4),         rep(0,6)) # true regression weights y <- X%*%matrix(b,ncol = 1) + rnorm(N,0,.2)  data <- list(\"y\" = y,              \"X\" = cbind(1,X)) parameters <- rep(0, ncol(data$X)) names(parameters) <- paste0(\"b\", 0:(length(parameters)-1))  s <- gpScadCpp(par = parameters,                   regularized = paste0(\"b\", 1:(length(b)-1)),                  fn = ffp,                   gr = gfp,                   lambdas = seq(0,1,.1),                   thetas = seq(2.1,3,.1),                  additionalArguments = data)  s@parameters # }"},{"path":"/reference/istaCappedL1SEM.html","id":null,"dir":"Reference","previous_headings":"","what":"cappedL1 optimization with ista — istaCappedL1SEM","title":"cappedL1 optimization with ista — istaCappedL1SEM","text":"Object elastic net optimization ista optimizer","code":""},{"path":"/reference/istaCappedL1SEM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"cappedL1 optimization with ista — istaCappedL1SEM","text":"list fit results","code":""},{"path":"/reference/istaCappedL1SEM.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"cappedL1 optimization with ista — istaCappedL1SEM","text":"new creates new object. Requires (1) vector weights parameter (2) list control elements optimize optimize model. Expects vector starting values, SEM type SEM_Cpp, theta value, lambda alpha value (alpha must 1).","code":""},{"path":"/reference/istaCappedL1mgSEM.html","id":null,"dir":"Reference","previous_headings":"","what":"cappedL1 optimization with ista — istaCappedL1mgSEM","title":"cappedL1 optimization with ista — istaCappedL1mgSEM","text":"Object elastic net optimization ista optimizer","code":""},{"path":"/reference/istaCappedL1mgSEM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"cappedL1 optimization with ista — istaCappedL1mgSEM","text":"list fit results","code":""},{"path":"/reference/istaCappedL1mgSEM.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"cappedL1 optimization with ista — istaCappedL1mgSEM","text":"new creates new object. Requires (1) vector weights parameter (2) list control elements optimize optimize model. Expects vector starting values, SEM type SEM_Cpp, theta value, lambda alpha value (alpha must 1).","code":""},{"path":"/reference/istaEnetGeneralPurpose.html","id":null,"dir":"Reference","previous_headings":"","what":"elastic net optimization with ista — istaEnetGeneralPurpose","title":"elastic net optimization with ista — istaEnetGeneralPurpose","text":"Object elastic net optimization ista optimizer","code":""},{"path":"/reference/istaEnetGeneralPurpose.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"elastic net optimization with ista — istaEnetGeneralPurpose","text":"list fit results","code":""},{"path":"/reference/istaEnetGeneralPurpose.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"elastic net optimization with ista — istaEnetGeneralPurpose","text":"new creates new object. Requires (1) vector weights parameter (2) list control elements optimize optimize model. Expects vector starting values, R function compute fit, R function compute gradients, list elements fit gradient function require, lambda alpha value.","code":""},{"path":"/reference/istaEnetGeneralPurposeCpp.html","id":null,"dir":"Reference","previous_headings":"","what":"elastic net optimization with ista — istaEnetGeneralPurposeCpp","title":"elastic net optimization with ista — istaEnetGeneralPurposeCpp","text":"Object elastic net optimization ista optimizer","code":""},{"path":"/reference/istaEnetGeneralPurposeCpp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"elastic net optimization with ista — istaEnetGeneralPurposeCpp","text":"list fit results","code":""},{"path":"/reference/istaEnetGeneralPurposeCpp.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"elastic net optimization with ista — istaEnetGeneralPurposeCpp","text":"new creates new object. Requires (1) vector weights parameter (2) list control elements optimize optimize model. Expects vector starting values, SEXP function pointer compute fit, SEXP function pointer compute gradients, list elements fit gradient function require, lambda alpha value.","code":""},{"path":"/reference/istaEnetMgSEM.html","id":null,"dir":"Reference","previous_headings":"","what":"elastic net optimization with ista optimizer — istaEnetMgSEM","title":"elastic net optimization with ista optimizer — istaEnetMgSEM","text":"Object elastic net optimization glmnet optimizer","code":""},{"path":"/reference/istaEnetMgSEM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"elastic net optimization with ista optimizer — istaEnetMgSEM","text":"list fit results","code":""},{"path":"/reference/istaEnetMgSEM.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"elastic net optimization with ista optimizer — istaEnetMgSEM","text":"new creates new object. Requires (1) vector weights parameter (2) list control elements optimize optimize model. Expects vector starting values, SEM type SEM_Cpp, lambda alpha value.","code":""},{"path":"/reference/istaEnetSEM.html","id":null,"dir":"Reference","previous_headings":"","what":"elastic net optimization with ista optimizer — istaEnetSEM","title":"elastic net optimization with ista optimizer — istaEnetSEM","text":"Object elastic net optimization glmnet optimizer","code":""},{"path":"/reference/istaEnetSEM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"elastic net optimization with ista optimizer — istaEnetSEM","text":"list fit results","code":""},{"path":"/reference/istaEnetSEM.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"elastic net optimization with ista optimizer — istaEnetSEM","text":"new creates new object. Requires (1) vector weights parameter (2) list control elements optimize optimize model. Expects vector starting values, SEM type SEM_Cpp, lambda alpha value.","code":""},{"path":"/reference/istaLSPMgSEM.html","id":null,"dir":"Reference","previous_headings":"","what":"lsp optimization with ista — istaLSPMgSEM","title":"lsp optimization with ista — istaLSPMgSEM","text":"Object lsp optimization ista optimizer","code":""},{"path":"/reference/istaLSPMgSEM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"lsp optimization with ista — istaLSPMgSEM","text":"list fit results","code":""},{"path":"/reference/istaLSPMgSEM.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"lsp optimization with ista — istaLSPMgSEM","text":"new creates new object. Requires (1) vector weights parameter (2) list control elements optimize optimize model. Expects vector starting values, SEM type SEM_Cpp, theta lambda value.","code":""},{"path":"/reference/istaLSPSEM.html","id":null,"dir":"Reference","previous_headings":"","what":"lsp optimization with ista — istaLSPSEM","title":"lsp optimization with ista — istaLSPSEM","text":"Object lsp optimization ista optimizer","code":""},{"path":"/reference/istaLSPSEM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"lsp optimization with ista — istaLSPSEM","text":"list fit results","code":""},{"path":"/reference/istaLSPSEM.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"lsp optimization with ista — istaLSPSEM","text":"new creates new object. Requires (1) vector weights parameter (2) list control elements optimize optimize model. Expects vector starting values, SEM type SEM_Cpp, theta lambda value.","code":""},{"path":"/reference/istaMcpMgSEM.html","id":null,"dir":"Reference","previous_headings":"","what":"mcp optimization with ista — istaMcpMgSEM","title":"mcp optimization with ista — istaMcpMgSEM","text":"Object mcp optimization ista optimizer","code":""},{"path":"/reference/istaMcpMgSEM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"mcp optimization with ista — istaMcpMgSEM","text":"list fit results","code":""},{"path":"/reference/istaMcpMgSEM.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"mcp optimization with ista — istaMcpMgSEM","text":"new creates new object. Requires (1) vector weights parameter (2) list control elements optimize optimize model. Expects vector starting values, SEM type SEM_Cpp, theta lambda value.","code":""},{"path":"/reference/istaMcpSEM.html","id":null,"dir":"Reference","previous_headings":"","what":"mcp optimization with ista — istaMcpSEM","title":"mcp optimization with ista — istaMcpSEM","text":"Object mcp optimization ista optimizer","code":""},{"path":"/reference/istaMcpSEM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"mcp optimization with ista — istaMcpSEM","text":"list fit results","code":""},{"path":"/reference/istaMcpSEM.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"mcp optimization with ista — istaMcpSEM","text":"new creates new object. Requires (1) vector weights parameter (2) list control elements optimize optimize model. Expects vector starting values, SEM type SEM_Cpp, theta lambda value.","code":""},{"path":"/reference/istaMixedPenaltyGeneralPurpose.html","id":null,"dir":"Reference","previous_headings":"","what":"mixed penalty optimization with ista — istaMixedPenaltyGeneralPurpose","title":"mixed penalty optimization with ista — istaMixedPenaltyGeneralPurpose","text":"Object elastic net optimization ista optimizer","code":""},{"path":"/reference/istaMixedPenaltyGeneralPurpose.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"mixed penalty optimization with ista — istaMixedPenaltyGeneralPurpose","text":"list fit results","code":""},{"path":"/reference/istaMixedPenaltyGeneralPurpose.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"mixed penalty optimization with ista — istaMixedPenaltyGeneralPurpose","text":"new creates new object. optimize optimize model.","code":""},{"path":"/reference/istaMixedPenaltyGeneralPurposeCpp.html","id":null,"dir":"Reference","previous_headings":"","what":"mixed penalty optimization with ista — istaMixedPenaltyGeneralPurposeCpp","title":"mixed penalty optimization with ista — istaMixedPenaltyGeneralPurposeCpp","text":"Object elastic net optimization ista optimizer","code":""},{"path":"/reference/istaMixedPenaltyGeneralPurposeCpp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"mixed penalty optimization with ista — istaMixedPenaltyGeneralPurposeCpp","text":"list fit results","code":""},{"path":"/reference/istaMixedPenaltyGeneralPurposeCpp.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"mixed penalty optimization with ista — istaMixedPenaltyGeneralPurposeCpp","text":"new creates new object. Requires (1) vector weights parameter, (2) vector indicating penalty used, (3) list control elements optimize optimize model.","code":""},{"path":"/reference/istaMixedPenaltySEM.html","id":null,"dir":"Reference","previous_headings":"","what":"mixed penalty optimization with ista — istaMixedPenaltySEM","title":"mixed penalty optimization with ista — istaMixedPenaltySEM","text":"Object elastic net optimization ista optimizer","code":""},{"path":"/reference/istaMixedPenaltySEM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"mixed penalty optimization with ista — istaMixedPenaltySEM","text":"list fit results","code":""},{"path":"/reference/istaMixedPenaltySEM.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"mixed penalty optimization with ista — istaMixedPenaltySEM","text":"new creates new object. Requires (1) vector weights parameter, (2) vector indicating penalty used, (3) list control elements optimize optimize model. Expects vector starting values, SEM type SEM_Cpp, theta value, lambda alpha value (alpha must 1).","code":""},{"path":"/reference/istaMixedPenaltymgSEM.html","id":null,"dir":"Reference","previous_headings":"","what":"mixed penalty optimization with ista — istaMixedPenaltymgSEM","title":"mixed penalty optimization with ista — istaMixedPenaltymgSEM","text":"Object elastic net optimization ista optimizer","code":""},{"path":"/reference/istaMixedPenaltymgSEM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"mixed penalty optimization with ista — istaMixedPenaltymgSEM","text":"list fit results","code":""},{"path":"/reference/istaMixedPenaltymgSEM.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"mixed penalty optimization with ista — istaMixedPenaltymgSEM","text":"new creates new object. Requires (1) vector weights parameter, (2) vector indicating penalty used, (3) list control elements optimize optimize model. Expects vector starting values, SEM type SEM_Cpp, theta value, lambda alpha value (alpha must 1).","code":""},{"path":"/reference/istaScadMgSEM.html","id":null,"dir":"Reference","previous_headings":"","what":"scad optimization with ista — istaScadMgSEM","title":"scad optimization with ista — istaScadMgSEM","text":"Object scad optimization ista optimizer","code":""},{"path":"/reference/istaScadMgSEM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"scad optimization with ista — istaScadMgSEM","text":"list fit results","code":""},{"path":"/reference/istaScadMgSEM.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"scad optimization with ista — istaScadMgSEM","text":"new creates new object. Requires (1) vector weights parameter (2) list control elements optimize optimize model. Expects vector starting values, SEM type SEM_Cpp, theta lambda value.","code":""},{"path":"/reference/istaScadSEM.html","id":null,"dir":"Reference","previous_headings":"","what":"scad optimization with ista — istaScadSEM","title":"scad optimization with ista — istaScadSEM","text":"Object scad optimization ista optimizer","code":""},{"path":"/reference/istaScadSEM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"scad optimization with ista — istaScadSEM","text":"list fit results","code":""},{"path":"/reference/istaScadSEM.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"scad optimization with ista — istaScadSEM","text":"new creates new object. Requires (1) vector weights parameter (2) list control elements optimize optimize model. Expects vector starting values, SEM type SEM_Cpp, theta lambda value.","code":""},{"path":"/reference/lasso.html","id":null,"dir":"Reference","previous_headings":"","what":"lasso — lasso","title":"lasso — lasso","text":"Implements lasso regularization structural equation models. penalty function given : $$p( x_j) = \\lambda |x_j|$$ Lasso regularization set parameters zero \\(\\lambda\\) large enough","code":""},{"path":"/reference/lasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"lasso — lasso","text":"","code":"lasso(   lavaanModel,   regularized,   lambdas = NULL,   nLambdas = NULL,   reverse = TRUE,   curve = 1,   method = \"glmnet\",   modifyModel = lessSEM::modifyModel(),   control = lessSEM::controlGlmnet() )"},{"path":"/reference/lasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"lasso — lasso","text":"lavaanModel model class lavaan regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda nLambdas alternative lambda: alpha = 1, lessSEM can automatically compute first lambda value sets regularized parameters zero. generate nLambda values 0 computed lambda. reverse set TRUE nLambdas used, lessSEM start largest lambda gradually decrease lambda. Otherwise, lessSEM start smallest lambda gradually increase . curve Allows unequally spaced lambda steps (e.g., .01,.02,.05,1,5,20). curve close 1 lambda values equally spaced, curve large lambda values concentrated close 0. See ?lessSEM::curveLambda information. method optimizer used? Currently implemented ista glmnet. ista, control argument can used switch related procedures (currently gist). modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. element generated controlIsta controlGlmnet functions. See ?controlIsta ?controlGlmnet details.","code":""},{"path":"/reference/lasso.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"lasso — lasso","text":"Model class regularizedSEM","code":""},{"path":"/reference/lasso.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"lasso — lasso","text":"Identical regsem, models specified using lavaan. Currently, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. Lasso regularization: Tibshirani, R. (1996). Regression shrinkage selection via lasso. Journal Royal Statistical Society. Series B (Methodological), 58(1), 267–288. Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classification. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/lasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"lasso — lasso","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +       l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +       l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Regularization:  lsem <- lasso(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   # names of the regularized parameters:   regularized = paste0(\"l\", 6:15),   # in case of lasso and adaptive lasso, we can specify the number of lambda   # values to use. lessSEM will automatically find lambda_max and fit   # models for nLambda values between 0 and lambda_max. For the other   # penalty functions, lambdas must be specified explicitly   nLambdas = 50)  # use the plot-function to plot the regularized parameters: plot(lsem)  # the coefficients can be accessed with: coef(lsem) # if you are only interested in the estimates and not the tuning parameters, use coef(lsem)@estimates # or estimates(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters[1,]  # fit Measures: fitIndices(lsem)  # The best parameters can also be extracted with: coef(lsem, criterion = \"AIC\") # or estimates(lsem, criterion = \"AIC\")   #### Advanced ### # Switching the optimizer #  # Use the \"method\" argument to switch the optimizer. The control argument # must also be changed to the corresponding function: lsemIsta <- lasso(   lavaanModel = lavaanModel,   regularized = paste0(\"l\", 6:15),   nLambdas = 50,   method = \"ista\",   control = controlIsta())  # Note: The results are basically identical: lsemIsta@parameters - lsem@parameters"},{"path":"/reference/lavaan2lslxLabels.html","id":null,"dir":"Reference","previous_headings":"","what":"lavaan2lslxLabels — lavaan2lslxLabels","title":"lavaan2lslxLabels — lavaan2lslxLabels","text":"helper function: lslx lavaan use slightly different parameter labels. function can used get sets labels.","code":""},{"path":"/reference/lavaan2lslxLabels.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"lavaan2lslxLabels — lavaan2lslxLabels","text":"","code":"lavaan2lslxLabels(lavaanModel)"},{"path":"/reference/lavaan2lslxLabels.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"lavaan2lslxLabels — lavaan2lslxLabels","text":"lavaanModel model class lavaan","code":""},{"path":"/reference/lavaan2lslxLabels.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"lavaan2lslxLabels — lavaan2lslxLabels","text":"list lavaan labels lslx labels","code":""},{"path":"/reference/lavaan2lslxLabels.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"lavaan2lslxLabels — lavaan2lslxLabels","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +       l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +       l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  lavaan2lslxLabels(lavaanModel)"},{"path":"/reference/lessSEM.html","id":null,"dir":"Reference","previous_headings":"","what":"lessSEM — lessSEM","title":"lessSEM — lessSEM","text":"Please see vignettes readme GitHub date description package","code":""},{"path":"/reference/lessSEM.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"lessSEM — lessSEM","text":"lessSEM (lessSEM estimates sparse SEM) R package regularized structural equation modeling (regularized SEM) non-smooth penalty functions (e.g., lasso) building lavaan. lessSEM heavily inspired regsem package lslx packages similar functionality. use lessSEM, please also cite regsem lslx! objectives lessSEM provide ... flexible framework regularizing SEM optimizers SEM packages can used interface similar optim. Important: Please also check implementations regularized SEM mature R packages regsem lslx. Finally, may want check julia package StructuralEquationModels.jl.","code":""},{"path":"/reference/lessSEM.html","id":"regsem-lslx-and-lesssem","dir":"Reference","previous_headings":"","what":"regsem, lslx, and lessSEM","title":"lessSEM — lessSEM","text":"packages regsem, lslx, lessSEM can used regularize basic SEM. fact, outlined , lessSEM heavily inspired regsem lslx. However, packages differ targets: objective lessSEM replace major packages regsem lslx. Instead, objective provide method developers flexible framework regularized SEM. following shows incomplete comparison features implemented three packages: lessSEM fairly new, currently recommend using lslx cases covered , lessSEM lslx.","code":""},{"path":"/reference/lessSEM.html","id":"introduction","dir":"Reference","previous_headings":"","what":"Introduction","title":"lessSEM — lessSEM","text":"find short introduction regularized SEM lessSEM package vignette('lessSEM', package = 'lessSEM'). information also provided documentation individual functions (e.g., see ?lessSEM::scad). Finally, find templates selection models can used lessSEM (e.g., cross-lagged panel model) package lessTemplates.","code":""},{"path":"/reference/lessSEM.html","id":"example","dir":"Reference","previous_headings":"","what":"Example","title":"lessSEM — lessSEM","text":"","code":"library(lessSEM) library(lavaan)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \"       f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +             l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +             l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15       f ~~ 1*f       \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Regularization:  lsem <- lasso(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   # names of the regularized parameters:   regularized = c(\"l6\", \"l7\", \"l8\", \"l9\", \"l10\",                   \"l11\", \"l12\", \"l13\", \"l14\", \"l15\"),   # in case of lasso and adaptive lasso, we can specify the number of lambda   # values to use. lessSEM will automatically find lambda_max and fit   # models for nLambda values between 0 and lambda_max. For the other   # penalty functions, lambdas must be specified explicitly   nLambdas = 50)  # use the plot-function to plot the regularized parameters: plot(lsem)  # use the coef-function to show the estimates coef(lsem)  # The best parameters can be extracted with: coef(lsem, criterion = \"AIC\") coef(lsem, criterion = \"BIC\")  # elements of lsem can be accessed with the @ operator: lsem@parameters[1,]  # AIC and BIC for all tuning parameter configurations: AIC(lsem) BIC(lsem)  # cross-validation cv <- cvLasso(lavaanModel = lavaanModel,               regularized = c(\"l6\", \"l7\", \"l8\", \"l9\", \"l10\",                               \"l11\", \"l12\", \"l13\", \"l14\", \"l15\"),               lambdas = seq(0,1,.1),               standardize = TRUE)  # get best model according to cross-validation: coef(cv)  #### Advanced ### # Switching the optimizer #  # Use the \"method\" argument to switch the optimizer. The control argument # must also be changed to the corresponding function: lsemIsta <- lasso(   lavaanModel = lavaanModel,   regularized = paste0(\"l\", 6:15),   nLambdas = 50,   method = \"ista\",   control = controlIsta(     # Here, we can also specify that we want to use multiple cores:     nCores = 2))  # Note: The results are basically identical: lsemIsta@parameters - lsem@parameters"},{"path":"/reference/lessSEM.html","id":"transformations","dir":"Reference","previous_headings":"","what":"Transformations","title":"lessSEM — lessSEM","text":"lessSEM allows parameter transformations , instance, used test measurement invariance longitudinal models (e.g., Liang, 2018; Bauer et al., 2020). thorough introduction provided vignette('Parameter-transformations', package = 'lessSEM'). example, test measurement invariance PoliticalDemocracy data set.   Finally, can extract best parameters:   differences (delta_a2, delta_b2, delta_c2) zeroed, can assume measurement invariance.","code":"library(lessSEM) library(lavaan) # we will use the PoliticalDemocracy from lavaan (see ?lavaan::sem) model <- '    # latent variable definitions      ind60 =~ x1 + x2 + x3      # assuming different loadings for different time points:      dem60 =~ y1 + a1*y2 + b1*y3 + c1*y4      dem65 =~ y5 + a2*y6 + b2*y7 + c2*y8    # regressions     dem60 ~ ind60     dem65 ~ ind60 + dem60    # residual correlations     y1 ~~ y5     y2 ~~ y4 + y6     y3 ~~ y7     y4 ~~ y8     y6 ~~ y8 '  fit <- sem(model, data = PoliticalDemocracy)  # We will define a transformation which regularizes differences # between loadings over time:  transformations <- \" // which parameters do we want to use? parameters: a1, a2, b1, b2, c1, c2, delta_a2, delta_b2, delta_c2  // transformations: a2 = a1 + delta_a2; b2 = b1 + delta_b2; c2 = c1 + delta_c2; \"  # setting delta_a2, delta_b2, or delta_c2 to zero implies measurement invariance # for the respective parameters (a1, b1, c1) lassoFit <- lasso(lavaanModel = fit,                    # we want to regularize the differences between the parameters                   regularized = c(\"delta_a2\", \"delta_b2\", \"delta_c2\"),                   nLambdas = 100,                   # Our model modification must make use of the modifyModel - function:                   modifyModel = modifyModel(transformations = transformations) ) coef(lassoFit, criterion = \"BIC\")"},{"path":"/reference/lessSEM.html","id":"experimental-features","dir":"Reference","previous_headings":"","what":"Experimental Features","title":"lessSEM — lessSEM","text":"following features relatively new may still experience bugs. Please aware using features.","code":""},{"path":"/reference/lessSEM.html","id":"from-lesssem-to-lavaan","dir":"Reference","previous_headings":"","what":"From lessSEM to lavaan","title":"lessSEM — lessSEM","text":"lessSEM supports exporting specific models lavaan. can useful plotting final model. case, best model given :   can get lavaan model parameters corresponding regularized model lambda = lambdaBest follows:   result can plotted , instance, semPlot:","code":"lambdaBest <- coef(lsem, criterion = \"BIC\")@tuningParameters$lambda lavaanModel <- lessSEM2Lavaan(regularizedSEM = lsem,                                lambda = lambdaBest) library(semPlot) semPaths(lavaanModel,          what = \"est\",          fade = FALSE)"},{"path":"/reference/lessSEM.html","id":"multi-group-models-and-definition-variables","dir":"Reference","previous_headings":"","what":"Multi-Group Models and Definition Variables","title":"lessSEM — lessSEM","text":"lessSEM supports multi-group SEM , degree, definition variables. Regularized multi-group SEM proposed Huang (2018) implemented lslx (Huang, 2020). , differences groups regularized. detailed introduction can found vignette(topic = \"Definition-Variables--Multi-Group-SEM\", package = \"lessSEM\"). Therein also explained multi-group SEM can used implement definition variables (e.g., latent growth curve models).","code":""},{"path":"/reference/lessSEM.html","id":"mixed-penalties","dir":"Reference","previous_headings":"","what":"Mixed Penalties","title":"lessSEM — lessSEM","text":"lessSEM allows defining different penalties different parts model. feature new experimental. Please keep mind using procedure. detailed introduction can found vignette(topic = \"Mixed-Penalties\", package = \"lessSEM\"). provide short example, regularize loadings regression parameters Political Democracy data set different penalties. following script adapted ?lavaan::sem.   best model according BIC can extracted :","code":"model <- '    # latent variable definitions      ind60 =~ x1 + x2 + x3 + c2*y2 + c3*y3 + c4*y4      dem60 =~ y1 + y2 + y3 + y4      dem65 =~ y5 + y6 + y7 + c*y8    # regressions     dem60 ~ r1*ind60     dem65 ~ r2*ind60 + r3*dem60 '  lavaanModel <- sem(model,                    data = PoliticalDemocracy)  # Let's add a lasso penalty on the cross-loadings c2 - c4 and  # scad penalty on the regressions r1-r3 fitMp <- lavaanModel |>   mixedPenalty() |>   addLasso(regularized = c(\"c2\", \"c3\", \"c4\"),             lambdas = seq(0,1,.1)) |>   addScad(regularized = c(\"r1\", \"r2\", \"r3\"),            lambdas = seq(0,1,.2),           thetas = 3.7) |>   fit() coef(fitMp, criterion = \"BIC\")"},{"path":"/reference/lessSEM.html","id":"optimizers","dir":"Reference","previous_headings":"","what":"Optimizers","title":"lessSEM — lessSEM","text":"Currently, lessSEM following optimizers: (variants ) iterative shrinkage thresholding (e.g., Beck & Teboulle, 2009; Gong et al., 2013; Parikh & Boyd, 2013); optimization cappedL1, lsp, scad, mcp based Gong et al. (2013) glmnet (Friedman et al., 2010; Yuan et al., 2012; Huang, 2020) optimizers implemented based regCtsem package. importantly, optimizers lessSEM available packages. three ways implement documented vignette(\"General-Purpose-Optimization\", package = \"lessSEM\"). short, : using R interface: general purpose implementations functions called prefix \"gp\" (gpLasso, gpScad, ...). information examples can found documentation functions (e.g., ?lessSEM::gpLasso, ?lessSEM::gpAdaptiveLasso, ?lessSEM::gpElasticNet). interface similar optim optimizers R. using Rcpp, can pass C++ function pointers general purpose optimizers gpLassoCpp, gpScadCpp, ... (e.g., ?lessSEM::gpLassoCpp) optimizers implemented C++ header-files lessSEM. Thus, can accessed packages using C++. interface similar ensmallen library. implemented simple example elastic net regularization linear regressions lessLM package. can also find details general design optimizer interface vignette(\"-optimizer-interface\", package = \"lessSEM\").","code":""},{"path":[]},{"path":"/reference/lessSEM.html","id":"r-packages-software","dir":"Reference","previous_headings":"","what":"R - Packages / Software","title":"lessSEM — lessSEM","text":"lavaan Rosseel, Y. (2012). lavaan: R Package Structural Equation Modeling. Journal Statistical Software, 48(2), 1-36. https://doi.org/10.18637/jss.v048.i02 regsem: Jacobucci, R. (2017). regsem: Regularized Structural Equation Modeling. ArXiv:1703.08489 Stat. https://arxiv.org/abs/1703.08489 lslx: Huang, P.-H. (2020). lslx: Semi-confirmatory structural equation modeling via penalized likelihood. Journal Statistical Software, 93(7). https://doi.org/10.18637/jss.v093.i07 fasta: Another implementation fista algorithm (Beck & Teboulle, 2009). ensmallen: Curtin, R. R., Edel, M., Prabhu, R. G., Basak, S., Lou, Z., & Sanderson, C. (2021). ensmallen library ﬂexible numerical optimization. Journal Machine Learning Research, 22, 1–6. regCtsem: Orzek, J. H., & Voelkle, M. C. (press). Regularized continuous time structural equation models: network perspective. Psychological Methods.","code":""},{"path":"/reference/lessSEM.html","id":"regularized-structural-equation-modeling","dir":"Reference","previous_headings":"","what":"Regularized Structural Equation Modeling","title":"lessSEM — lessSEM","text":"Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Huang, P.-H. (2018). penalized likelihood method multi-group structural equation modelling. British Journal Mathematical Statistical Psychology, 71(3), 499–522. https://doi.org/10.1111/bmsp.12130 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793","code":""},{"path":"/reference/lessSEM.html","id":"penalty-functions","dir":"Reference","previous_headings":"","what":"Penalty Functions","title":"lessSEM — lessSEM","text":"Candès, E. J., Wakin, M. B., & Boyd, S. P. (2008). Enhancing Sparsity Reweighted l1 Minimization. Journal Fourier Analysis Applications, 14(5–6), 877–905. https://doi.org/10.1007/s00041-008-9045-x Fan, J., & Li, R. (2001). Variable selection via nonconcave penalized likelihood oracle properties. Journal American Statistical Association, 96(456), 1348–1360. https://doi.org/10.1198/016214501753382273 Hoerl, . E., & Kennard, R. W. (1970). Ridge Regression: Biased Estimation Nonorthogonal Problems. Technometrics, 12(1), 55–67. https://doi.org/10.1080/00401706.1970.10488634 Tibshirani, R. (1996). Regression shrinkage selection via lasso. Journal Royal Statistical Society. Series B (Methodological), 58(1), 267–288. Zhang, C.-H. (2010). Nearly unbiased variable selection minimax concave penalty. Annals Statistics, 38(2), 894–942. https://doi.org/10.1214/09-AOS729 Zhang, T. (2010). Analysis Multi-stage Convex Relaxation Sparse Regularization. Journal Machine Learning Research, 11, 1081–1107. Zou, H. (2006). adaptive lasso oracle properties. Journal American Statistical Association, 101(476), 1418–1429. https://doi.org/10.1198/016214506000000735 Zou, H., & Hastie, T. (2005). Regularization variable selection via elastic net. Journal Royal Statistical Society: Series B, 67(2), 301–320. https://doi.org/10.1111/j.1467-9868.2005.00503.x","code":""},{"path":[]},{"path":"/reference/lessSEM.html","id":"glmnet","dir":"Reference","previous_headings":"","what":"GLMNET","title":"lessSEM — lessSEM","text":"Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization paths generalized linear models via coordinate descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421","code":""},{"path":"/reference/lessSEM.html","id":"variants-of-ista","dir":"Reference","previous_headings":"","what":"Variants of ISTA","title":"lessSEM — lessSEM","text":"Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). general iterative shrinkage thresholding algorithm non-convex regularized optimization problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/lessSEM.html","id":"miscellaneous","dir":"Reference","previous_headings":"","what":"Miscellaneous","title":"lessSEM — lessSEM","text":"Liang, X., Yang, Y., & Huang, J. (2018). Evaluation structural relationships autoregressive cross-lagged models longitudinal approximate invariance: Bayesian analysis. Structural Equation Modeling: Multidisciplinary Journal, 25(4), 558–572. https://doi.org/10.1080/10705511.2017.1410706 Bauer, D. J., Belzak, W. C. M., & Cole, V. T. (2020). Simplifying Assessment Measurement Invariance Multiple Background Variables: Using Regularized Moderated Nonlinear Factor Analysis Detect Differential Item Functioning. Structural Equation Modeling: Multidisciplinary Journal, 27(1), 43–55. https://doi.org/10.1080/10705511.2019.1642754","code":""},{"path":"/reference/lessSEM.html","id":"important-notes","dir":"Reference","previous_headings":"","what":"Important Notes","title":"lessSEM — lessSEM","text":"SOFTWARE PROVIDED '', WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":[]},{"path":"/reference/lessSEM.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"lessSEM — lessSEM","text":"Jannik Orzek orzek@mpib-berlin.mpg.de","code":""},{"path":"/reference/lessSEM2Lavaan.html","id":null,"dir":"Reference","previous_headings":"","what":"lessSEM2Lavaan — lessSEM2Lavaan","title":"lessSEM2Lavaan — lessSEM2Lavaan","text":"Creates lavaan model object lessSEM (possible). Pass either criterion combination lambda, alpha, theta.","code":""},{"path":"/reference/lessSEM2Lavaan.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"lessSEM2Lavaan — lessSEM2Lavaan","text":"","code":"lessSEM2Lavaan(   regularizedSEM,   criterion = NULL,   lambda = NULL,   alpha = NULL,   theta = NULL )"},{"path":"/reference/lessSEM2Lavaan.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"lessSEM2Lavaan — lessSEM2Lavaan","text":"regularizedSEM object created lessSEM criterion criterion used model selection. Currently supported \"AIC\" \"BIC\" lambda value tuning parameter lambda alpha value tuning parameter alpha theta value tuning parameter theta","code":""},{"path":"/reference/lessSEM2Lavaan.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"lessSEM2Lavaan — lessSEM2Lavaan","text":"lavaan model","code":""},{"path":"/reference/lessSEM2Lavaan.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"lessSEM2Lavaan — lessSEM2Lavaan","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +       l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +       l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Regularization: regularized <- lasso(lavaanModel,                      regularized = paste0(\"l\", 11:15),                       lambdas = seq(0,1,.1))  # using criterion lessSEM2Lavaan(regularizedSEM = regularized,                 criterion = \"AIC\")                 # using tuning parameters (note: we only have to specify the tuning # parameters that are actually used by the penalty function. In case # of lasso, this is lambda): lessSEM2Lavaan(regularizedSEM = regularized,                 lambda = 1)"},{"path":"/reference/lessSEMCoef-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Class for the coefficients estimated by lessSEM. — lessSEMCoef-class","title":"Class for the coefficients estimated by lessSEM. — lessSEMCoef-class","text":"Class coefficients estimated lessSEM.","code":""},{"path":"/reference/lessSEMCoef-class.html","id":"slots","dir":"Reference","previous_headings":"","what":"Slots","title":"Class for the coefficients estimated by lessSEM. — lessSEMCoef-class","text":"tuningParameters tuning parameters estimates parameter estimates transformations transformations parameters","code":""},{"path":"/reference/loadings.html","id":null,"dir":"Reference","previous_headings":"","what":"loadings — loadings","title":"loadings — loadings","text":"Extract labels loadings found lavaan model.","code":""},{"path":"/reference/loadings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"loadings — loadings","text":"","code":"loadings(lavaanModel)"},{"path":"/reference/loadings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"loadings — loadings","text":"lavaanModel fitted lavaan model","code":""},{"path":"/reference/loadings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"loadings — loadings","text":"vector parameter labels","code":""},{"path":"/reference/loadings.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"loadings — loadings","text":"","code":"# The following is adapted from ?lavaan::sem library(lessSEM) model <- '    # latent variable definitions   ind60 =~ x1 + x2 + x3   dem60 =~ y1 + a*y2 + b*y3 + c*y4   dem65 =~ y5 + a*y6 + b*y7 + c*y8    # regressions   dem60 ~ ind60   dem65 ~ ind60 + dem60    # residual correlations   y1 ~~ y5   y2 ~~ y4 + y6   y3 ~~ y7   y4 ~~ y8   y6 ~~ y8 '  fit <- sem(model, data = PoliticalDemocracy)  loadings(fit)"},{"path":"/reference/logLik-Rcpp_SEMCpp-method.html","id":null,"dir":"Reference","previous_headings":"","what":"logLik — logLik,Rcpp_SEMCpp-method","title":"logLik — logLik,Rcpp_SEMCpp-method","text":"logLik","code":""},{"path":"/reference/logLik-Rcpp_SEMCpp-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"logLik — logLik,Rcpp_SEMCpp-method","text":"","code":"# S4 method for Rcpp_SEMCpp logLik(object, ...)"},{"path":"/reference/logLik-Rcpp_SEMCpp-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"logLik — logLik,Rcpp_SEMCpp-method","text":"object object class Rcpp_SEMCpp ... used","code":""},{"path":"/reference/logLik-Rcpp_SEMCpp-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"logLik — logLik,Rcpp_SEMCpp-method","text":"log-likelihood model","code":""},{"path":"/reference/logLik-Rcpp_mgSEM-method.html","id":null,"dir":"Reference","previous_headings":"","what":"logLik — logLik,Rcpp_mgSEM-method","title":"logLik — logLik,Rcpp_mgSEM-method","text":"logLik","code":""},{"path":"/reference/logLik-Rcpp_mgSEM-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"logLik — logLik,Rcpp_mgSEM-method","text":"","code":"# S4 method for Rcpp_mgSEM logLik(object, ...)"},{"path":"/reference/logLik-Rcpp_mgSEM-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"logLik — logLik,Rcpp_mgSEM-method","text":"object object class Rcpp_mgSEM ... used","code":""},{"path":"/reference/logLik-Rcpp_mgSEM-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"logLik — logLik,Rcpp_mgSEM-method","text":"log-likelihood model","code":""},{"path":"/reference/logLikelihood-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Class for log-likelihood of regularized SEM. Note: we define a custom logLik -\nFunction because the generic one is using df = number of parameters which might be confusing. — logLikelihood-class","title":"Class for log-likelihood of regularized SEM. Note: we define a custom logLik -\nFunction because the generic one is using df = number of parameters which might be confusing. — logLikelihood-class","text":"Class log-likelihood regularized SEM. Note: define custom logLik - Function generic one using df = number parameters might confusing.","code":""},{"path":"/reference/logLikelihood-class.html","id":"slots","dir":"Reference","previous_headings":"","what":"Slots","title":"Class for log-likelihood of regularized SEM. Note: we define a custom logLik -\nFunction because the generic one is using df = number of parameters which might be confusing. — logLikelihood-class","text":"logLik log-Likelihood nParameters number parameters model N number persons data set","code":""},{"path":"/reference/logicalMatch.html","id":null,"dir":"Reference","previous_headings":"","what":"logicalMatch — logicalMatch","title":"logicalMatch — logicalMatch","text":"Returns rows elements boolean matrix X equal elements boolean vector x","code":""},{"path":"/reference/logicalMatch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"logicalMatch — logicalMatch","text":"","code":"logicalMatch(X, x)"},{"path":"/reference/logicalMatch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"logicalMatch — logicalMatch","text":"X matrix booleans x vector booleans","code":""},{"path":"/reference/logicalMatch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"logicalMatch — logicalMatch","text":"numerical vector indices matching rows","code":""},{"path":"/reference/lsp.html","id":null,"dir":"Reference","previous_headings":"","what":"lsp — lsp","title":"lsp — lsp","text":"Implements lsp regularization structural equation models. penalty function given : $$p( x_j) = \\lambda \\log(1 + |x_j|/\\theta)$$ \\(\\theta > 0\\).","code":""},{"path":"/reference/lsp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"lsp — lsp","text":"","code":"lsp(   lavaanModel,   regularized,   lambdas,   thetas,   modifyModel = lessSEM::modifyModel(),   method = \"glmnet\",   control = lessSEM::controlGlmnet() )"},{"path":"/reference/lsp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"lsp — lsp","text":"lavaanModel model class lavaan regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda thetas parameters whose absolute value threshold penalized constant (theta) modifyModel used modify lavaanModel. See ?modifyModel. method optimizer used? Currently implemented ista glmnet. ista, control argument can used switch related procedures control used control optimizer. element generated controlIsta (see ?controlIsta)","code":""},{"path":"/reference/lsp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"lsp — lsp","text":"Model class regularizedSEM","code":""},{"path":"/reference/lsp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"lsp — lsp","text":"Identical regsem, models specified using lavaan. Currently, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. lsp regularization: Candès, E. J., Wakin, M. B., & Boyd, S. P. (2008). Enhancing Sparsity Reweighted l1 Minimization. Journal Fourier Analysis Applications, 14(5–6), 877–905. https://doi.org/10.1007/s00041-008-9045-x Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classification. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/lsp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"lsp — lsp","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +      l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +      l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Regularization:  lsem <- lsp(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   # names of the regularized parameters:   regularized = paste0(\"l\", 6:15),   lambdas = seq(0,1,length.out = 20),   thetas = seq(0.01,2,length.out = 5))  # the coefficients can be accessed with: coef(lsem) # if you are only interested in the estimates and not the tuning parameters, use coef(lsem)@estimates # or estimates(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters[1,]  # fit Measures: fitIndices(lsem)  # The best parameters can also be extracted with: coef(lsem, criterion = \"AIC\") # or estimates(lsem, criterion = \"AIC\")  # optional: plotting the paths requires installation of plotly # plot(lsem)"},{"path":"/reference/makePtrs.html","id":null,"dir":"Reference","previous_headings":"","what":"makePtrs — makePtrs","title":"makePtrs — makePtrs","text":"function helps create pointers necessary use Cpp interface","code":""},{"path":"/reference/makePtrs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"makePtrs — makePtrs","text":"","code":"makePtrs(fitFunName, gradFunName)"},{"path":"/reference/makePtrs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"makePtrs — makePtrs","text":"fitFunName name C++ fit function (IMPORTANT: must name used C++) gradFunName name C++ gradient function (IMPORTANT: must name used C++)","code":""},{"path":"/reference/makePtrs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"makePtrs — makePtrs","text":"string can copied C++ function create pointers.","code":""},{"path":"/reference/makePtrs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"makePtrs — makePtrs","text":"","code":"# see vignette(\"General-Purpose-Optimization\", package = \"lessSEM\") for an example"},{"path":"/reference/mcp.html","id":null,"dir":"Reference","previous_headings":"","what":"mcp — mcp","title":"mcp — mcp","text":"Implements mcp regularization structural equation models. penalty function given : $$p( x_j) = \\begin{cases} \\lambda |x_j| - x_j^2/(2\\theta) & \\text{} |x_j| \\leq \\theta\\lambda\\\\ \\theta\\lambda^2/2 & \\text{} |x_j| > \\lambda\\theta \\end{cases}$$ \\(\\theta > 1\\).","code":""},{"path":"/reference/mcp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"mcp — mcp","text":"","code":"mcp(   lavaanModel,   regularized,   lambdas,   thetas,   modifyModel = lessSEM::modifyModel(),   method = \"ista\",   control = lessSEM::controlIsta() )"},{"path":"/reference/mcp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"mcp — mcp","text":"lavaanModel model class lavaan regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda thetas parameters whose absolute value threshold penalized constant (theta) modifyModel used modify lavaanModel. See ?modifyModel. method optimizer used? Currently implemented ista glmnet. ista, control argument can used switch related procedures (currently gist). control used control optimizer. element generated controlIsta (see ?controlIsta)","code":""},{"path":"/reference/mcp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"mcp — mcp","text":"Model class regularizedSEM","code":""},{"path":"/reference/mcp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"mcp — mcp","text":"Identical regsem, models specified using lavaan. Currently, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. experience, glmnet optimizer can run issues mcp penalty. Therefor, default using ista. mcp regularization: Zhang, C.-H. (2010). Nearly unbiased variable selection minimax concave penalty. Annals Statistics, 38(2), 894–942. https://doi.org/10.1214/09-AOS729 Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classification. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/mcp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"mcp — mcp","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +      l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +      l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Regularization:  lsem <- mcp(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   # names of the regularized parameters:   regularized = paste0(\"l\", 6:15),   lambdas = seq(0,1,length.out = 20),   thetas = seq(0.01,2,length.out = 5))  # the coefficients can be accessed with: coef(lsem)  # if you are only interested in the estimates and not the tuning parameters, use coef(lsem)@estimates # or estimates(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters[1,]  # fit Measures: fitIndices(lsem)  # The best parameters can also be extracted with: coef(lsem, criterion = \"AIC\") # or estimates(lsem, criterion = \"AIC\")  # optional: plotting the paths requires installation of plotly # plot(lsem)"},{"path":"/reference/mcpPenalty_C.html","id":null,"dir":"Reference","previous_headings":"","what":"mcpPenalty_C — mcpPenalty_C","title":"mcpPenalty_C — mcpPenalty_C","text":"mcpPenalty_C","code":""},{"path":"/reference/mcpPenalty_C.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"mcpPenalty_C — mcpPenalty_C","text":"","code":"mcpPenalty_C(par, lambda_p, theta)"},{"path":"/reference/mcpPenalty_C.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"mcpPenalty_C — mcpPenalty_C","text":"par single parameter value lambda_p lambda value parameter theta theta value parameter","code":""},{"path":"/reference/mcpPenalty_C.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"mcpPenalty_C — mcpPenalty_C","text":"penalty value","code":""},{"path":"/reference/mgSEM.html","id":null,"dir":"Reference","previous_headings":"","what":"mgSEM class — mgSEM","title":"mgSEM class — mgSEM","text":"internal mgSEM representation","code":""},{"path":"/reference/mgSEM.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"mgSEM class — mgSEM","text":"new Creates new mgSEM. addModel add model. Expects Rcpp::List addTransformation adds transforamtions model implied Computes implied means covariance matrix fit Fits model. Returns objective value fitting function getParameters Returns data frame model parameters. getParameterLabels Returns vector unique parameter labels used internally. getEstimator Returns vector names estimators used submodels. getGradients Returns matrix scores. getScores Returns matrix scores. yet implemented getHessian Returns hessian model. Expects labels parameters values parameters well boolean indicating raw. Finally, double (eps) controls precision approximation. computeTransformations compute transformations. setTransformationGradientStepSize change step size gradient computation transformations","code":""},{"path":"/reference/mixedPenalty.html","id":null,"dir":"Reference","previous_headings":"","what":"mixedPenalty — mixedPenalty","title":"mixedPenalty — mixedPenalty","text":"Provides possibility impose different penalties different parameters.","code":""},{"path":"/reference/mixedPenalty.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"mixedPenalty — mixedPenalty","text":"","code":"mixedPenalty(   lavaanModel,   modifyModel = lessSEM::modifyModel(),   method = \"glmnet\",   control = lessSEM::controlGlmnet() )"},{"path":"/reference/mixedPenalty.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"mixedPenalty — mixedPenalty","text":"lavaanModel model class lavaan modifyModel used modify lavaanModel. See ?modifyModel. method optimizer used? Currently supported \"glmnet\" \"ista\". control used control optimizer. element generated controlIsta controlGlmnet functions. See ?controlIsta ?controlGlmnet details.","code":""},{"path":"/reference/mixedPenalty.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"mixedPenalty — mixedPenalty","text":"Model class regularizedSEM","code":""},{"path":"/reference/mixedPenalty.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"mixedPenalty — mixedPenalty","text":"mixedPenalty function allows add multiple penalties single model. instance, may want regularize loadings regressions SEM. case, using penalty (e.g., lasso) types penalties may actually want use penalty function sensitive scales parameters. Instead, may want use two separate lasso penalties loadings regressions. Similarly, separate penalties different parameters , instance, proposed multi-group models (Geminiani et al., 2021). Identical regsem, models specified using lavaan. Currently, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. Models fitted glmnet ista optimizer. Note optimizers differ penalties support. following table provides overview: default, glmnet used. Note elastic net penalty can combined elastic net penalties. Check vignette(topic = \"Mixed-Penalties\", package = \"lessSEM\") details. Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Geminiani, E., Marra, G., & Moustaki, . (2021). Single- multiple-group penalized factor analysis: trust-region algorithm approach integrated automatic multiple tuning parameter selection. Psychometrika, 86(1), 65–95. https://doi.org/10.1007/s11336-021-09751-8 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/mixedPenalty.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"mixedPenalty — mixedPenalty","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +       l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +       l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Regularization:  # In this example, we want to regularize the loadings l6-l10  # independently of the loadings l11-15. This could, for instance, # reflect that the items y6-y10 and y11-y15 may belong to different # subscales.   regularized <- lavaanModel |>   # create template for regularized model with mixed penalty:   mixedPenalty() |>   # add lasso penalty on loadings l6 - l10:   addLasso(regularized = paste0(\"l\", 6:10),             lambdas = seq(0,1,length.out = 4)) |>   # add scad penalty on loadings l11 - l15:   addScad(regularized = paste0(\"l\", 11:15),            lambdas = seq(0,1,length.out = 3),           thetas = 3.1) |>   # fit the model:   fit()  # elements of regularized can be accessed with the @ operator: regularized@parameters[1,]  # AIC and BIC: AIC(regularized) BIC(regularized)  # The best parameters can also be extracted with: coef(regularized, criterion = \"AIC\") coef(regularized, criterion = \"BIC\")  # The tuningParameterConfiguration corresponds to the rows # in the lambda, theta, and alpha matrices in regularized@tuningParamterConfigurations. # Configuration 3, for example, is given by regularized@tuningParameterConfigurations$lambda[3,] regularized@tuningParameterConfigurations$theta[3,] regularized@tuningParameterConfigurations$alpha[3,]  # Note that lambda, theta, and alpha may correspond to tuning parameters # of different penalties for different parameters (e.g., lambda for l6 is the lambda # of the lasso penalty, while lambda for l12 is the lambda of the scad penalty)."},{"path":"/reference/modifyModel.html","id":null,"dir":"Reference","previous_headings":"","what":"modifyModel — modifyModel","title":"modifyModel — modifyModel","text":"Modify model lavaan fit needs","code":""},{"path":"/reference/modifyModel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"modifyModel — modifyModel","text":"","code":"modifyModel(   addMeans = FALSE,   activeSet = NULL,   dataSet = NULL,   transformations = NULL,   transformationList = list(),   transformationGradientStepSize = 1e-06 )"},{"path":"/reference/modifyModel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"modifyModel — modifyModel","text":"addMeans lavaanModel meanstructure = FALSE, addMeans = TRUE add mean structure. FALSE set means observed variables observed means. activeSet Option use subset individuals data set. Logical vector length N indicating subjects remain sample. dataSet option replace data set lavaan model different data set. Can useful cross-validation transformations allows transformations parameters - useful measurement invariance tests etc. transformationList optional list used within transformations. NOTE: must used Rcpp::List. transformationGradientStepSize step size used compute gradients transformations","code":""},{"path":"/reference/modifyModel.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"modifyModel — modifyModel","text":"Object class modifyModel","code":""},{"path":"/reference/modifyModel.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"modifyModel — modifyModel","text":"","code":"modification <- modifyModel(addMeans = TRUE) # adds intercepts to a lavaan object # that was fitted without explicit intercepts"},{"path":"/reference/newTau.html","id":null,"dir":"Reference","previous_headings":"","what":"newTau — newTau","title":"newTau — newTau","text":"assign new value parameter tau used approximate optimization. regularized value tau evaluated zeroed directly impacts AIC, BIC, etc.","code":""},{"path":"/reference/newTau.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"newTau — newTau","text":"","code":"newTau(regularizedSEM, tau)"},{"path":"/reference/newTau.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"newTau — newTau","text":"regularizedSEM object fitted approximate optimization tau new tau value","code":""},{"path":"/reference/newTau.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"newTau — newTau","text":"regularizedSEM, new regularizedSEM@fits$nonZeroParameters","code":""},{"path":"/reference/newTau.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"newTau — newTau","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +      l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +      l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Regularization:  lsem <- smoothLasso(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   # names of the regularized parameters:   regularized = paste0(\"l\", 6:15),   epsilon = 1e-10,   tau = 1e-4,   lambdas = seq(0,1,length.out = 50)) newTau(regularizedSEM = lsem, tau = .1)"},{"path":"/reference/plot-cvRegularizedSEM-missing-method.html","id":null,"dir":"Reference","previous_headings":"","what":"plots the cross-validation fits — plot,cvRegularizedSEM,missing-method","title":"plots the cross-validation fits — plot,cvRegularizedSEM,missing-method","text":"plots cross-validation fits","code":""},{"path":"/reference/plot-cvRegularizedSEM-missing-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"plots the cross-validation fits — plot,cvRegularizedSEM,missing-method","text":"","code":"# S4 method for cvRegularizedSEM,missing plot(x, y, ...)"},{"path":"/reference/plot-cvRegularizedSEM-missing-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"plots the cross-validation fits — plot,cvRegularizedSEM,missing-method","text":"x object class cvRegularizedSEM y used ... used","code":""},{"path":"/reference/plot-cvRegularizedSEM-missing-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"plots the cross-validation fits — plot,cvRegularizedSEM,missing-method","text":"either object ggplot2 plotly","code":""},{"path":"/reference/plot-gpRegularized-missing-method.html","id":null,"dir":"Reference","previous_headings":"","what":"plots the regularized and unregularized parameters for all levels of lambda — plot,gpRegularized,missing-method","title":"plots the regularized and unregularized parameters for all levels of lambda — plot,gpRegularized,missing-method","text":"plots regularized unregularized parameters levels lambda","code":""},{"path":"/reference/plot-gpRegularized-missing-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"plots the regularized and unregularized parameters for all levels of lambda — plot,gpRegularized,missing-method","text":"","code":"# S4 method for gpRegularized,missing plot(x, y, ...)"},{"path":"/reference/plot-gpRegularized-missing-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"plots the regularized and unregularized parameters for all levels of lambda — plot,gpRegularized,missing-method","text":"x object class gpRegularized y used ... use regularizedOnly=FALSE plot parameters","code":""},{"path":"/reference/plot-gpRegularized-missing-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"plots the regularized and unregularized parameters for all levels of lambda — plot,gpRegularized,missing-method","text":"either object ggplot2 plotly","code":""},{"path":"/reference/plot-regularizedSEM-missing-method.html","id":null,"dir":"Reference","previous_headings":"","what":"plots the regularized and unregularized parameters for all levels of lambda — plot,regularizedSEM,missing-method","title":"plots the regularized and unregularized parameters for all levels of lambda — plot,regularizedSEM,missing-method","text":"plots regularized unregularized parameters levels lambda","code":""},{"path":"/reference/plot-regularizedSEM-missing-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"plots the regularized and unregularized parameters for all levels of lambda — plot,regularizedSEM,missing-method","text":"","code":"# S4 method for regularizedSEM,missing plot(x, y, ...)"},{"path":"/reference/plot-regularizedSEM-missing-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"plots the regularized and unregularized parameters for all levels of lambda — plot,regularizedSEM,missing-method","text":"x object class gpRegularized y used ... use regularizedOnly=FALSE plot parameters","code":""},{"path":"/reference/plot-regularizedSEM-missing-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"plots the regularized and unregularized parameters for all levels of lambda — plot,regularizedSEM,missing-method","text":"either object ggplot2 plotly","code":""},{"path":"/reference/plot-stabSel-missing-method.html","id":null,"dir":"Reference","previous_headings":"","what":"plots the regularized and unregularized parameters for all levels of the tuning parameters — plot,stabSel,missing-method","title":"plots the regularized and unregularized parameters for all levels of the tuning parameters — plot,stabSel,missing-method","text":"plots regularized unregularized parameters levels tuning parameters","code":""},{"path":"/reference/plot-stabSel-missing-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"plots the regularized and unregularized parameters for all levels of the tuning parameters — plot,stabSel,missing-method","text":"","code":"# S4 method for stabSel,missing plot(x, y, ...)"},{"path":"/reference/plot-stabSel-missing-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"plots the regularized and unregularized parameters for all levels of the tuning parameters — plot,stabSel,missing-method","text":"x object class stabSel y used ... use regularizedOnly=FALSE plot parameters","code":""},{"path":"/reference/plot-stabSel-missing-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"plots the regularized and unregularized parameters for all levels of the tuning parameters — plot,stabSel,missing-method","text":"either object ggplot2 plotly","code":""},{"path":"/reference/regressions.html","id":null,"dir":"Reference","previous_headings":"","what":"regressions — regressions","title":"regressions — regressions","text":"Extract labels regressions found lavaan model.","code":""},{"path":"/reference/regressions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"regressions — regressions","text":"","code":"regressions(lavaanModel)"},{"path":"/reference/regressions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"regressions — regressions","text":"lavaanModel fitted lavaan model","code":""},{"path":"/reference/regressions.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"regressions — regressions","text":"vector parameter labels","code":""},{"path":"/reference/regressions.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"regressions — regressions","text":"","code":"# The following is adapted from ?lavaan::sem library(lessSEM) model <- '    # latent variable definitions   ind60 =~ x1 + x2 + x3   dem60 =~ y1 + a*y2 + b*y3 + c*y4   dem65 =~ y5 + a*y6 + b*y7 + c*y8    # regressions   dem60 ~ ind60   dem65 ~ ind60 + dem60    # residual correlations   y1 ~~ y5   y2 ~~ y4 + y6   y3 ~~ y7   y4 ~~ y8   y6 ~~ y8 '  fit <- sem(model, data = PoliticalDemocracy)  regressions(fit)"},{"path":"/reference/regsem2LavaanParameters.html","id":null,"dir":"Reference","previous_headings":"","what":"regsem2LavaanParameters — regsem2LavaanParameters","title":"regsem2LavaanParameters — regsem2LavaanParameters","text":"helper function: regsem lavaan use slightly different parameter labels. function can used translate parameter labels cv_regsem object lavaan labels","code":""},{"path":"/reference/regsem2LavaanParameters.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"regsem2LavaanParameters — regsem2LavaanParameters","text":"","code":"regsem2LavaanParameters(regsemModel, lavaanModel)"},{"path":"/reference/regsem2LavaanParameters.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"regsem2LavaanParameters — regsem2LavaanParameters","text":"regsemModel model class regsem lavaanModel model class lavaan","code":""},{"path":"/reference/regsem2LavaanParameters.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"regsem2LavaanParameters — regsem2LavaanParameters","text":"regsem parameters lavaan labels","code":""},{"path":"/reference/regsem2LavaanParameters.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"regsem2LavaanParameters — regsem2LavaanParameters","text":"","code":"## The following is adapted from ?regsem::regsem. #library(lessSEM) #library(regsem) ## put variables on same scale for regsem #HS <- data.frame(scale(HolzingerSwineford1939[,7:15])) # #mod <- ' #f =~ 1*x1 + l1*x2 + l2*x3 + l3*x4 + l4*x5 + l5*x6 + l6*x7 + l7*x8 + l8*x9 #' ## Recommended to specify meanstructure in lavaan #lavaanModel <- cfa(mod, HS, meanstructure=TRUE) # #regsemModel <- regsem(lavaanModel,  #                lambda = 0.3,  #                gradFun = \"ram\", #                type=\"lasso\", #                pars_pen=c(\"l1\", \"l2\", \"l6\", \"l7\", \"l8\")) # regsem2LavaanParameters(regsemModel = regsemModel, #                         lavaanModel = lavaanModel)"},{"path":"/reference/regularizedSEM-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Class for regularized SEM — regularizedSEM-class","title":"Class for regularized SEM — regularizedSEM-class","text":"Class regularized SEM","code":""},{"path":"/reference/regularizedSEM-class.html","id":"slots","dir":"Reference","previous_headings":"","what":"Slots","title":"Class for regularized SEM — regularizedSEM-class","text":"penalty penalty used (e.g., \"lasso\") parameters data.frame parameter estimates fits data.frame fit results parameterLabels character vector names parameters weights vector weights given parameters penalty regularized character vector names regularized parameters transformations model transformations, transformed parameters returned internalOptimization list elements used internally inputArguments list elements passed user general notes internal notes come fitting model","code":""},{"path":"/reference/regularizedSEMMixedPenalty-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Class for regularized SEM — regularizedSEMMixedPenalty-class","title":"Class for regularized SEM — regularizedSEMMixedPenalty-class","text":"Class regularized SEM","code":""},{"path":"/reference/regularizedSEMMixedPenalty-class.html","id":"slots","dir":"Reference","previous_headings":"","what":"Slots","title":"Class for regularized SEM — regularizedSEMMixedPenalty-class","text":"penalty penalty used (e.g., \"lasso\") tuningParameterConfigurations list settings lambda, theta, alpha tuning parameters. parameters data.frame parameter estimates fits data.frame fit results parameterLabels character vector names parameters weights vector weights given parameters penalty regularized character vector names regularized parameters transformations model transformations, transformed parameters returned internalOptimization list elements used internally inputArguments list elements passed user general notes internal notes come fitting model","code":""},{"path":"/reference/regularizedSEMWithCustomPenalty-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Class for regularized SEM using Rsolnp — regularizedSEMWithCustomPenalty-class","title":"Class for regularized SEM using Rsolnp — regularizedSEMWithCustomPenalty-class","text":"Class regularized SEM using Rsolnp","code":""},{"path":"/reference/regularizedSEMWithCustomPenalty-class.html","id":"slots","dir":"Reference","previous_headings":"","what":"Slots","title":"Class for regularized SEM using Rsolnp — regularizedSEMWithCustomPenalty-class","text":"parameters data.frame parameter estimates fits data.frame fit results parameterLabels character vector names parameters internalOptimization list elements used internally inputArguments list elements passed user general notes internal notes come fitting model","code":""},{"path":"/reference/ridge.html","id":null,"dir":"Reference","previous_headings":"","what":"ridge — ridge","title":"ridge — ridge","text":"Implements ridge regularization structural equation models. penalty function given : $$p( x_j) = \\lambda x_j^2$$ Note ridge regularization set parameters zero result shrinkage towards zero.","code":""},{"path":"/reference/ridge.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"ridge — ridge","text":"","code":"ridge(   lavaanModel,   regularized,   lambdas,   method = \"glmnet\",   modifyModel = lessSEM::modifyModel(),   control = lessSEM::controlGlmnet() )"},{"path":"/reference/ridge.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"ridge — ridge","text":"lavaanModel model class lavaan regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda method optimizer used? Currently implemented ista glmnet. ista, control argument can used switch related procedures (currently gist). modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. element generated controlIsta controlGlmnet functions. See ?controlIsta ?controlGlmnet details.","code":""},{"path":"/reference/ridge.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"ridge — ridge","text":"Model class regularizedSEM","code":""},{"path":"/reference/ridge.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"ridge — ridge","text":"Identical regsem, models specified using lavaan. Currently, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. Ridge regularization: Hoerl, . E., & Kennard, R. W. (1970). Ridge Regression: Biased Estimation Nonorthogonal Problems. Technometrics, 12(1), 55–67. https://doi.org/10.1080/00401706.1970.10488634 Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classification. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/ridge.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"ridge — ridge","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +      l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +      l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Regularization:  lsem <- ridge(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   # names of the regularized parameters:   regularized = paste0(\"l\", 6:15),   lambdas = seq(0,1,length.out = 20))  # use the plot-function to plot the regularized parameters: plot(lsem)  # the coefficients can be accessed with: coef(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters[1,]   #### Advanced ### # Switching the optimizer # # Use the \"method\" argument to switch the optimizer. The control argument # must also be changed to the corresponding function: lsemIsta <- ridge(   lavaanModel = lavaanModel,   regularized = paste0(\"l\", 6:15),   lambdas = seq(0,1,length.out = 20),   method = \"ista\",   control = controlIsta())  # Note: The results are basically identical: lsemIsta@parameters - lsem@parameters"},{"path":"/reference/ridgeBfgs.html","id":null,"dir":"Reference","previous_headings":"","what":"ridgeBfgs — ridgeBfgs","title":"ridgeBfgs — ridgeBfgs","text":"function allows regularization models built lavaan ridge penalty. elements can accessed \"@\" operator (see examples).","code":""},{"path":"/reference/ridgeBfgs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"ridgeBfgs — ridgeBfgs","text":"","code":"ridgeBfgs(   lavaanModel,   regularized,   lambdas = NULL,   modifyModel = lessSEM::modifyModel(),   control = lessSEM::controlBFGS() )"},{"path":"/reference/ridgeBfgs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"ridgeBfgs — ridgeBfgs","text":"lavaanModel model class lavaan regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. element generated controlBFGS function. See ?controlBFGS details.","code":""},{"path":"/reference/ridgeBfgs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"ridgeBfgs — ridgeBfgs","text":"Model class regularizedSEM","code":""},{"path":"/reference/ridgeBfgs.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"ridgeBfgs — ridgeBfgs","text":"details, see: Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9","code":""},{"path":"/reference/ridgeBfgs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"ridgeBfgs — ridgeBfgs","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +      l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +      l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Regularization:  # names of the regularized parameters: regularized = paste0(\"l\", 6:15)  lsem <- ridgeBfgs(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   regularized = regularized,   lambdas = seq(0,1,length.out = 50))  plot(lsem)  # the coefficients can be accessed with: coef(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters[1,]"},{"path":"/reference/scad.html","id":null,"dir":"Reference","previous_headings":"","what":"scad — scad","title":"scad — scad","text":"Implements scad regularization structural equation models. penalty function given :  $$p( x_j) = \\begin{cases} \\lambda |x_j| & \\text{} |x_j| \\leq \\theta\\\\ \\frac{-x_j^2 + 2\\theta\\lambda |x_j| - \\lambda^2}{2(\\theta -1)} &  \\text{} \\lambda < |x_j| \\leq \\lambda\\theta \\\\ (\\theta + 1) \\lambda^2/2 & \\text{} |x_j| \\geq \\theta\\lambda\\\\ \\end{cases}$$ \\(\\theta > 2\\).","code":""},{"path":"/reference/scad.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"scad — scad","text":"","code":"scad(   lavaanModel,   regularized,   lambdas,   thetas,   modifyModel = lessSEM::modifyModel(),   method = \"glmnet\",   control = lessSEM::controlGlmnet() )"},{"path":"/reference/scad.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"scad — scad","text":"lavaanModel model class lavaan regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda thetas parameters whose absolute value threshold penalized constant (theta) modifyModel used modify lavaanModel. See ?modifyModel. method optimizer used? Currently implemented ista glmnet. ista, control argument can used switch related procedures (currently gist). control used control optimizer. element generated controlIsta (see ?controlIsta)","code":""},{"path":"/reference/scad.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"scad — scad","text":"Model class regularizedSEM","code":""},{"path":"/reference/scad.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"scad — scad","text":"Identical regsem, models specified using lavaan. Currently, standard SEM supported. lessSEM also provides full information maximum likelihood missing data. use functionality, fit lavaan model argument sem(..., missing = 'ml'). lessSEM automatically switch full information maximum likelihood well. scad regularization: Fan, J., & Li, R. (2001). Variable selection via nonconcave penalized likelihood oracle properties. Journal American Statistical Association, 96(456), 1348–1360. https://doi.org/10.1198/016214501753382273 Regularized SEM Huang, P.-H., Chen, H., & Weng, L.-J. (2017). Penalized Likelihood Method Structural Equation Modeling. Psychometrika, 82(2), 329–354. https://doi.org/10.1007/s11336-017-9566-9 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 details GLMNET, see: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths Generalized Linear Models via Coordinate Descent. Journal Statistical Software, 33(1), 1–20. https://doi.org/10.18637/jss.v033.i01 Yuan, G.-X., Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2010). Comparison Optimization Methods Software Large-scale L1-regularized Linear Classification. Journal Machine Learning Research, 11, 3183–3234. Yuan, G.-X., Ho, C.-H., & Lin, C.-J. (2012). improved GLMNET l1-regularized logistic regression. Journal Machine Learning Research, 13, 1999–2030. https://doi.org/10.1145/2020408.2020421 details ISTA, see: Beck, ., & Teboulle, M. (2009). Fast Iterative Shrinkage-Thresholding Algorithm Linear Inverse Problems. SIAM Journal Imaging Sciences, 2(1), 183–202. https://doi.org/10.1137/080716542 Gong, P., Zhang, C., Lu, Z., Huang, J., & Ye, J. (2013). General Iterative Shrinkage Thresholding Algorithm Non-convex Regularized Optimization Problems. Proceedings 30th International Conference Machine Learning, 28(2)(2), 37–45. Parikh, N., & Boyd, S. (2013). Proximal Algorithms. Foundations Trends Optimization, 1(3), 123–231.","code":""},{"path":"/reference/scad.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"scad — scad","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +      l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +      l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Regularization:  lsem <- scad(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   # names of the regularized parameters:   regularized = paste0(\"l\", 6:15),   lambdas = seq(0,1,length.out = 20),   thetas = seq(2.01,5,length.out = 5))  # the coefficients can be accessed with: coef(lsem)  # if you are only interested in the estimates and not the tuning parameters, use coef(lsem)@estimates # or estimates(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters[1,]  # fit Measures: fitIndices(lsem)  # The best parameters can also be extracted with: coef(lsem, criterion = \"AIC\") # or estimates(lsem, criterion = \"AIC\")  # optional: plotting the paths requires installation of plotly # plot(lsem)"},{"path":"/reference/scadPenalty_C.html","id":null,"dir":"Reference","previous_headings":"","what":"scadPenalty_C — scadPenalty_C","title":"scadPenalty_C — scadPenalty_C","text":"scadPenalty_C","code":""},{"path":"/reference/scadPenalty_C.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"scadPenalty_C — scadPenalty_C","text":"","code":"scadPenalty_C(par, lambda_p, theta)"},{"path":"/reference/scadPenalty_C.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"scadPenalty_C — scadPenalty_C","text":"par single parameter value lambda_p lambda value parameter theta theta value parameter","code":""},{"path":"/reference/scadPenalty_C.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"scadPenalty_C — scadPenalty_C","text":"penalty value","code":""},{"path":"/reference/show-Rcpp_SEMCpp-method.html","id":null,"dir":"Reference","previous_headings":"","what":"show — show,Rcpp_SEMCpp-method","title":"show — show,Rcpp_SEMCpp-method","text":"show","code":""},{"path":"/reference/show-Rcpp_SEMCpp-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"show — show,Rcpp_SEMCpp-method","text":"","code":"# S4 method for Rcpp_SEMCpp show(object)"},{"path":"/reference/show-Rcpp_SEMCpp-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"show — show,Rcpp_SEMCpp-method","text":"object object class Rcpp_SEMCpp","code":""},{"path":"/reference/show-Rcpp_SEMCpp-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"show — show,Rcpp_SEMCpp-method","text":"return value, just prints estimates","code":""},{"path":"/reference/show-Rcpp_mgSEM-method.html","id":null,"dir":"Reference","previous_headings":"","what":"show — show,Rcpp_mgSEM-method","title":"show — show,Rcpp_mgSEM-method","text":"show","code":""},{"path":"/reference/show-Rcpp_mgSEM-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"show — show,Rcpp_mgSEM-method","text":"","code":"# S4 method for Rcpp_mgSEM show(object)"},{"path":"/reference/show-Rcpp_mgSEM-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"show — show,Rcpp_mgSEM-method","text":"object object class Rcpp_mgSEM","code":""},{"path":"/reference/show-Rcpp_mgSEM-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"show — show,Rcpp_mgSEM-method","text":"return value, just prints estimates","code":""},{"path":"/reference/show-cvRegularizedSEM-method.html","id":null,"dir":"Reference","previous_headings":"","what":"Show method for objects of class cvRegularizedSEM. — show,cvRegularizedSEM-method","title":"Show method for objects of class cvRegularizedSEM. — show,cvRegularizedSEM-method","text":"Show method objects class cvRegularizedSEM.","code":""},{"path":"/reference/show-cvRegularizedSEM-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Show method for objects of class cvRegularizedSEM. — show,cvRegularizedSEM-method","text":"","code":"# S4 method for cvRegularizedSEM show(object)"},{"path":"/reference/show-cvRegularizedSEM-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Show method for objects of class cvRegularizedSEM. — show,cvRegularizedSEM-method","text":"object object class cvRegularizedSEM","code":""},{"path":"/reference/show-cvRegularizedSEM-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Show method for objects of class cvRegularizedSEM. — show,cvRegularizedSEM-method","text":"return value, just prints estimates","code":""},{"path":"/reference/show-gpRegularized-method.html","id":null,"dir":"Reference","previous_headings":"","what":"show — show,gpRegularized-method","title":"show — show,gpRegularized-method","text":"show","code":""},{"path":"/reference/show-gpRegularized-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"show — show,gpRegularized-method","text":"","code":"# S4 method for gpRegularized show(object)"},{"path":"/reference/show-gpRegularized-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"show — show,gpRegularized-method","text":"object object class gpRegularized","code":""},{"path":"/reference/show-gpRegularized-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"show — show,gpRegularized-method","text":"return value, just prints estimates","code":""},{"path":"/reference/show-lessSEMCoef-method.html","id":null,"dir":"Reference","previous_headings":"","what":"show — show,lessSEMCoef-method","title":"show — show,lessSEMCoef-method","text":"show","code":""},{"path":"/reference/show-lessSEMCoef-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"show — show,lessSEMCoef-method","text":"","code":"# S4 method for lessSEMCoef show(object)"},{"path":"/reference/show-lessSEMCoef-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"show — show,lessSEMCoef-method","text":"object object class lessSEMCoef","code":""},{"path":"/reference/show-lessSEMCoef-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"show — show,lessSEMCoef-method","text":"return value, just prints estimates","code":""},{"path":"/reference/show-logLikelihood-method.html","id":null,"dir":"Reference","previous_headings":"","what":"show — show,logLikelihood-method","title":"show — show,logLikelihood-method","text":"show","code":""},{"path":"/reference/show-logLikelihood-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"show — show,logLikelihood-method","text":"","code":"# S4 method for logLikelihood show(object)"},{"path":"/reference/show-logLikelihood-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"show — show,logLikelihood-method","text":"object object class logLikelihood","code":""},{"path":"/reference/show-logLikelihood-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"show — show,logLikelihood-method","text":"return value, just prints estimates","code":""},{"path":"/reference/show-regularizedSEM-method.html","id":null,"dir":"Reference","previous_headings":"","what":"show — show,regularizedSEM-method","title":"show — show,regularizedSEM-method","text":"show","code":""},{"path":"/reference/show-regularizedSEM-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"show — show,regularizedSEM-method","text":"","code":"# S4 method for regularizedSEM show(object)"},{"path":"/reference/show-regularizedSEM-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"show — show,regularizedSEM-method","text":"object object class regularizedSEM","code":""},{"path":"/reference/show-regularizedSEM-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"show — show,regularizedSEM-method","text":"return value, just prints estimates","code":""},{"path":"/reference/show-regularizedSEMMixedPenalty-method.html","id":null,"dir":"Reference","previous_headings":"","what":"show — show,regularizedSEMMixedPenalty-method","title":"show — show,regularizedSEMMixedPenalty-method","text":"show","code":""},{"path":"/reference/show-regularizedSEMMixedPenalty-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"show — show,regularizedSEMMixedPenalty-method","text":"","code":"# S4 method for regularizedSEMMixedPenalty show(object)"},{"path":"/reference/show-regularizedSEMMixedPenalty-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"show — show,regularizedSEMMixedPenalty-method","text":"object object class regularizedSEM","code":""},{"path":"/reference/show-regularizedSEMMixedPenalty-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"show — show,regularizedSEMMixedPenalty-method","text":"return value, just prints estimates","code":""},{"path":"/reference/show-stabSel-method.html","id":null,"dir":"Reference","previous_headings":"","what":"show — show,stabSel-method","title":"show — show,stabSel-method","text":"show","code":""},{"path":"/reference/show-stabSel-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"show — show,stabSel-method","text":"","code":"# S4 method for stabSel show(object)"},{"path":"/reference/show-stabSel-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"show — show,stabSel-method","text":"object object class stabSel","code":""},{"path":"/reference/show-stabSel-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"show — show,stabSel-method","text":"return value, just prints estimates","code":""},{"path":"/reference/simulateExampleData.html","id":null,"dir":"Reference","previous_headings":"","what":"simulateExampleData — simulateExampleData","title":"simulateExampleData — simulateExampleData","text":"simulate data simple CFA model","code":""},{"path":"/reference/simulateExampleData.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"simulateExampleData — simulateExampleData","text":"","code":"simulateExampleData(   N = 100,   loadings = c(rep(1, 5), rep(0.4, 5), rep(0, 5)),   percentMissing = 0 )"},{"path":"/reference/simulateExampleData.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"simulateExampleData — simulateExampleData","text":"N number persons data set loadings loadings latent variable manifest observations percentMissing percentage missing data","code":""},{"path":"/reference/simulateExampleData.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"simulateExampleData — simulateExampleData","text":"data set single-factor CFA.","code":""},{"path":"/reference/simulateExampleData.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"simulateExampleData — simulateExampleData","text":"","code":"y <- lessSEM::simulateExampleData()"},{"path":"/reference/smoothAdaptiveLasso.html","id":null,"dir":"Reference","previous_headings":"","what":"smoothAdaptiveLasso — smoothAdaptiveLasso","title":"smoothAdaptiveLasso — smoothAdaptiveLasso","text":"function allows regularization models built lavaan smooth adaptive lasso penalty. returned object S4 class; elements can accessed \"@\" operator (see examples).","code":""},{"path":"/reference/smoothAdaptiveLasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"smoothAdaptiveLasso — smoothAdaptiveLasso","text":"","code":"smoothAdaptiveLasso(   lavaanModel,   regularized,   weights = NULL,   lambdas,   epsilon,   tau,   modifyModel = lessSEM::modifyModel(),   control = lessSEM::controlBFGS() )"},{"path":"/reference/smoothAdaptiveLasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"smoothAdaptiveLasso — smoothAdaptiveLasso","text":"lavaanModel model class lavaan regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object weights labeled vector weights parameters model. unsure parameters called, use getLavaanParameters(model) lavaan model object. set NULL, default weights used: inverse absolute values unregularized parameter estimates lambdas numeric vector: values tuning parameter lambda epsilon epsilon > 0; controls smoothness approximation. Larger values = smoother tau parameters threshold tau seen zeroed modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. element generated controlBFGS function. See ?controlBFGS details.","code":""},{"path":"/reference/smoothAdaptiveLasso.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"smoothAdaptiveLasso — smoothAdaptiveLasso","text":"Model class regularizedSEM","code":""},{"path":"/reference/smoothAdaptiveLasso.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"smoothAdaptiveLasso — smoothAdaptiveLasso","text":"details, see: Zou, H. (2006). Adaptive Lasso Oracle Properties. Journal American Statistical Association, 101(476), 1418–1429. https://doi.org/10.1198/016214506000000735 Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 Lee, S.-., Lee, H., Abbeel, P., & Ng, . Y. (2006). Efficient L1 Regularized Logistic Regression. Proceedings Twenty-First National Conference Artificial Intelligence (AAAI-06), 401–408.","code":""},{"path":"/reference/smoothAdaptiveLasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"smoothAdaptiveLasso — smoothAdaptiveLasso","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +      l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +      l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Regularization:  # names of the regularized parameters: regularized = paste0(\"l\", 6:15)  # define adaptive lasso weights: # We use the inverse of the absolute unregularized parameters # (this is the default in adaptiveLasso and can also specified # by setting weights = NULL) weights <- 1/abs(getLavaanParameters(lavaanModel)) weights[!names(weights) %in% regularized] <- 0  lsem <- smoothAdaptiveLasso(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   regularized = regularized,   weights = weights,   epsilon = 1e-10,   tau = 1e-4,   lambdas = seq(0,1,length.out = 50))  # use the plot-function to plot the regularized parameters: plot(lsem)  # the coefficients can be accessed with: coef(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters[1,]  # AIC and BIC: AIC(lsem) BIC(lsem)  # The best parameters can also be extracted with: coef(lsem, criterion = \"AIC\") coef(lsem, criterion = \"BIC\")"},{"path":"/reference/smoothElasticNet.html","id":null,"dir":"Reference","previous_headings":"","what":"smoothElasticNet — smoothElasticNet","title":"smoothElasticNet — smoothElasticNet","text":"function allows regularization models built lavaan smooth elastic net penalty. elements can accessed \"@\" operator (see examples).","code":""},{"path":"/reference/smoothElasticNet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"smoothElasticNet — smoothElasticNet","text":"","code":"smoothElasticNet(   lavaanModel,   regularized,   lambdas = NULL,   nLambdas = NULL,   alphas,   epsilon,   tau,   modifyModel = lessSEM::modifyModel(),   control = lessSEM::controlBFGS() )"},{"path":"/reference/smoothElasticNet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"smoothElasticNet — smoothElasticNet","text":"lavaanModel model class lavaan regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda nLambdas alternative lambda: alpha = 1, lessSEM can automatically compute first lambda value sets regularized parameters zero. generate nLambda values 0 computed lambda. alphas numeric vector values tuning parameter alpha. Must 0 1. 0 = ridge, 1 = lasso. epsilon epsilon > 0; controls smoothness approximation. Larger values = smoother tau parameters threshold tau seen zeroed modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. element generated controlBFGS function. See ?controlBFGS details.","code":""},{"path":"/reference/smoothElasticNet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"smoothElasticNet — smoothElasticNet","text":"Model class regularizedSEM","code":""},{"path":"/reference/smoothElasticNet.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"smoothElasticNet — smoothElasticNet","text":"details, see: Zou, H., & Hastie, T. (2005). Regularization variable selection via elastic net. Journal Royal Statistical Society: Series B, 67(2), 301–320. https://doi.org/10.1111/j.1467-9868.2005.00503.x details regularization technique. Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793 Lee, S.-., Lee, H., Abbeel, P., & Ng, . Y. (2006). Efficient L1 Regularized Logistic Regression. Proceedings Twenty-First National Conference Artificial Intelligence (AAAI-06), 401–408.","code":""},{"path":"/reference/smoothElasticNet.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"smoothElasticNet — smoothElasticNet","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +      l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +      l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Regularization:  # names of the regularized parameters: regularized = paste0(\"l\", 6:15)  lsem <- smoothElasticNet(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   regularized = regularized,   epsilon = 1e-10,   tau = 1e-4,   lambdas = seq(0,1,length.out = 5),   alphas = seq(0,1,length.out = 3))  # the coefficients can be accessed with: coef(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters[1,]"},{"path":"/reference/smoothLasso.html","id":null,"dir":"Reference","previous_headings":"","what":"smoothLasso — smoothLasso","title":"smoothLasso — smoothLasso","text":"function allows regularization models built lavaan smoothed lasso penalty. returned object S4 class; elements can accessed \"@\" operator (see examples). recommend using function. Use lasso() instead.","code":""},{"path":"/reference/smoothLasso.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"smoothLasso — smoothLasso","text":"","code":"smoothLasso(   lavaanModel,   regularized,   lambdas,   epsilon,   tau,   modifyModel = lessSEM::modifyModel(),   control = lessSEM::controlBFGS() )"},{"path":"/reference/smoothLasso.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"smoothLasso — smoothLasso","text":"lavaanModel model class lavaan regularized vector names parameters regularized. unsure parameters called, use getLavaanParameters(model) lavaan model object lambdas numeric vector: values tuning parameter lambda epsilon epsilon > 0; controls smoothness approximation. Larger values = smoother tau parameters threshold tau seen zeroed modifyModel used modify lavaanModel. See ?modifyModel. control used control optimizer. element generated controlBFGS function. See ?controlBFGS details.","code":""},{"path":"/reference/smoothLasso.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"smoothLasso — smoothLasso","text":"Model class regularizedSEM","code":""},{"path":"/reference/smoothLasso.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"smoothLasso — smoothLasso","text":"details, see: Lee, S.-., Lee, H., Abbeel, P., & Ng, . Y. (2006). Efficient L1 Regularized Logistic Regression. Proceedings Twenty-First National Conference Artificial Intelligence (AAAI-06), 401–408. Jacobucci, R., Grimm, K. J., & McArdle, J. J. (2016). Regularized Structural Equation Modeling. Structural Equation Modeling: Multidisciplinary Journal, 23(4), 555–566. https://doi.org/10.1080/10705511.2016.1154793","code":""},{"path":"/reference/smoothLasso.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"smoothLasso — smoothLasso","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +      l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +      l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Regularization:  lsem <- smoothLasso(   # pass the fitted lavaan model   lavaanModel = lavaanModel,   # names of the regularized parameters:   regularized = paste0(\"l\", 6:15),   epsilon = 1e-10,   tau = 1e-4,   lambdas = seq(0,1,length.out = 50))  # use the plot-function to plot the regularized parameters: plot(lsem)  # the coefficients can be accessed with: coef(lsem)  # elements of lsem can be accessed with the @ operator: lsem@parameters[1,]  # AIC and BIC: AIC(lsem) BIC(lsem)  # The best parameters can also be extracted with: coef(lsem, criterion = \"AIC\") coef(lsem, criterion = \"BIC\")"},{"path":"/reference/stabSel-class.html","id":null,"dir":"Reference","previous_headings":"","what":"Class for stability selection — stabSel-class","title":"Class for stability selection — stabSel-class","text":"Class stability selection","code":""},{"path":"/reference/stabSel-class.html","id":"slots","dir":"Reference","previous_headings":"","what":"Slots","title":"Class for stability selection — stabSel-class","text":"regularized names regularized parameters tuningParameters data.frame tuning parameter values stabilityPaths matrix percentage parameters non-zero averaged subsets setting tuning parameters percentSelected percentage parameter selected tuning parameter settings selectedParameters final selected parameters settings internal","code":""},{"path":"/reference/stabilitySelection.html","id":null,"dir":"Reference","previous_headings":"","what":"stabilitySelection — stabilitySelection","title":"stabilitySelection — stabilitySelection","text":"Provides rudimentary stability selection regularized SEM. Stability selection proposed Meinshausen & Bühlmann (2010) extended SEM Li & Jacobucci (2021). problem stabiltiy selection tries solve instability regularization procedures: Small changes data set may result different parameters selected. address issue, stability selection uses random subsamples initial data set fits models subsamples. parameter, can now check often included model given set tuning parameters. Plotting probabilities can provide overview parameters often removed remain model time. get final selection, threshold t can defined: parameter model t% time, retained.","code":""},{"path":"/reference/stabilitySelection.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"stabilitySelection — stabilitySelection","text":"","code":"stabilitySelection(   modelSpecification,   subsampleSize,   numberOfSubsamples = 100,   threshold = 70,   maxTries = 10 * numberOfSubsamples )"},{"path":"/reference/stabilitySelection.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"stabilitySelection — stabilitySelection","text":"modelSpecification call one penalty functions lessSEM. See examples details subsampleSize number subjects subsample. Must smaller number subjects original data set numberOfSubsamples number times procedure subsample recompute model. According Meinshausen & Bühlmann (2010), 100 seems work quite well also default regsem threshold percentage models, parameter contained order final model maxTries fitting models subset may fail. maxTries sets maximal number subsets try.","code":""},{"path":"/reference/stabilitySelection.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"stabilitySelection — stabilitySelection","text":"estimates subsample aggregated percentages parameter","code":""},{"path":"/reference/stabilitySelection.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"stabilitySelection — stabilitySelection","text":"Li, X., & Jacobucci, R. (2021). Regularized structural equation modeling stability selection. Psychological Methods, 27(4), 497–518. https://doi.org/10.1037/met0000389 Meinshausen, N., & Bühlmann, P. (2010). Stability selection. Journal Royal Statistical Society: Series B (Statistical Methodology), 72(4), 417–473. https://doi.org/10.1111/j.1467-9868.2010.00740.x","code":""},{"path":"/reference/stabilitySelection.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"stabilitySelection — stabilitySelection","text":"","code":"library(lessSEM)  # Identical to regsem, lessSEM builds on the lavaan # package for model specification. The first step # therefore is to implement the model in lavaan.  dataset <- simulateExampleData()  lavaanSyntax <- \" f =~ l1*y1 + l2*y2 + l3*y3 + l4*y4 + l5*y5 +       l6*y6 + l7*y7 + l8*y8 + l9*y9 + l10*y10 +       l11*y11 + l12*y12 + l13*y13 + l14*y14 + l15*y15 f ~~ 1*f \"  lavaanModel <- lavaan::sem(lavaanSyntax,                            data = dataset,                            meanstructure = TRUE,                            std.lv = TRUE)  # Stability selection stabSel <- stabilitySelection(   # IMPORTANT: Wrap your call to the penalty function in an rlang::expr-Block:   modelSpecification =      rlang::expr(       lasso(         # pass the fitted lavaan model         lavaanModel = lavaanModel,         # names of the regularized parameters:         regularized = paste0(\"l\", 6:15),         # in case of lasso and adaptive lasso, we can specify the number of lambda         # values to use. lessSEM will automatically find lambda_max and fit         # models for nLambda values between 0 and lambda_max. For the other         # penalty functions, lambdas must be specified explicitly         nLambdas = 50)     ),   subsampleSize = 80,   numberOfSubsamples = 5, # should be set to a much higher number (e.g., 100)   threshold = 70 ) stabSel plot(stabSel)"},{"path":"/reference/summary-cvRegularizedSEM-method.html","id":null,"dir":"Reference","previous_headings":"","what":"summary method for objects of class cvRegularizedSEM. — summary,cvRegularizedSEM-method","title":"summary method for objects of class cvRegularizedSEM. — summary,cvRegularizedSEM-method","text":"summary method objects class cvRegularizedSEM.","code":""},{"path":"/reference/summary-cvRegularizedSEM-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"summary method for objects of class cvRegularizedSEM. — summary,cvRegularizedSEM-method","text":"","code":"# S4 method for cvRegularizedSEM summary(object, ...)"},{"path":"/reference/summary-cvRegularizedSEM-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"summary method for objects of class cvRegularizedSEM. — summary,cvRegularizedSEM-method","text":"object object class cvRegularizedSEM ... used","code":""},{"path":"/reference/summary-cvRegularizedSEM-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"summary method for objects of class cvRegularizedSEM. — summary,cvRegularizedSEM-method","text":"return value, just prints estimates","code":""},{"path":"/reference/summary-gpRegularized-method.html","id":null,"dir":"Reference","previous_headings":"","what":"summary — summary,gpRegularized-method","title":"summary — summary,gpRegularized-method","text":"summary","code":""},{"path":"/reference/summary-gpRegularized-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"summary — summary,gpRegularized-method","text":"","code":"# S4 method for gpRegularized summary(object, ...)"},{"path":"/reference/summary-gpRegularized-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"summary — summary,gpRegularized-method","text":"object object class gpRegularized ... used","code":""},{"path":"/reference/summary-gpRegularized-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"summary — summary,gpRegularized-method","text":"return value, just prints estimates","code":""},{"path":"/reference/summary-regularizedSEM-method.html","id":null,"dir":"Reference","previous_headings":"","what":"summary — summary,regularizedSEM-method","title":"summary — summary,regularizedSEM-method","text":"summary","code":""},{"path":"/reference/summary-regularizedSEM-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"summary — summary,regularizedSEM-method","text":"","code":"# S4 method for regularizedSEM summary(object, ...)"},{"path":"/reference/summary-regularizedSEM-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"summary — summary,regularizedSEM-method","text":"object object class regularizedSEM ... used","code":""},{"path":"/reference/summary-regularizedSEM-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"summary — summary,regularizedSEM-method","text":"return value, just prints estimates","code":""},{"path":"/reference/summary-regularizedSEMMixedPenalty-method.html","id":null,"dir":"Reference","previous_headings":"","what":"summary — summary,regularizedSEMMixedPenalty-method","title":"summary — summary,regularizedSEMMixedPenalty-method","text":"summary","code":""},{"path":"/reference/summary-regularizedSEMMixedPenalty-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"summary — summary,regularizedSEMMixedPenalty-method","text":"","code":"# S4 method for regularizedSEMMixedPenalty summary(object, ...)"},{"path":"/reference/summary-regularizedSEMMixedPenalty-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"summary — summary,regularizedSEMMixedPenalty-method","text":"object object class regularizedSEMMixedPenalty ... used","code":""},{"path":"/reference/summary-regularizedSEMMixedPenalty-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"summary — summary,regularizedSEMMixedPenalty-method","text":"return value, just prints estimates","code":""},{"path":"/reference/summary-regularizedSEMWithCustomPenalty-method.html","id":null,"dir":"Reference","previous_headings":"","what":"summary — summary,regularizedSEMWithCustomPenalty-method","title":"summary — summary,regularizedSEMWithCustomPenalty-method","text":"summary","code":""},{"path":"/reference/summary-regularizedSEMWithCustomPenalty-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"summary — summary,regularizedSEMWithCustomPenalty-method","text":"","code":"# S4 method for regularizedSEMWithCustomPenalty summary(object, ...)"},{"path":"/reference/summary-regularizedSEMWithCustomPenalty-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"summary — summary,regularizedSEMWithCustomPenalty-method","text":"object object class regularizedSEMWithCustomPenalty ... used","code":""},{"path":"/reference/summary-regularizedSEMWithCustomPenalty-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"summary — summary,regularizedSEMWithCustomPenalty-method","text":"return value, just prints estimates","code":""},{"path":"/reference/variances.html","id":null,"dir":"Reference","previous_headings":"","what":"variances — variances","title":"variances — variances","text":"Extract labels variances found lavaan model.","code":""},{"path":"/reference/variances.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"variances — variances","text":"","code":"variances(lavaanModel)"},{"path":"/reference/variances.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"variances — variances","text":"lavaanModel fitted lavaan model","code":""},{"path":"/reference/variances.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"variances — variances","text":"vector parameter labels","code":""},{"path":"/reference/variances.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"variances — variances","text":"","code":"# The following is adapted from ?lavaan::sem library(lessSEM) model <- '    # latent variable definitions   ind60 =~ x1 + x2 + x3   dem60 =~ y1 + a*y2 + b*y3 + c*y4   dem65 =~ y5 + a*y6 + b*y7 + c*y8    # regressions   dem60 ~ ind60   dem65 ~ ind60 + dem60    # residual correlations   y1 ~~ y5   y2 ~~ y4 + y6   y3 ~~ y7   y4 ~~ y8   y6 ~~ y8 '  fit <- sem(model, data = PoliticalDemocracy)  variances(fit)"},{"path":"/news/index.html","id":"lesssem-152","dir":"Changelog","previous_headings":"","what":"lessSEM 1.5.2","title":"lessSEM 1.5.2","text":"CRAN release: 2023-06-25 Fixed bug extreme cases missing data added functions easily extract names loadings, regressions, variances, covariances lavaan model.","code":""},{"path":"/news/index.html","id":"lesssem-150","dir":"Changelog","previous_headings":"","what":"lessSEM 1.5.0","title":"lessSEM 1.5.0","text":"Added NEWS.md file track changes package. glmnet now supports penalty functions wls estimator now supported added stability selection added estimator - function easily access parameter estimates","code":""}]
